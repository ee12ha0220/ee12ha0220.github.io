---
title: 'Denoising with Kernel Prediction and Asymmetric Loss Functions'
# image : ""
date: '2021-10-30'
categories: [Paper review, MC denoising]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

We present a modular convolutional architecture for denoising rendered images. We expand on the capabilities of kernel-predicting networks by combining them with a number of task-specific modules, and optimizing the assembly using an asymmetric loss. The source-aware encoder—the first module in the assembly—extracts low-level features and embeds them into a common feature space, enabling quick adaptation of a trained network to novel data. The spatial and temporal modules extract abstract, high-level features for kernel-based reconstruction, which is performed at three differ- ent spatial scales to reduce low-frequency artifacts. The complete network is trained using a class of asymmetric loss functions that are designed to pre- serve details and provide the user with a direct control over the variance-bias trade-off during inference. We also propose an error-predicting module for inferring reconstruction error maps that can used to drive adaptive sampling. Finally, we present a theoretical analysis of convergence rates of kernel- predicting architectures, shedding light on why kernel prediction performs better than synthesizing the colors directly, complementing the empirical evidence presented in this and previous works. We demonstrate that our networks attain results that compare favorably to state-of-the-art methods in terms of detail preservation, low-frequency noise removal, and temporal stability on a variety of production and academic data sets.

# Introduction

Machine learning을 이용한 Monte Carlo denoising은 이전의 기법들과 비교했을 때 뛰어난 성능을 보여준다. 하지만 이렇게 ML을 사용하는 방법은 neural network의 data-efficiency 문제로 엄청난 양의 training data가 있어야 학습이 가능하다는 단점이 있다. 본 논문에서는 이를 비롯해 여러 Neural network을 사용하는 것에 대한 단점들을 해결하고자 하는데, main contribution은 아래와 같다. 

- 다양한 source에서 만들어진 input data(다른 renderer 사용, 다른 auxiliary buffer 사용)를 사용하기 위해 **source-aware encoder**를 사용한다. 이는 서로 다른 dataset을 학습에 사용할 수 있는 것 뿐만 아니라, pre-trained model이 새로운 data source에 빠르게 적응할 수 있게 해준다. 

- Animated sequence에 적합한 temporal domain을 만들어 temporal stability를 보장하고, 이전 연구들에 비해 학습에 더 적은 ground truth data필요로 하게 해준다. 

- Error-predicting model을 통해 rendering 단계에서 adaptive sampling을 가능하게 해준다. 

- Asymmetric loss function을 이용해 reconstruct된 image의 variance-bias tradeoff를 user 측면에서 조절할 수 있게 해준다. 

# Background

## Temporal stability

Temporal stability는 animation에서 인접한 frame들이 서로 유사해야 된다는 concept이다. 예를 들어 temporal stability가 보장되지 않으면, frame들 사이의 미세한 차이로 인해 animation에서 flickering 현상이 발생하게 된다. 

## Error estimation and adoptive sampling

Adoptive sampling은 엄청나게 큰 연구분야이기 때문에, 자세히 설명하지는 않을 것이다. 다만 adoptive sampling과 MC reconstruction 을 접목시키고자 하는 연구들이 등장했고, 이들은 대부분 sample들에 초점을 맞춰서 진행된다. 본 논문에서는 neural network에 noisy image와 denoised image만 제공해 더 robust하게 만들고, **Symmetric Mean Absolute Percentage Error (SMAPE)**라는 metric을 사용해서 이를 학습시킨다고 한다. 


# Modular architecture







<img src="/assets/images/SBMC_1.png" width="90%" height="90%"> *Ground truth image와 denoised image를 비교해 봤을 때, sample-based method가 motion blur을 더 잘 표현한다.*

## Permutation invariance in Neural Networks

Sample에 대한 정보를 사용하면, neural network에 여러개의 input이 들어가게 된다. 실제 Monte carlo rendering에서 image를 만드는 과정을 생각해보면, input의 순서에 따라 network의 결과값이 달라지면 안된다. 

# Sample-based denoising network

SBMC는 sample-based method이기 때문에 pixel $(x,y)$ 에 대한 $s$개의 sample (noisy radiance $L_{xys}$, auxiliary features $f_{xys}$)를 input으로 받는다. Sample의 개수가 고정되어 있지 않기 때문에 기존의 single-pass feedforward neural network은 적합하지 않고, RNN 같은 경우에는 permutation-invariant하지 않기 때문에 적합하지 않다. 그래서 본 논문에서는 여러 per-sample non-linear processing들의 spatial information을 CNN을 통해 sharing하는 novel architecture을 소개한다. 

Previous work [Bako et al. 2017; Vogels et al. 2018]과 마찬가지로, SBMC도 noisy image에 apply 될 수 있는 kernel을 만드는 것을 목표로 한다. 하지만 pixel단위가 아닌 sample단위이기 때문에, **각 sample이 근처에 있는 pixel에 얼마나 contribute 하는지**를 알아내는 것을 목표로 한다고 생각할 수 있다. 이 때문에 SBMC에서는 일반적인 gathering kernel이 아니라 **splatting kernel**, 즉 sample에 대한 정보를 주변 pixel에 splat하는 kernel을 사용한다(\ref{eq1}). 

---

$$
I_{uv} = \frac{\sum_{x,y,s}K_{xyuvs}L_{xys}}{\sum_{x,y,s}K_{xyuvs}}
\label{eq1} \tag{1}
$$

---

이때 $s$는 pixel $(x,y)$에 존재하는 각 sample, $K$는 predict하고자 하는 kernel, $I_{uv}$는 근처에 있는 pixel $(u,v)$의 denoised된 값을 의미한다. $K$는 [KPCN](https://ee12ha0220.github.io/posts/KPCN/)과 마찬가지로 $21\times21$의 크기로 사용한다. 

## Sample embeddings and context features

Permutation invariance를 만족시키기 위해, SBMC에서는 per-sample feature extraction과 spatial information sharing을 분리시킨다. 이때 **individual sample embedding**과 **per-pixel context feature** 라는 개념이 쓰인다. 

먼저 fully connected layer을 이용해 각 sample마다 embedding을 만든다. 이때 각 sample은 서로에게 영향을 주지 않기 때문에, sample의 순서에 영향을 받지 않는다. 즉, permutation invariance를 갖게된다(\ref{eq2}). 

---

$$
E^0_{xys} = \text{FC}(L_{xys},f_{xys};\theta^0_E) \label{eq2} \tag{2}
$$

---

그 후 각 sample들 사이의 정보 교환을 위해 $E^0$를 이용해 per-pixel context feature $C^0$를 계산해준다(\ref{eq3}).

---

$$
C^0_{xy} = \underset{s}{\text{reduce_mean}}(E^0_{xys}) \label{eq3} \tag{3}
$$

---

그냥 평균을 취하는 것이기 때문에, permutation invariance가 유지된다. 추가적으로 $C^0$는 sample의 개수와 상관없이 같은 차원을 갖기 때문에 CNN을 사용할 수 있게 된다. 

이렇게 만들어진 $C^0$는 U-Net을 통해 같은 차원의 또다른 context feature $C^1$으로 변환되고, 이는 sample embedding $E^0$과 함께 fully connected layer에 feed되어 새로운 sample embedding $E^1$을 생성한다(\ref{eq4}).

---

$$
C^1 = \text{UNet}(C^0;\theta^0_C) \quad E^1_{xys} = \text{FC}(E^0_{xys},C^1_{xy}; \theta^1_{E})
\label{eq4} \tag{4}
$$

---

이 과정을 3번정도 반복해서 만들어진 sample embedding이 kernel prediction에 사용되는데, 이는 자신에 대한 정보 뿐만 아니라 neighborhood의 정보도 담고 있기 때문에 coherent한 결과를 낼 수 있도록 해준다. 

## Per-sample splatting kernels

Splatting kernel은 앞서 생성한 sample embedding과 fully connected network을 이용해서 만들어진다(\ref{eq5}).

---

$$
K_{xyuvs} = \text{FC}(E^n_{xys}, C^n_{xy};\theta_K) \label{eq5} \tag{5}
$$

---

이때 $u,v$는 $x,y$의 neighbor pixel을 의미한다. 이때 gather kernel이 아니라 splatting kernel을 사용하는 이유는, sample을 기준으로 만든 embedding을 이용해 kernel을 만들기 때문에 sample의 view에서 작업을 수행하는 것이 permutation invariance를 유지하기 쉽기 때문이다. 더 자세한 내용은 아래 figure과 함께 설명되어 있다. 

<img src="/assets/images/SBMC_4.png" width="100%" height="100%">*앞서 kernel을 생성할 때 현재 pixel에 있는 sample embedding만을 사용하기 때문에, 이 상황처럼 a와 b의 index가 바뀌면 a에 b를 고려하며 학습된 잘못된 weight가 들어가게 된다. Splatting kernel을 사용할 경우 이를 방지할 수 있다.*

## Overall structure and algorithm

<img src="/assets/images/SBMC_2.png" width="90%" height="90%">*SBMC model의 전체적인 구조*

<img src="/assets/images/SBMC_3.png" width="90%" height="90%">*SBMC model의 algorithm*

# Dataset and training procedure

SBMC에서는 $128\times128$ 해상도의 rendered image 300,000장을 학습에 사용했고, 동일한 방식으로 1,000장을 rendering해서 validation에 사용했다. Ground truth image의 경우에는 4096 spp로 rendering된 image를 사용했다. 더 자세한 설명은 [SBMC 논문](https://groups.csail.mit.edu/graphics/rendernet/)을 참조하길 바란다. 

# Result

SBMC는 previous work와 비교했을 때 좋은 성능을 보여줬으며, 특히 motion blur이나 depth of field등 auxiliary buffer에 noise가 큰 경우에 특출나게 좋은 성능을 보여줬다. 더 자세한 설명은 [SBMC 논문](https://groups.csail.mit.edu/graphics/rendernet/)을 참조하길 바란다.

<img src="/assets/images/SBMC_5.png" width="90%" height="90%">*Depth of field 상황에서 KPCN보다 확연하게 뛰어난 성능을 보여준다.*


# Conclusion

We propose a new convolutional neural network for denoising Monte Carlo renderings. The key innovations that explain the suc- cess of our method are the use of samples rather than pixel statistics and splatting rather than gathering. For this, we introduce a new kernel-splatting architecture that is invariant to input permutations and accepts arbitrary numbers of samples. We show that our approach is robust to severe, heavy-tailed noise in low sample count settings and excels at rendering scenes with distributed effects such as depth-of-field, achieving significantly-reduced error on our extensive tests.

