---
title: "Generative Modeling by Estimating Gradients of the Data Distribution"
# image : ""
date: '2022-08-10'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to
gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.

# Introduction

Generative model은 machine learning의 다양한 분야에서 사용되는데, 대표적으로 log-likelihood를 이용해서 학습을 하는 likelihood-based models와, adversarial training을 이용하는 GAN이 있다. 이 두 model은 뛰어난 성능을 보이지만 한계점도 존재하는데, likelihood based model의 경우에는 normalized probability model을 만들기 위해 특정한 architecture(autoregressive model, flow model)이 강제되고, surrogate loss(VAE의 evidence lower bound)를 사용해야 한다는 한계점이 있다. GAN은 likelihood based model의 문제점들을 어느정도 완화했지만, adversarial training이 불안정하다는 단점이 있다. 

본 논문(SMLD)에서는 log-probability의 gradient인 ***'score'*** 라는 개념을 이용한 새로운 generative model을 제안하는데, score matching을 통해 neural network를 학습시킨다. 

# Score based generative modeling

Probability density $p(\mathbf{x})$에 대해, score은 $\nabla _\mathbf{x} \log p(\mathbf{x})$로 정의할 수 있다. Score network $\mathbf{s} _\mathbf{\theta}$는 $p _\text{data} (\mathbf{x})$의 score를 예측하는 것을 목표로 한다. 여기에는 score matching과 Langevin dynamics의 2가지 개념이 사용된다. 

## Score matching for score estimation

Score estimation의 기본적인 objective은 $\frac{1}{2} \mathbb{E} _{p _\text{data}}[ \|\| \mathbf{s} _\mathbf{\theta}(\mathbf{x}) -  \nabla _\mathbf{x} \log p _\text{data} (\mathbf{x})) \|\| ^2 _2 ]$를 minimize 하는 것이다. 이는 [Song et al. 2019](https://arxiv.org/abs/1905.07088)에 의해 다음 식으로 다시 쓸 수 있다(\ref{eq1}). 

---

$$
\mathbb{E}_{p_\text{data}(\mathbf{x})} \left[ \text{tr}(\nabla_\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x})) + \frac{1}{2} ||\mathbf{s}_\mathbf{\theta}(\mathbf{x})||^2_2 \right] \label{eq1} \tag{1}
$$

---

이때 $\nabla _\mathbf{x} \mathbf{s} _\mathbf{\theta}(\mathbf{x})$는 $\mathbf{s} _\mathbf{\theta}(\mathbf{x})$의 Jacobian을 의미한다. 하지만 image같은 high-dimensional data를 사용하는 경우에 $\text{tr}(\nabla _\mathbf{x} \mathbf{s} _\mathbf{\theta}(\mathbf{x}))$의 계산이 너무 복잡하기 때문에, 이를 그대로 사용하는 것에는 무리가 있다. 이를 해결하기 위해 다음과 같은 방법들이 사용될 수 있다. 

### Denoising score matching

Denoising score matching은 $\text{tr}(\nabla _\mathbf{x} \mathbf{s} _\mathbf{\theta}(\mathbf{x}))$항의 계산을 통째로 없애버리는 방법이다. Data point $\mathbf{x}$를 pre-defined noise distribution $q _\sigma (\tilde{\mathbf{x}}\|\mathbf{x})$를 이용해 $q _\sigma (\tilde{\mathbf{x}}) = \int q _\sigma (\tilde{\mathbf{x}}\|\mathbf{x})p _\text{data}(\mathbf{x})\text{d}\mathbf{x}$으로 perturb 시키고, 이 perturb된 data distribution의 score를 estimate 한다. 그러면 objective은 다음과 같이 쓸 수 있다(\ref{eq2}).

---

$$
\frac{1}{2}\mathbb{E}_{q _\sigma (\tilde{\mathbf{x}}|\mathbf{x})p_\text{data}(\mathbf{x})} [ || \mathbf{s}_\mathbf{\theta}(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q _\sigma (\tilde{\mathbf{x}}|\mathbf{x}) ||^2_2 ] \label{eq2} \tag{2}
$$

---

이때 $q _\sigma (\tilde{\mathbf{x}}\|\mathbf{x})$는 알고 있는 값이기 때문에, $\text{tr}(\nabla _\mathbf{x} \mathbf{s} _\mathbf{\theta}(\mathbf{x}))$를 계산하지 않고 equation \ref{eq2}를 바로 풀 수 있다. 이렇게 구해진 optimal한 $\mathbf{s} _\mathbf{\theta}(\tilde{\mathbf{x}})$는 $\nabla _\mathbf{x} \log q _\sigma (\mathbf{x})$와 같아지게 되고, $q _\sigma(\mathbf{x}) \approx p _\text{data} (\mathbf{x})$를 만족할정도로 작은 noise를 사용하면 원래 objective 역시 풀 수 있다. 

### Sliced score matching

Sliced score matching은 $\text{tr}(\nabla _\mathbf{x} \mathbf{s} _\mathbf{\theta}(\mathbf{x}))$를 쉽게 계산하기 위해 random projection을 사용하는 방법이다. [Song et al. 2019](https://arxiv.org/abs/1905.07088)에 따르면 trace를 $\mathbf{v}^\intercal \nabla _\mathbf{x}\mathbf{s} _\mathbf{\theta}(\mathbf{x})\mathbf{v}$로 근사할 수 있다고 한다. 이때 $\mathbf{v}$는 그냥 random한 vector이다. 이 경우에 objective은 다음과 같다(\ref{eq3}).

---

$$
\mathbb{E}_{p_\mathbf{v}}\mathbb{E}_{p_\text{data}}\left[ \mathbf{v}^\intercal \nabla_\mathbf{x}\mathbf{s}_\mathbf{\theta}(\mathbf{x})\mathbf{v} + \frac{1}{2}|| \mathbf{x}_\mathbf{\theta}(\mathbf{x}) ||^2_2 \right] \label{eq3} \tag{3}
$$

---

이 방법은 perturbed data를 사용하는 denoising score matching과는 다르게 unperturbed data를 사용한다는 이점이 있지만, 그만큼 더 많은 계산량을 필요로 한다. 

## Sampling with Langevin dynamics

Langevin dynamics를 사용하면 score function $\nabla _\mathbf{x} \log p(\mathbf{x})$만을 사용해서 $p(\mathbf{x})$를 따르는 sample을 얻을 수 있다(\ref{eq4}).

---

$$
\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \frac{\epsilon}{2} \nabla_\mathbf{x} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon} \mathbf{z}_t \label{eq4} \tag{4}
$$

---

이때 $\mathbf{z} _t \sim \mathcal{N}(0, I)$이고, $\epsilon$은 fixed step size이다. $\epsilon \rightarrow 0$, $T \rightarrow 0$으로 갈 때 $\tilde{\mathbf{x}} _T$의 distribution은 $p(\mathbf{x})$와 같아지게 된다. 

# Challenges of score-based generative modeling












Palette는 기본적으로 [DDPM](https://ee12ha0220.github.io/posts/DDPM/)과 동일한 구조를 사용하는데, 달라진 점이라면 input image를 prior로 사용한다는 점이다. Optimization에 사용되는 loss를 수식으로 나타내면 다음과 같다(\ref{eq1}).


<img src="/assets/images/palette_1.png" width="100%" height="100%">*Colorization 결과*

<img src="/assets/images/palette_2.png" width="100%" height="100%">*Inpainting 결과*

<img src="/assets/images/palette_3.png" width="100%" height="100%">*Uncropping 결과*

<img src="/assets/images/palette_4.png" width="100%" height="100%">*JPEG restoration 결과*

<img src="/assets/images/palette_5.png" width="100%" height="100%">*Multi-task learning 결과. 맨 오른쪽은 inpainting에 대해서만 학습된 model이다.*



# Conclusion

We present Palette, a simple, general framework for image-to-image translation. Palette achieves strong results on four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG restoration), outperforming strong GAN and regression baselines. Unlike many GAN models, Palette produces diverse and high fidelity outputs. This is accomplished without task-specific customization nor optimization instability. We also present a multi-task Palette model, that performs just as well or better over their task-specific counterparts. Further exploration and investigation of multi-task diffusion models is an exciting avenue for future work. This paper shows some of the potential of image-to-image diffusion models, but we look forward to seeing new applications.
