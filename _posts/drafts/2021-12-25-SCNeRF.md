---
title: "Self-Calibrating Neural Radiance Fields"
layout: post # do not change
current: post # do not change
cover: ../assets/images/SCNeRF_cover.png #../assets/images/[img]
use_cover: false
navigation: true
date: '2021-12-25'
tags: paper-review
class: post-template # do not change
subclass: post # do not change
author: saha # do not change
use_math: true # do not change
---

# Introduction

### Camera calibration

- Camera calibration is one of the crucial steps in computer vision, which was usually done by placing calibration objects(e.g., checkerboard pattern). 
- But always using calibration objects are cumbersome; therefore calibrating without any external objects, self calibration, is a important research topic. 
- Previous self calibration models have their limitations, and below is the brief summery of them. 
    1. They are usually based on pin hole camera model, due to reduce computational cost. So they cannot handle non-linear distortions in real cameras. 
    2. They solely rely on geometric loss(e.g., comparing interest points), so when the scene does not have enough interest points the results will be poor. 
    3. They used feature matching algorithm that could not improve or learn the geometry itself. 

### Aim of the paper

- They propose a self-calibration algorithm for generic camera models that learn parameters for the basic pinhole model, radial distortion, and non-linear camera noise in an end-to-end manner. 
- They also propose a geometric consistency designed for their camera model, and train the system together with the photometric consistency for self-calibration, which provides a large set of constraints. 
- These models can learn camera parameters(intrinsics, extrinsics), without using COLMAP. Also, they fine-tune the camera parameters which improves the underlying geometry and NVS. 

# Preliminary

- NeRF, NeRF++

# Differentiable self-calibrating cameras

### Basics

- Combined model of pinhole camera model, radial distortion, and a generic non-linear camera distortion
- A camera model is normally known as a mapping $\mathbf{p} = \pi (\mathbf{r})$. But in this paper, the unprojection function $\pi ^{-1}(\mathbf{p}) = \mathbf{r}(\mathbf{p})$ matters, so the term camera model and camera unprojection is used interchangeably. 
- A ray $\mathbf{r}(p)$ of a pixel $\mathbf{p}$ is represented as a pair of 3-vectors: direction vector $\mathbf{r}_d$ and an ray origin vector $\mathbf{r}_o$. 
- The camera unprojection process consists of two components: unprojection of pixels using a differentiable pinhole camera model, and generic non-linear ray distortions. 



