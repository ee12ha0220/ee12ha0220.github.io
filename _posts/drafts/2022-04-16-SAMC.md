---
title: Monte Carlo Denoising via Auxiliary Feature Guided Self-Attention
layout: post # do not change
current: post # do not change
cover: https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c003cd31-1f12-4506-8d25-af112fdcf560/Untitled.png
use_cover: false
navigation: true
# date: #'2022-04-07 19:00:00'
tags: paper-review
class: post-template # do not change
subclass: post # do not change
author: saha # do not change
use_math: true # do not change
---

# Advantage of using self-attention

- Self-attention can directly compute the interaction between any pair of features.
    - CNN can only interaction between features that are close to each other.
- Convolution is content-independent(response at every spatial position is obtained with the same set of weights), but self-attention is content-dependent.
- Self-attention is essentially a non-local means filtering in the embedding space, which is known to be effective in image denoising.

# Methodology

## Overall network

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5382c890-83a7-4797-b6d5-e14aef7753b2/Untitled.png)

- An auxiliary feature guided self-attention which computes the interactions between pixels in an image.

## Self-attention

- Self-attention is related to Transformer network architecture.
- Let $X^{i-1}$ be the input to the ith Transformer block, the self-attention operator is defined as:
    
    $$
    SA(X^{i-1}) = Softmax(\frac{QK^T}{\sqrt{d_k}})
    $$
    
    where, 
    
    $$
    Q = W_QX^{i-1}, K = W_KX^{i-1}, V = W_VX^{i-1}
    $$
    
- Using this self-attention operator, a Transformer block is defined as :
    
    $$
    \hat{X}^{i-1} = LN(SA(X^{i-1}))+X^{i-1} \\ X^i = LN(FFN(\hat{X}^{i-1})) + \hat{X}^{i-1}
    $$
    

## Multi-scale feature extractor

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2bcf6058-0642-4af1-acca-35fef3ab0860/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9ac2faa-8d05-40eb-8f61-721203497627/Untitled.png)

- Features calculated by a single convolution layer limit the performance of self-attention modules.
    - This is because features extracted from a single-scale receptive field cannot fully reflect the original information of the image.
- Using mult-scale feature extraction, the self-attention modules are provided with more raw features, facilitating the self-attention mechanism to discover more useful information.

## Auxiliary features guided self-attention

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d1bb52d6-ea6f-4402-875b-c8843e45591f/Untitled.png)

- After the mult-scale feature is extacted, 5 transformer blocks are stacked to extract more representitive features.
- There are few choices how to use auxiliary features
    - Not using them : This scheme yields relatively poor denoising results because of not using the helpful auxiliary information
    - Using concatenation : Both the input noisy image and the auxiliary buffers are used in the computation of $Q,K,V$. **But the auxiliary buffers should not be used in calculating $V$**.
        - Auxiliary buffers are more suited for creating edge-preserving weighting scores, i.e., $QK^T$
        - $V$ represents the image pixel values, so it should not be contaminated by the auxiliary features.
    - Using AFGSA operator : Use auxiliary buffers only in computing $Q,K$, but not $V$.