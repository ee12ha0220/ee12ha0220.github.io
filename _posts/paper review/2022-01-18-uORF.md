---
title: "Unsupervised Discovery of Object Radiance Fields"
cover: ../assets/images/uORF_cover.png
date: '2022-01-18'
tags: [paper-review]
math: true # do not change
mermaid: true
pin : false
---

# Introduction
- Object-centric representation is a constant topic of interest in computer vision and machine learning. 
- Such representation should bear three characteristics:
    - Should be learned in a unsupervised manner. 
    - Should explain the image formation process.
    - Should be 3D-aware, capturing geometric and physical object properties. 
- Until now, there is no work satisfying all these characteristics. 
- In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields. 
- During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision. 
- During testing, uORF infers the set of object radiance fields from an single image. 

# Methods

### Convolutional feature extraction
- Uses a convolutional net to extract features for the slot attention module. 
- Represents foreground object position and pose in the viewer coordinate system, in order to help learning the 3D object position and generalization. 
- So, feeds pixel coordinates and viewer-space ray direction as additional input channels to the encoder. 

### Background-aware slot attention
- Adopt [Slot Attention module]({{ site.baseurl }}OCLSA) to produce a set of permutation-invariant latent codes. 
- Since the geometry and appearance of the background are usually highly different from those of foreground objects, explicitly separate the foreground objects and background. 
- To achieve this, make a single slot that lie in a different latent space from the other slots, for background features. 

### Object-centric Encoding
- Using convolutional feature extraction and background-aware slot attention, infers latent object-centric representations from a single image. 
- Given $N$ input features with dimension $D$, the slots are initialized by sampling from two learnable Gaussians. $slot^b$ denotes a single slot for background, and $slots^f$ denotes the slots for foreground objects. 

    $$slot^b \sim \mathcal{N}^b(\mu^b, diag(\sigma^b)) \in \mathbf{R}^{1\times D}, \, slots^f \sim \mathcal{N}^f(\mu^f, diag(\sigma^f)) \in \mathbf{R}^{K \times D}$$

- The rest are similar to [Slot Attention module]({{ site.baseurl }}/posts/OCLSA). 
<img src="/assets/images/uORF_1.png" width="90%" height="90%"> 

### Compositional Neural Rendering
- Use a Conditional NeRF $g(\cdot \| z)$ that acts like an implicit decoder for each object. $z$ is the generated latent codes. 
- To compose individual objects and background into holistic scene, use a scene mixture model that uses density-weighted mean to combine all components. 

    $$\bar{\sigma} = \sum_{i=0}^K\omega_i\sigma_i, \, \bar{\mathbf{c}} = \sum_{i=0}^K\omega_i\mathbf{c}_i, \, \omega_i = \sigma_i/\sum_{j=0}^K\sigma_i$$

<img src="/assets/images/uORF_2.png" width="50%" height="50%"> 

### Loss functions
- Use reconstruction loss, perceptual loss, and adversarial loss. 
- Since 3D radiance fields are estimated from a single view, there can be uncertainties about the appearance from other views. Therefore, perceptual loss, which is tolerant to mild appearance changes, is used. 
- There can also exist multi-model distribution. Therefore, adversarial loss which can deal with multi-model distributions is used. 
- Reconstruction loss : $\mathcal{L}_{recon} = \|\|\mathbf{I} - \hat{\mathbf{I}}\|\|^2$ where $\mathbf{I}$ and $\hat{\mathbf{I}}$ denote the ground truth image and rendered image, respectively. 
- Perceptual loss : $\mathcal{L}_{percept} = \|\|p(\mathbf{I}) - p(\hat{\mathbf{I}})\|\|^2$ where $p$ is a deep feature extractor. 
- Adversarial loss : $\mathcal{L}_{adv} = \mathbb{E}[f(D(\hat{\mathbf{I}}))] + \mathbb{E}[f(-D(\mathbf{I})) + \lambda_R\|\| \nabla D(\mathbf{I})\|\|^2]$, where $f(t) = -log(1+exp(-t))$

### Coarse-to-fine Progressive Training
- Training compositional NeRF requires immense computational cost. To allow training on a higher resolution, coarse-to-fine progressive training is used. 
- In a coarse training stage, uORF is trained on the bilinearly downsampled images to a base resolution. 
- In the following fine training stage, the model is refined by training on patches randomly cropped from images of the higher target resolution. 

### The overall process
<img src="/assets/images/uORF_cover.png" width="90%" height="90%"> 

# Results
- Refer to [original paper](https://arxiv.org/abs/2107.07905)