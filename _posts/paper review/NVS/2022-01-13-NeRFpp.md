---
title: 'NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS'
# image: ../assets/images/NeRF_cover.png
date: '2022-01-13'
categories: [Paper review, NVS]
# tags: [paper-review]
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi- layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method im- proves view synthesis fidelity in this challenging scenario. Code is available at [this link](https://github.com/Kai-46/nerfplusplus).

# Introduction

[NeRF](https://ee12ha0220.github.io/posts/NeRF/)는 NVS 분야에서 뛰어난 성능을 보였다. 본 논문에서는 NeRF model의 성능이 좋은 이유에 대한 분석과, outdoor scene같은 unbounded scene에서 NeRF의 성능을 올리는 NeRF++ model을 제시했다. 

# Shape-Radiance Ambiguity

NeRF는 view-dependent하게 scene을 generate하기 때문에, 3D shape과 radiance사이의 ambiguity가 만족되지 않으면 degenerate solution이 도출될 가능성이 크다. 즉, input된 training data의 정확한 shape를 학습하지 못한다면, 학습 과정에서는 좋은 성능을 보일 수 있지만 novel view에서의 image는 이상하게 나올 수 있다. 

예를 들면 서로 다른 위치에서 같은 point를 바라보고 있는 2개의 ray를 생각해보자. 이 2개의 ray에서 관측되는 color값은 같은 point를 보고 있지만, object의 surface geometry 때문에 다른 값을 가진다. 하지만 이 두 ray는 viewing direction이 다르기 때문에, 색이 다른 것이 geometry가 달라서가 아니라 viewing direction이 다르기 때문에서라고 학습할 가능성이 있다. 이렇게 되면 전체 model이 incorrect shape을 기준으로 학습되며, 당연하게 novel view에서의 image는 이상하게 나오는 것이다. 

<img src="/assets/images/NeRFpp_1.png" width="50%" height="50%"> *잘못된 shape $\hat{S}$에 맞게 학습되면, 실제로 같은 곳을 바라보고 있는 $C_0$와 $C_1$이 다른 곳을 바라보고 있는 것을 알 수 있다.*

<img src="/assets/images/NeRFpp_2.png" width="100%" height="100%"> *Unit sphere로 shape를 고정하고 학습한 결과, training에서는 좋은 결과를 보이지만 testing에서 novel view에 대해 이상한 결과를 보인다.*

굳이 shape를 unit sphere로 고정해놓지 않더라도, 학습을 진행하는 과정에서 NeRF가 올바른 shape를 알아내지 못하고, 이상한 shape에 맞게 학습 될 가능성이 충분히 존재한다. 하지만 실제 NeRF를 학습하면 이러한 degenerate solution이 발생하지 않는다. NeRF++의 저자들은 이를 다음 2가지 이유로 설명한다. 

- 잘못된 shape으로 학습이 되면 그만큼 color가 high-dimensional하게 표현될 가능성이 크다. 왜냐하면 서로 같은 지점을 바라보는 2개의 viewpoint를 생각해 봤을 때, 옳은 shape이라면 그냥 1개의 값만 predict하면 되지만, 잘못된 shape이라면 서로 다른 지점을 바라보는 것으로 간주되어 그만큼 더 복잡한 prediction이 이루어져야 한다. NeRF에서 사용하는 MLP는 그러한 high-dimensional한 prediction을 하기에 더 어렵기 때문에, 이러한 경우가 잘 발생하지 않는다. 

- NeRF의 MLP 구조를 보면 position $\mathbf{x}$보다 direction $\mathbf{d}$가 MLP layer에 더 늦게 input되는 것을 알 수 있다. $\mathbf{d}$는 그만큼 학습에 큰 영향력을 행사하기 힘들기 때문에, 이로 인한 잘못된 prediction도 잘 발생하지 않는다. 

<img src="/assets/images/NeRF_2.png" width="100%" height="100%"> *NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공되었다.*

2번째 이유는 실제로 아주 큰 영향을 미치는데, NeRF의 MLP와 다르게 $\mathbf{d}$가 $\mathbf{x}$와 함께 처음부터 input되는 vanilla MLP의 경우 model의 성능이 현저하게 떨어졌다. 

<img src="/assets/images/NeRFpp_3.png" width="100%" height="100%"> *일반적인 MLP를 사용했을 때 성능이 현저하게 떨어진 것을 확인할 수 있다.*

# Handling unbounded scene

NeRF에서는 ray를 따라 volume rendering을 해서 최종 color를 구하는데, 이 식은 다음과 같다(\ref{eq1}).

---

$$
C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt, \text{ where } T(t) = \exp\left( -\int_{t_n}^t\sigma(\mathbf{r}(s))ds \right) \label{eq1} \tag{1}
$$

---

이 적분은 ray의 near bound $t_n$과 far bound $t_f$사이에서 sampling을 통해 얻은 sample들을 더하는 방식으로 discrete하게 바껴서 진행된다. 이는 $t_n$과 $t_f$의 차이가 크지 않은 bounded scene에서는 잘 먹히지만, $t_f$가 infinity인 unbounded scene의 경우에는 sampling이 너무 sparse하게 일어나서 좋은 결과가 나오지 않는다. 

<img src="/assets/images/NeRFpp_4.png" width="100%" height="100%"> *Bounded scene의 경우 dense한 sample들을 얻을 수 있다.*

<img src="/assets/images/NeRFpp_5.png" width="100%" height="100%"> *Unbounded scene에서는 sample들이 너무 sparse하다.*

NeRF++에서는 이를 해결하기 위해 ***Inverted sphere parametrization***을 제시한다. NeRF와 비슷하게 이 역시 2개의 MLP model을 사용하는데, 전체 scene을 foreground과 background으로 나눠서 각각 학습시킨다. Foreground과 background는 input image들의 camera를 모두 감싸는 sphere를 경계로 나누어진다. 

<img src="/assets/images/NeRFpp_6.png" width="50%" height="50%"> *Camera(초록색)를 모두 감싸는 sphere(빨간색)을 기준으로 foreground(sphere 안쪽)과 background(sphere 바깥쪽)이 나눠진다.*


## Foreground

Foreground는 bounded scene이기 때문에, NeRF과 같은 방식으로 sampling이 이루어진다. 

## Background

Background는 아직도 unbounded scene이기 때문에, inverted sphere parametrization을 적용해준다. Background에 속한 3D point는 다음과 같이 표현될 수 있다(\ref{eq2}). 

---

$$
\mathbf{x} = (x,y,z), r(\mathbf{x}) = \sqrt{x^2+y^2+z^2} \quad \rightarrow \quad \mathbf{x}^\prime = (x/r, y/r, z/r, 1/r) = (x^\prime, y^\prime, z^\prime, 1/r) \label{eq2} \tag{2}
$$ 

---

즉, 멀리 있는 point를 경계가 되는 sphere에 project 시킨 것으로 생각할 수 있다. 이때 sampling은 $1/r$을 기준으로 일어나게 되는데, $r > 1$이기 때문에 $0 < 1/r < 1$을 만족해 sparse sample들을 dense하게 만드는 효과가 있다. 이때 사실 $(x^\prime, y^\prime, z^\prime)$은 실제 projection과는 다른 점이지만, $\mathbf{x}$가 멀리 떨어진 점이기 때문에 실제 projection과 같다고 근사할 수 있다. 

<img src="/assets/images/NeRFpp_7.png" width="50%" height="50%"> *$(x^\prime, y^\prime, z^\prime)$는 실제 project된 point $a$와는 다르지만, $a$에 비해 계산하는 것이 훨씬 쉽고, $p$가 멀리 있기 때문에 같다고 근사할 수 있기 때문에 사용한다.*

## Volumetric rendering of NeRF++

결과적으로 NeRF++의 volumetric rendering은 다음과 같이 다시 쓸 수 있다(\ref{eq3}). 

---

$$
\begin{align}
    \mathbf{C}(\mathbf{r}) = &\underset{\text{foreground}}{\underbrace{\int_{t=0}^{t^\prime}\sigma(\mathbf{o}+t\mathbf{d})\cdot \mathbf{c}(\mathbf{o} + t\mathbf{d}, \mathbf{d})\cdot \exp \left(-\int_{s=0}^t\sigma(\mathbf{o}+s\mathbf{d})ds \right)dt}}\\
    &\underset{\text{background}}{\underbrace{+ \exp \left(-\int_{s=0}^{t^\prime}\sigma(\mathbf{o}+s\mathbf{d})ds \right)\cdot\int_{t=t^\prime}^{\infty}\sigma(\mathbf{o}+t\mathbf{d})\cdot \mathbf{c}(\mathbf{o} + t\mathbf{d}, \mathbf{d})\cdot \exp \left(-\int_{s=t^\prime}^t\sigma(\mathbf{o}+s\mathbf{d})ds \right)dt}} \label{eq3}\tag{3}
\end{align} 
$$

---

# Training details

NeRF와 거의 동일하다. 자세한 사항은 [NeRF++ 논문](https://arxiv.org/abs/2010.07492)을 참조하길 바란다. 

# Results

NeRF보다 더 발전된 결과를 보였다. 자세한 사항은 [NeRF++ 논문](https://arxiv.org/abs/2010.07492)을 참조하길 바란다. 

<img src="/assets/images/NeRFpp_8.png" width="80%" height="80%"> *NeRF++의 결과. NeRF보다 더 발전된 것을 확인할 수 있다.*

# Conclusion

NeRF++ improves the parameterization of unbounded scenes in which both the foreground and the background need to be faithfully represented for photorealism. However, there remain a number of open challenges. First, the training and testing of NeRF and NeRF++ on a single large-scale scene is quite time-consuming and memory-intensive. Training NeRF++ on a node with 4 RTX 2080 Ti GPUs takes ∼24 hours. Rendering a single 1280x720 image on one such GPU takes ∼30 seconds at test time. Liu et al. (2020) have sped up the inference, but rendering is still far from real-time. Second, small camera calibration errors may impede photorealistic synthesis. Robust loss functions, such as the contextual loss (Mechrez et al., 2018), could be applied. Third, photometric effects such as auto-exposure and vignetting can also be taken into account to increase image fidelity. This line of investigation is related to the lighting changes addressed in the orthogonal work of Martin-Brualla et al. (2020).
