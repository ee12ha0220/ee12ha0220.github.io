---
title: 'NeRF−−: Neural Radiance Fields Without Known Camera Parameters'
# cover: ../assets/images/NeRF--_1.png
date: '2022-01-23'
categories: [Paper review, NVS]
# tags: [paper-review]
author: saha
math: true # do not change
mermaid: true
pin : false
---

# Abstract

This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses or intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF−−, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including
both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF
benchmark, our model achieves novel view synthesis results on par with the baseline trained with COLMAP pre-computed camera parameters. We
also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP
fails, our model still produces robust results.

# Introduction

NeRF는 NVS에 뛰어난 성능을 보이지만, input image들의 camera pose, intrinsic을 알아야 model을 학습할 수 있다는 단점이 있다. Synthetic data에서는 이것이 큰 문제가 되지 않지만, 실제 카메라로 찍힌 real data에서는 정확한 pose와 intrinsic을 알아내는 것이 어렵다. NeRF에서는 structure-from-motion에 기반한 SFM colmap을 사용해서 이를 구했지만, 이는 100% 정확한 값은 아니었다. 본 논문(NeRF--)에서는 camera pose와 intrinsic을 모르는 상태에서도 학습 가능한 NeRF model을 제시했으며, camera parameter은 학습 과정에서 joint optimization을 통해 얻어진다고 한다. 

<img src="/assets/images/NeRFmm_1.png" width="90%" height="90%">*대략적인 학습 과정. Camera extrinsics(pose)와 intrinsics는 NeRF model과 함께 학습된다.*

# Method

NeRF--는 NeRF를 기반으로 만들어졌기 때문에, [NeRF](https://ee12ha0220.github.io/posts/NeRF/)에 대한 내용을 확인하는 것을 추천한다. 

NeRF--는 camera parameters들도 학습을 시켜야 하기 때문에, 다음과 같은 loss function을 사용한다(\ref{eq1}).

---

$$
\Theta^\star, \Pi^\star = \underset{\Theta, \Pi}{\text{argmin}}\,\mathcal{L}(\hat{\mathcal{I}}, \hat{\Pi}|\mathcal{I}) \label{eq1} \tag{1}
$$

---

이때 $\mathcal{I}$는 RGB image, $\Theta$는 NeRF model의 weight, $\Pi$는 camera parameters를 의미한다. 

## Camera parameters

먼저 camera intrinsics는 다음과 같이 정의된다(\ref{eq2}).

---

$$
K = \begin{pmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{pmatrix} \label{eq2} \tag{2}
$$

---

이때 $f_x$, $f_y$는 focal length, $c_x$, $c_y$는 principle point이다. 학습 과정에서 $c_x$, $c_y$는 각각 $W/2$, $H/2$로 고정되었다. 

Camera extrinsics(pose)는 다음과 같이 정의된다(\ref{eq3}).

---

$$
\mathbf{R} = \mathbf{I} + \frac{sin(\alpha)}{\alpha} \mathbf{\phi}^{\wedge} + \frac{1 - cos(\alpha)}{\alpha ^2} (\mathbf{\phi}^{\wedge})^2 \label{eq3}\tag{3}
$$

---

이는 axis-angle representation으로, $\mathbf{\phi} := \alpha\mathbf{\omega}$, $\mathbf{\phi} \in \mathbb{R}^3$는 rotation axis $\mathbf{\omega}$와 rotation angle $\alpha$의 곱이다. Skew operator $(\cdot)^\wedge$ 는 $\mathbf{\phi}$를 skew matrix으로 바꿔준다(\ref{eq4}). 

---

$$
\phi^{\wedge} = \begin{pmatrix} \phi_0 \\ \phi_1 \\ \phi_2 \end{pmatrix}^{\wedge} = \begin{pmatrix} 0 & -\phi_2 & \phi_1 \\ \phi_2 & 0 & -\phi_0 \\ -\phi_1 & \phi_0 & 0 \end{pmatrix} \label{eq4} \tag{4}
$$

---

## Joint optimization of NeRF and Camera Parameters

NeRF와 마찬가지로 각 training image $I_i$에 대해, 무작위로 $M$개의 pixel $(p_{i,m})_ {m=1}^M$을 선택해서, 각 pixel에 ray $\hat{\mathbf{r}}_{i,m}(h)$을 쏜다. Ray는 다음과 같이 정의된다(\ref{eq5}). 

---

$$
\begin{align}
    &\hat{\mathbf{r}}_{i,m}(h) = \hat{\mathbf{t}}_i + h\hat{\mathbf{d}}_{i,m} \\
    &\hat{\mathbf{d}}_{i,m} = \hat{\mathbf{R}}_i\begin{pmatrix} (u-W/2)/\hat{f}_x \\ -(v-H/2)/\hat{f}_y \\ -1 \end{pmatrix}
\end{align} \label{eq5} \tag{5}
$$

---

이후 과정은 NeRF와 동일하며, NeRF model과 함께 $\hat{pi}_i = (\hat{f}_x, \hat{f}_y, \hat{\mathbf{\phi}}_i, \hat{\mathbf{t}}_i)$를 학습한다. 


## Refinement of Camera Parameters

NeRF--의 저자들에 따르면 camera parameter들은 sub-optimal에 빠질 가능성이 크다고 한다. 그렇기 때문에 initialization이 중요한데, NeRF-- model을 조금 학습시킨 뒤 NeRF부분의 parameter은 다시 초기화하고, camera parameter부분은 그대로 유지하는 방법의 refinement를 사용한다고 한다. 

## Overall framework

<img src="/assets/images/NeRFmm_2.png" width="90%" height="90%">*NeRF--의 전체 framework.*

# Training details

NeRF와 비슷한 dataset을 사용했으며, evaluation metric또한 NeRF와 동일한 PSNR, SSIM, LPIPS를 사용했다. 더 자세한 사항은 [NeRF-- paper](https://arxiv.org/abs/2102.07064)을 참조하길 바란다.

# Result

먼저 SFM colmap이 잘 작동하는 scene에 대해서는 거의 유사한 결과를 보였다. 하지만 colmap에서는 pose와 intrinsic을 알아내지 못해서 NeRF에서는 NVS에 실패했지만, NeRF--에서는 효과적으로 이를 알아내어 NVS에 성공한 경우가 있었다. 더 자세한 사항은 [NeRF-- paper](https://arxiv.org/abs/2102.07064)을 참조하길 바란다.

<img src="/assets/images/NeRFmm_3.png" width="90%" height="90%">*SFM colmap이 잘 작동하는 scene에서는 거의 유사한 결과를 보인다.*

<img src="/assets/images/NeRFmm_4.png" width="90%" height="90%">*Rotation-dominant sequence에서는 NeRF에서 NVS에 실패한 반면, NeRF--에서는 성공한 것을 확인할 수 있다.*

# Conclusion

In this work, we present an end-to-end NeRF-based pipeline, called
NeRF−−, for novel view synthesis from sparse input views, which
does not require any information about the camera parameters for
training. Specifically, our model jointly optimise the camera parameters
for each input image while simultaneously training the NeRF
model. This eliminates the need of pre-computing the camera parameters
using potentially erroneous SfM methods (e.g. COLMAP) and
still achieves comparable view synthesis results as the COLMAPbased
NeRF baseline.We present extensive experimental results and
demonstrate the effectiveness of this joint optimisation framework
under different camera trajectory patterns, even when the baseline
COLMAP fails to estimate the camera parameters. Despite its current
limitations discussed above, our proposed joint optimisation
pipeline has demonstrated promising results on this highly challenging
task, which presents a step forward towards novel view
synthesis on more general scenes with an end-to-end approach.