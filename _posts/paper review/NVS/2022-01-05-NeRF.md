---
title: 'NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis'
# image: ../assets/images/NeRF_cover.png
date: '2022-01-05'
categories: [Paper review, NVS]
# tags: [paper-review]
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate(spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose
output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses.We describe how to
effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.

# Introduction

Novel view synthesis(NVS)는 특정한 scene에 대한 input이 주어졌을 때, 그 scene을 학습하는 것이다. 즉, input들과는 다른 임의의 viewpoint에서 scene을 봤을 때 어떻게 보일지를 예측하는 것을 목표로 한다. 본 논문(NeRF)에서는 MLP를 사용해서 5D input(color + viewing direction)에서 3D view-dependent color을 만들어내는 것을 목표로 한다. 

NeRF의 간단한 과정은 다음과 같다. 
1. Pixel마다 ray를 쏴서 3D point를 ray를 따라 sampling 한다.
2. Sample된 point와 해당하는 2D viewing direction을 MLP에게 feed해서 color과 density값을 학습한다. 
3. 구한 color과 density에 volume rendering을 적용해 2D image를 만들어낸다.

<img src="/assets/images/NeRF_1.png" width="100%" height="100%"> *NeRF의 대략적인 scheme.*

# Neural Radiance Field Scene Representation

## MLP network

NeRF에서는 특정한 scene을 3D location $\mathbf{x} = (x,y,z)$와 2D viewing direction $(\theta, \phi)$를 이용해서 나타내고, 이를 MLP network에 input으로 줘서 color $\mathbf{c} = (r,g,b)$와 volume density $\sigma$를 구한다. 이를 식으로 나타내면 다음과 같다(\ref{eq1}).

---

$$
F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma) \label{eq1} \tag{1}
$$

---

<img src="/assets/images/NeRF_2.png" width="100%" height="100%"> *NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공된 것을 알 수 있는데, 이는 [다른논문](https://ee12ha0220.github.io/posts/NeRFpp/)에서 자세히 설명하고 있다.*


## Volume rendering

MLP network을 통해 구한 color $\mathbf{c}(\mathbf{x})$와 volume density $\sigma(\mathbf{x})$는 volume rendering을 통해 실제 color value $C(\mathbf{r})$로 변환된다. Volume rendering은 $\mathbf{r}$을 origin $\mathbf{o}$와  viewing direction $\mathbf{d}$ 을 이용해 $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$로 표현했을 때, 다음과 같이 표현된다(\ref{eq2}).

---

$$
C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt, \text{ where } T(t) = \exp\left( -\int_{t_n}^t\sigma(\mathbf{r}(s))ds \right) \label{eq2} \tag{2}
$$

---

이때 $t_n, t_f$는 scene의 near, far boundary가 된다. 실제로 적분을 하려면 위처럼 continuous 한 식이 아니라, discrete한 형태로 바꿔줘야 한다. 이를 위해서 NeRF에서는 $t_n$과 $t_f$사이에서 stratified sampling을 통해 $N$개의 discrete한 sample을 만든다(\ref{eq3}). 

---

$$
t_i \sim \mathcal{U}\left[ t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n) \right] \label{eq3} \tag{3}
$$

---

위 sample들을 이용해 equation \ref{eq2}를 다시 쓰면 다음과 같다(\ref{eq4}).

---

$$
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))\mathbf{c}_i, \text{ where } T_i = \exp\left( -\sum_{j=1}^{i-1}\sigma_j\delta_j \right) \label{eq4} \tag{4}
$$

---

이때 $\delta_i = t_{i+1} - t_i$로, 인접한 sample간의 거리를 의미한다. 

<img src="/assets/images/NeRF_3.png" width="100%" height="100%"> *NeRF의 학습 과정.*

# Optimizing NeRF

위의 방법대로 NeRF를 학습시켰을 때는 좋은 결과가 나오지 않았다고 한다. NeRF의 저자들은 여러 방법을 통해 NeRF의 성능을 끌여올렸다. 

## Positional encoding

MLP network $F_\Theta$는 $xyz\theta\phi$의 5D input만으로는 좋은 성능을 보이지 못한다. 그렇기 때문에 **positional encoding** $\gamma$를 이용해 input의 dimension을 증가시켜 줬다(\ref{eq5}).

---

$$
\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), ..., \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p))
$$

---

본 논문에서는 $\gamma(\mathbf{x})$에는 $L=10$을, $\gamma(\mathbf{d})$에는 $L=4$를 사용했다. 

## Hierarchical volume sampling

