---
title: 'NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis'
# image: ../assets/images/NeRF_cover.png
date: '2021-11-19'
categories: [Paper review, NVS]
# tags: [paper-review]
author: saha
math: true
mermaid: true
pin : false
---

# Introduction
### Novel view synthesis

- Novel view synthesis(NVS) is generating a 3D scene given a certain input. The input can vary, but normally it was the current location (x,y,z). 

### Aim of this paper

- In this paper, along with 3D location the 2D viewing direction was also used as input, resulting in NVS of good quality.
- Specifically, MLP and volume rendering was used in this paper. Also, positional encoding and hierarchical sampling was used to improve performance. 

# Overall pipeline
![](/assets/images/NeRF_1.png) 

1. March camera rays through the scene to generate a sampled set of 3D points(stratified sampling). 
2. Use those points and their corresponding 2D viewing directions as input to the neural network to produce and output set of colors and densities. 
3. Use classical volume rendering techniques to accumulate those colors and densities into a 2D image. 
4. Use gradient descent to optimize the model by minimizing error between the ground truth and generated image.
5. Enforce performance by transforming input 5D coordinates with a positional encoding to a higher dimensional space that enables MLP to represent higher frequency functions
6. Use a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.

# Methods
### MLP

- MLP network can be expressed as below
    
    $$ F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma) $$
    
- σ is predicted only using location x, while c is predicted using both x and d.
- First, process x with 8 FCNs, using ReLU and 256 channels per layer, which outputs σ and 256-dimensional feature vector.
- Feature vector is concatenated with the camera ray’s viewing direction and passed to one additional FCN(ReLU + 128 channels), which outputs view-dependent RGB color.

### Volume rendering

- Volume density $ \sigma(x) $ can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location $ x $.
- Expected color $ C(r) $ of camera ray $ r(t) = o + t_d $ can be expressed as below.
    
    $$C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt$$
    
    $$T(t) = exp(-\int_{t_n}^t \sigma(\mathbf{r}(s))ds)$$
    
- $T$ is the accumulated transmittance along  the ray, the probability that the ray travels from $t_n$ to $t$ without hitting any other particle.

### Ray sampling 

- The continuous integral in Volume rendering is numerically estimated using quadrature. 
- Use stratified sampling to partition [$t_n$, $t_f$] into $N$ evenly-spaced bins and then draw one sample uniformly at random from within each bin. 

    $$t_i \sim \mathcal{U} \left[ t_n + \frac{i-1}{N} (t_f - t_n) , \, t_n + \frac{i}{N} (t_f - t_n) \right]$$

- $C(t)$ can be changed to discrete form using this sampling. 

    $$\hat{C}(r) = \sum\limits_{i=1}^NT_i(1-exp(-\sigma_i\delta_i))\mathbf{c}_i$$
    
    $$T_i = exp(-\sum\limits_{j=1}^{i-1}\sigma_j\delta_j), \, \delta_i = t_{i+1} - t_i$$

### Positional encoding

- Having the network directly operate on 5D input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry.
- Rahaman et al. → deep networks are biased towards learning lower frequency functions, mapping inputs to a higher dimensional space enables better fitting of data that contains high frequency variation.
    
    $$F_\Theta = F^{\prime}_\Theta\cdot\gamma$$

    $$\gamma(p): \mathbb{R} \rightarrow \mathbb{R}^{2L} = (sin(2^0\pi p), cos(2^0\pi p), ..., sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))$$

    Where $F^{\prime}_\Theta$ is Regular MLP. 
    
- x and d are given different L (L = 10 for x, L = 4 for d in this paper)

### Hierarchical volume sampling

- Densely evaluating the neural radiance field network at N query points along each camera ray is inefficient, because free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly.
- Using hierarchical representation that allocating samples proportionally to their expected effect will improve efficiency.
- Instead of using one network, use two networks: 'coarse' and 'fine'.
- First sample $N_c$ locations and evaluate 'coarse' network. It can be represented as below.
    
    $$\hat{C}_c(\mathbf{r}) = \sum\limits _{i=1}^{N_c} \omega _i c_i, \, \omega _i = T_i(1-exp(-\sigma_i\delta _i))$$
    
- Normalizing the weights will produce a piecewise-constant PDF along the ray, and $N_f$ locations will be sampled from this distribution. 'fine' network will be evaluated using $N_c + N_f$ samples.

### Implementation details
- The camera poses and intrinsic parameters are obtained using SFM colmap. 
- At each optimization iteration, a batch of camera rays are sampled from the set of all pixels in the dataset, and followed by the hierarchical sampling. 
- The loss is simply the total squared error between the rendered and true pixel colors for both coarse and fine renderings: 

    $$\mathcal{L} = \sum\limits _{r\in \mathcal{R}} \left[\lVert \hat{C}_c(\mathbf{r}) - C(\mathbf{r}) \rVert _2 ^2 + \lVert \hat{C}_f(\mathbf{r}) - C(\mathbf{r}) \rVert _2 ^2 \right]$$ 

### Results
- Refer to [original paper](https://arxiv.org/abs/2003.08934)