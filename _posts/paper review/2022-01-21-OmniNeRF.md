---
title: "Moving in a 360 World : Synthesizing Panoramic Parallaxes from a Single Panorama(OmniNeRF)"
# cover: ../assets/images/OmniNeRF_cover.png
date: '2022-01-21'
tags: [paper-review]
author: saha
math: true # do not change
mermaid: true
pin : false
---

# Introduction
- Synthesizing novel views provides immersive 3D experiences, but the needed cost of computing power is very high in both time and capacity. 
- While many techniques are proposed to synthesize novel views by taking the perspective image(s) as the input, prior work rarely considers the panorama image as a single source for modeling and rendering. 
    - Perspective images can be obtained easily. But to construct the whole scene, very dense samples are required. 
    - Panorama images have more data of a scene in a single image, so using less images will be achievable. 
- In this paper, they propose OmniNeRF, which uses a single panorama image with depth value to synthesize a novel view of the whole scene. 
- Also in reconstruction, training with merely a single image is apparently not sufficient. So they proposed a method to create images form different views from a single image. 

# Related work
- This paper is based on [NeRF](https://ee12ha0220.github.io/posts/NeRF/)

# Methods
### Input data
- They use a single panorama image(omnidirectional image), with depth value. (RGB-D panorama image)

### Generating training samples
- Generating novel views using only one image is very challenging. 
    - In previous works, multi-view data with known camera parameters were mostly used. 
    - This paper adopted a similar process, by simulating multi-view images from the single RGB-D panorama image. 
- All pixels are mapped to a uniform sphere by their 2D coordinates. 
    - The coordinate center will be the current camera position, namely the ray origin. 
    - The ray direction means a unit vector from the center to the sphere. 
- A novel panoramic view can be obtained by moving the camera to a new position and sampling on a new unit sphere. 
    - To get the actual training image, these points have to projected back to the original pose. 

### Visibility
- In transforming the image to a different pose, a ray from a translated camera view has a chance to "see through" the sparse points of an obstacle and reach a point that was visible to the original view(but not to the current view).
- To handle this, median filter is applied to the depth map of the translated view, and pixels with depth value larger then the local minimum depth multiplied by a tolerance ratio are filtered out. 
<img src="/assets/images/OmniNeRF_1.png" width="100%" height="100%"> 

### Regressing with gradient
- Since the given training data are not dense enough to cover all areas of the scene, rendering images at new positions forces the model to predict some regions that the model has never seen before. 
- Use additional loss term to make the model learn to interpolate from one pixel to another according to ray origin and direction information. 