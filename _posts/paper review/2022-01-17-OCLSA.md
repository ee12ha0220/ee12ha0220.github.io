---
title: Object-Centric Learning with Slot Attention
# cover: ../assets/images/OCLSA_1.png
date: '2022-01-17'
tags: [paper-review]
author: saha
math: true # do not change
mermaid: true
pin : false
---

# Introduction
- Object-centric representations have the potential to improve sample efficiency and generalization of machine learning algorithms across a range of application domains. 
- However, obtaining object-centric representations from raw perceptual input, such as an image or a video, is challenging and often requires either supervision or task-specific architectures. 
- To overcome this challenge, this paper introduce the Slot Attention module, a differentiable interface between perceptual representations (e.g., the output of a CNN) and a set of variables called slots. 

# Methods

### Slot Attention Module

- Maps from a set of $N$ input feature vectors to a set of $K$ output vectors called 'slots'. 
- Iterative attention mechanism is used to map the inputs to the slots: slots are initialized at random and refined at each iteration. 
- At each iteration, slots compete for explaining pars via softmax-based attention mechanism and update there representation using a recurrent update function. 
- Slot Attention uses a dot-product attention with coefficients normalized over the slots. 
- Overall algorithm: 
<img src="/assets/images/OCLSA_2.png" width="100%" height="100%"> 
- The figure below shows how the Slot Attention is trained. 
- There are 4 slots, for each objects and the background. 
- In each iterations, the most dominant object is mapped to the slot. 
<img src="/assets/images/OCLSA_3.png" width="100%" height="100%"> 

### Object Discovery
- Unsupervised
- The order of which object will go into which slot do not matter. 
- Encoder : CNN backbone augmented with positional embeddings + slot attention module
- Decoder : Using spatial broadcast decoder, slot representations are broadcasted onto a 2D grid and augmented with position embeddings. 

### Set Prediction
- For a set of $K$ elements, there can be $K!$ possible equivalent representations. 
- Encoder : Similar to object discovery
- Classifier : For each slots, apply MLP with parameters shared between slots. Next, match them using the Hungarian algorithm. 

### Overall Pipeline
<img src="/assets/images/OCLSA_1.png" width="100%" height="100%"> 

# Results
- Refer to [original paper](https://arxiv.org/abs/2006.15055)