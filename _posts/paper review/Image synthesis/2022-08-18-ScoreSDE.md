---
title: "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS"
# image : ""
date: '2022-08-18'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024 \times 1024$ images for the first time from a score-based generative model.

# Introduction

Probabilistic generative model로, data에 작은 noise를 계속 줘서 corrupt시킨 다음, 이 corruption의 reverse process를 학습해 clean image를 얻는 것을 목표로 한다. Score matching with Langevin dynamics(SMLD) 은 각 noise scale에서의 score를 estimate하고, Langevin dynamics를 이용해서 decreasing noise scale의 sequence에서 sampling을 해서 clean image를 얻는다. Denoising diffusion probabilistic model(DDPM)은 noise corruption의 backward process를 적용하기 위한 probabilistic model을 학습시킨다. DDPM도 SMLD와 마찬가지로 특정 noise scale에서 일종의 score를 계산하기 때문에, 이 두 model은 모두 score-based generative model의 범주에 들어간다. 본 논문(ScoreSDE)에서는 score-based generative model을 일반화하기 위해 ***stochastic differential equations(SDEs)***를 사용한다. 

기존의 방법들처럼 data를 유한한 noise distribution들을 이용해 perturb시키는 것이 아니라, ScoreSDE에서는 diffusion process에 알맞은 연속적인 SDE를 사용한다. 이는 기존의 방법들처럼 data와 상관 없이 시간이 지나면 random noise로 보내고, reverse SDE도 구할 수 있다. 


<img src="/assets/images/ScoreSDE_1.png" width="80%" height="80%">*ScoreSDE의 key idea.*

추가적으로, 적절한 SDE를 선택하면 SMLD와 DDPM도 ScoreSDE에 포함될 수 있고, 더 좋은 architecture과 sampling algorithm을 이용해 기존의 연구들보다 더 좋은 결과를 얻을 수 있었다고 한다. 

# Background

## Denoising score matching with Langevin Dynamics(SMLD)

Data distribution $p_{data}$에 대해, perturbation kernel $p_\sigma(\tilde{\mathbf{x}}\|\mathbf{x}) := \mathcal{N}(\tilde{\mathbf{x}};\mathbf{x},\sigma^2 \mathbf{I})$, noise scale distribution $p_\sigma(\tilde{\mathbf{x}}) := \int p_{data}(\mathbf{x})p_\sigma(\tilde{\mathbf{x}}\|\mathbf{x})d\mathbf{x}$를 정의할 수 있다. 이는 noise scale의 sequence $\sigma_{min} = \sigma_1 < \sigma_2 < ... < \sigma_N = \sigma_{max}$에 따라 결정되는데, $\sigma_{min}$은 $p_{\sigma_{min}}(\mathbf{x}) \approx p_{data}(\mathbf{x})$ 을 만족할 정도로 작은 값이고, $\sigma_{max}$는 $p_{\sigma_{max}}(\mathbf{x}) \approx \mathcal{N}(\mathbf{x};\mathbf{0},\sigma_{max}^2 \mathbf{I})$를 만족할 정도로 크다. 학습은 Noise Conditional Score Network(NCSN) $\mathbf{s}_\theta(\mathbf{x}, \sigma)$를 다음과 같이 weighted sum of denoising score matching을 이용해 학습시켰다(\ref{eq1}).

---

$$
\mathbf{\theta}^\star = \underset{\mathbf{\theta}}{\text{argmin}} \sum_{i=1}^N \sigma_i^2 \mathbb{E}_{p_{data}(\mathbf{x})} \mathbb{E}_{p_{\sigma_i}(\tilde{\mathbf{x}}|\mathbf{x})} || \mathbf{s}_{\mathbf{\theta}}(\tilde{\mathbf{x}}, \sigma_i) - \nabla_{\tilde{\mathbf{x}}} \log p_{\sigma_i}(\tilde{\mathbf{x}} | \mathbf{x}) ||^2_2 \label{eq1} \tag{1}
$$

---

충분한 양의 data와 model capacity가 주어진다면, optimal score-based model $\mathbf{s}_{\mathbf{\theta}^\star}(\mathbf{x}, \sigma)$는 $\nabla _{\mathbf{x}} \log p _{\sigma}(\mathbf{x})$ 와 거의 모든 부분에서 일치하게 될 것이다. Sampling 단계에서는 Langevin MCMC를 $M$번 시행에서 $p _{\sigma_i}(\mathbf{x})$를 sequential하게 얻었다(\ref{eq2}).

---

$$
\mathbf{x}_i^m = \mathbf{x}_i^{m-1} + \epsilon_i \mathbf{s}_{\mathbf{\theta}^\star}(\mathbf{x}_i^{m-1}, \sigma_i) + \sqrt{2\epsilon_i}\mathbf{z}_i^m, \quad m = 1,2,..., M \label{eq2} \tag{2}
$$

---

이때 $\epsilon_i>0$는 step size이고, $\mathbf{z}_i^m$는 standard normal이다. 

## Denoising Diffusion Probabilistic Models(DDPM)

Data distribution $p_{data}$에 대해, discrete Markov chain $(\mathbf{x}_0, \mathbf{x}_1, ..., \mathbf{x}_N)$을 $p(\mathbf{x} _i \| \mathbf{x} _{i-1}) = \mathcal{N}(\mathbf{x} _i;\sqrt{1-\beta _i}\mathbf{x} _{i-1}, \beta _i\mathbf{I})$ 의 형태로 정의하고, 결과적으로 $p _{\alpha _i}(\mathbf{x}_i \| \mathbf{x}_0) = \mathcal{N}(\mathbf{x} _i;\sqrt{\alpha _i}\mathbf{x} _{i-1}, (1 - \alpha _i)\mathbf{I})$ 가 된다. 이때 $\beta _i$는 corruption을 위한 0과 1 사이의 schedule이고, $\alpha _i = \prod _{j=1} ^i (1 - \beta _j)$ 이다. SMLD와 비슷하게 $p _{\alpha _i}(\tilde{\mathbf{x}}) := \int p _{data}(\mathbf{x})p _{\alpha _i}(\tilde{\mathbf{x}}\|\mathbf{x})d\mathbf{x}$로 쓸 수 있고, Markov chain 의 reverse direction을 $p _\mathbf{\theta}(\mathbf{x} _{i-1} \| \mathbf{x} _{i}) = \mathcal{N} (\mathbf{x} _{i-1} ; \frac{1}{\sqrt{1-\beta _i}}(\mathbf{x} _{i} + \beta _i \mathbf{s} _\mathbf{\theta} (\mathbf{x} _{i}, i)), \beta _i \mathbf{I})$ 로 reparameterize 할 수 있다. 학습은 evidence lower bound(ELBO) loss를 이용해 진행된다(\ref{eq3}).

---

$$
\mathbf{\theta}^\star = \underset{\mathbf{\theta}}{\text{argmin}} \sum_{i=1}^N (1 - \alpha_i) \mathbb{E}_{p_{data}(\mathbf{x})} \mathbb{E}_{p_{\alpha_i}(\tilde{\mathbf{x}}|\mathbf{x})} || \mathbf{s}_{\mathbf{\theta}}(\tilde{\mathbf{x}}, i) - \nabla_{\tilde{\mathbf{x}}} \log p_{\alpha_i}(\tilde{\mathbf{x}} | \mathbf{x}) ||^2_2 \label{eq3} \tag{3}
$$

---

Sampling은 $\mathbf{x}_N \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$에서 부터 reverse Markov chain을 이용해 진행된다(\ref{eq4}).

---

$$
\mathbf{x}_{i-1} = \frac{1}{\sqrt{1-\beta_i}}(\mathbf{x}_{i} + \beta_i \mathbf{s}_{\mathbf{\theta}^\star}(\mathbf{x}_i,i)) + \sqrt{\beta_i} \mathbf{z}_i, \quad i = N, N-1, ..., 1 \label{eq4} \tag{4}
$$

---

전체적으로 SMLD와 비슷한 느낌으로 진행되는 것을 알 수 있다. 

# Score-Based Generative Modeling with SDEs

## Perturbing Data with SDEs

ScoreSDE의 목적은 data distribution $p_0$와 prior distribution $p_T$에 대해, continuous time variable $t \in [0,T]$로 indexing이 가능하고 $\mathbf{x}(0) \sim p_0$, $\mathbf{x}(T) \sim p_T$를 만족하는 diffusion process $[\mathbf{x}(t)]_ {t=0}^ T$를 construct 하는 것이다. 이는 일반인 SDE의 solution을 이용해서 modeling 할 수 있다(\ref{eq5}).

---

$$
\text{d}\mathbf{x} = \mathbf{f}(\mathbf{x},t) \text{d}t + g(t)\text{d}\mathbf{w} \label{eq5} \tag{5}
$$

---

이때 $\mathbf{w}$는 standard Wiener process(Brownian motion), $\mathbf{f} : \mathbb{R}^d \rightarrow \mathbb{R}^d$는 $\mathbf{x}(t)$의 drift coefficient, $\mathbf{g} : \mathbb{R} \rightarrow \mathbb{R}$은 $\mathbf{x}(t)$의 diffusion coefficient이다. 추가적으로, $\mathbf{x}(t)$의 probability density를 $p _t(\mathbf{x})$로, $\mathbf{x}(s)$에서 $\mathbf{x}(t)$로 가는 transition kernel을 $p _{st}(\mathbf{x}(t) \| \mathbf{x}(s))$로 정의한다. 이때 $p _T$는 $p _0$에 대한 아무런 정보도 담고 있지 않은 distribution(예를 들면 Gaussian)으로 설정된다. 

## Generating Samples by Reversing the SDE

Sampling은 [Reverse-time diffusion equation models](https://www.sciencedirect.com/science/article/pii/0304414982900515)에 의거한 reverse-time SDE를 이용해 실행할 수 있다(\ref{eq6}).

---

$$
\text{d}\mathbf{x} = [\mathbf{f}(\mathbf{x},t) - g(t)^2 \nabla_{\mathbf{x}}\log p_t(\mathbf{x})]\text{d}t + g(t)\text{d}\bar{\mathbf{w}} \label{eq6} \tag{6}
$$

---

이때 $\bar{\mathbf{w}}$는 $\mathbf{w}$과 마찬가지로 standard Wiener process이다. 

## Estimating Scores for the SDE

$\nabla_{\mathbf{x}}\log p_t(\mathbf{x})$ 를 estimate하기 위해, time-dependent score-based model $\mathbf{s} _\mathbf{\theta} (\mathbf{x}, t)$를 학습시킨다. 이는 equation\ref{eq1} \ref{eq3}의 일반화된 형태이다(\ref{eq7}).

---

$$
\mathbf{\theta}^\star = \underset{\mathbf{\theta}}{\text{argmin}} \mathbb{E}_t \left( \lambda_t \mathbb{E}_{\mathbf{x}(0)} \mathbb{E}_{\mathbf{x}(t)|\mathbf{x}(0)} || \mathbf{s}_{\mathbf{\theta}}(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) | \mathbf{x}(0)) ||^2_2 \right) \label{eq7} \tag{7}
$$

---

이때 $\lambda : [0,T] \rightarrow \mathbb{R} _{>0}$는 positive weighting function이고, $\mathbf{x}(0) \sim p _0(\mathbf{x})$, $\mathbf{x}(t) \sim p _{0t}(\mathbf{x}(t) \| \mathbf{x}(0))$이다. SMLD와 DDPM의 경우에는, $\lambda \propto 1/\mathbb{E}[\|\| \nabla _{\mathbf{x}(t)} \log p _{0t} (\mathbf{x}(t) \| \mathbf{x}(0)) \|\| ^2 _2]$ 으로 설정되었다. 

Equation \ref{eq7}를 풀려면 transition kernel $p _{0t}(\mathbf{x}(t) \| \mathbf{x}(0))$에 대한 정보가 있어야 한다. $\mathbf{f}$가 affine한 경우에 transition kernel은 항상 Gaussian이 되며, mean과 variance는 존재하는 여러 방법들을 이용해 구할 수 있다고 한다. 더 일반적인 경우는 **Kolmogorov's forward equation**를 풀면 된다고 한다. 

## Modeling SMLD, DDPM using ScoreSDE

$N$개의 noise scale을 사용할 때, SMLD의 각 perturbation kernel $p _{\sigma _i}(\mathbf{x} \| \mathbf{x} _0)$는 다음과 같이 쓸 수 있다(\ref{eq8}).

---

$$
\mathbf{x}_i = \mathbf{x}_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \mathbf{z}_{i-1}, \quad i = 1,2,..., N \label{eq8} \tag{8}
$$

---

이때 $N \rightarrow \infty$가 되면, $\sigma _i$는 함수 $\sigma(t)$가 되고, $\mathbf{z} _i$는 $\mathbf{z}(t)$, Markov chain $\mathbf{x} _i$는 continuous stochastic process $\mathbf{x}(t)$가 된다. $\mathbf{x}(t)$는 다음 SDE를 이용해서 쓸 수 있다(\ref{eq9}).

---

$$
\text{d}\mathbf{x} = \sqrt{\frac{\text{d}[\sigma^2(t)]}{\text{d}t}} \text{d} \mathbf{w} \label{eq9} \tag{9}
$$

---

이와 마찬가지로 DDPM도 각 perturbation kernel 을 다음과 같이 쓸 수 있다(\ref{eq10}).

---

$$
\mathbf{x}_i = \sqrt{1-\beta_i} \mathbf{x}_{i-1} + \sqrt{\beta_i} \mathbf{z}_{i-1}, \quad i = 1,2,..., N \label{eq10} \tag{10}
$$

---

$N \rightarrow \infty$가 되면, 이 역시 다음 SDE를 이용해서 쓸 수 있다(\ref{eq11}).

---

$$
\text{d}\mathbf{x} = -\frac{1}{2} \beta(t)\mathbf{x} \text{d}t + \sqrt{\beta(t)} \text{d}\mathbf{w} \label{eq11} \tag{11}
$$

---

더 자세한 사항은 [ScoreSDE 논문](https://arxiv.org/abs/2011.13456)의 ***Appendix B***를 참고하길 바란다. 

# Solving the Reverse SDE

***내용이 정말 방대하다. [ScoreSDE 논문](https://arxiv.org/abs/2011.13456)을 참고하는 것이 좋을 것 같다.***

# Controllable Generation

ScoreSDE의 framework은 $p _0$에서 data sample을 만들어내는 것 뿐만 아니라, $p _t(\mathbf{y} \| \mathbf{x}(t))$를 알고있다면 $p _0(\mathbf{x}(0) \| \mathbf{y})$에서도 data sample을 만들어낼 수 있다. 이는 다음 reverse-time SDE를 풀면 얻을 수 있다(\ref{eq12}).

---

$$
\text{d}\mathbf{x} = \left\{ \mathbf{f}(\mathbf{x},t) - g(t)^2[\nabla _\mathbf{x} \log p_t(\mathbf{x}) + \nabla_{\mathbf{x}} \log p_t (\mathbf{y} | \mathbf{x})] \right\} \text{d}t + g(t)\text{d}\bar{\mathbf{w}} \label{eq12} \tag{12}
$$

---

# Result

자세한 사항은 [ScoreSDE 논문](https://arxiv.org/abs/2011.13456)의 를 참고하길 바란다.

<img src="/assets/images/ScoreSDE_2.png" width="100%" height="100%">*Class-conditional samples(왼쪽)와 inpainting, colorization같은 image-to-image task(오른쪽)에 모두 좋은 성능을 보인다.*



# Conclusion

We presented a framework for score-based generative modeling based on SDEs. Our work enables a
better understanding of existing approaches, new sampling algorithms, exact likelihood computation,
uniquely identifiable encoding, latent code manipulation, and brings new conditional generation
abilities to the family of score-based generative models.
While our proposed sampling approaches improve results and enable more efficient sampling, they
remain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying
ways of combining the stable learning of score-based generative models with the fast sampling of
implicit models like GANs remains an important research direction. Additionally, the breadth of
samplers one can use when given access to score functions introduces a number of hyper-parameters.
Future work would benefit from improved methods to automatically select and tune these hyperparameters,
as well as more extensive investigation on the merits and limitations of various samplers.
