---
title: "Denoising Diffusion Probabilistic Models"
# image : ""
date: '2022-07-25'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

<img src="/assets/images/DDPM_1.png" width="100%" height="100%"> 
# Abstract

We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat- urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [https://github.com/hojonathanho/diffusion](https://github.com/hojonathanho/diffusion).

# Introduction

- A diffusion probabilistic model(diffusion model) is a **parameterized Markov chain** trained using variational inference to produce samples matching the data after finite time.
    - [Markov chain](https://brilliant.org/wiki/markov-chains/) : A Markov chain is aÂ stochastic process, but it differs from a general stochastic process in that a Markov chain must be "memory-less." That is, (the probability of) future actions are not dependent upon the steps that led up to the present state.
    - The Markov chain gradually adds noise to the data(diffusion process),  and the aim is to learn its reverse process, which is denoising.
    
<img src="/assets/images/DDPM_2.png" width="100%" height="100%"> 

# Background

## Diffusion process

- The diffusion process is a Markov chain that **gradually adds Gaussian noise** to the data according to a variance schedule $\beta_1,...,\beta_T$ :
    
    $$
    q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I}), \quad q(\mathbf{x}_{1:T}|\mathbf{x}_0) := \prod_{t=1}^Tq(\mathbf{x}_t|\mathbf{x}_{t-1})
    $$
    
    - The term $\mathbf{x}_{1:T}$ means a process from $1$ to $T$.
    - The forward process variances  $\beta_t$ can be learned by reparameterization, or held constant as hyperparameters.
- The original data $\mathbf{x}_0$ gradually loses its feature as the step $t$ becomes larger, and in the end converges to gaussian noise, $\mathbf{x}_T$.

### Reparameterization of diffusion process

- Since the diffusion process is a markov chain, we can obtain $\mathbf{x}_t$ at any arbitrary time step $t$ using a reparameterization trick :
    
    $$
    \mathbf{x}_t = \sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\mathbf{\epsilon}, \quad \alpha_t = 1-\beta_t, \quad\bar\alpha_t = \prod_{i=1}^t\alpha_i, \quad \mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0,I}) \quad
    $$
    
    - This can be obtained by simply defining $\mathbf{x}_t$ recursivly.
    - $\mathbf{x}_0$ can be obtained by modifying the above equation :
        
        $$
        \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}\mathbf{x}_t - \sqrt{\frac{1}{\bar{\alpha}_t}-1}\cdot \epsilon
        $$
        

## Reverse process

- If $\mathbf{x_0}$ is given, the reverse conditional probability is tractable :
    
    $$
    q(\mathbf{x_{t-1}}|\mathbf{x_t},\mathbf{x_0}) = \mathcal{N}(\mathbf{x_{t-1}};\tilde{\mu}(\mathbf{x_t},\mathbf{x_0}),\tilde{\beta}_t\mathbf{}I)
    $$
    
    $$
    \text{where} \quad \tilde{\mu}(\mathbf{x_t},\mathbf{x_0}) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon)\quad\text{and} \quad\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t
    $$

    
- Since obtaining $q(\mathbf{x_{t-1}}\|\mathbf{x_t})$ is very hard(it needs to use the entire dataset), we aim to learn a model $p_\theta$ to approximate it. 

$$
q(\mathbf{x_{t-1}}|\mathbf{x_{t}}) \simeq p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mathcal{\mu}_\theta(\mathcal{x}_t,t),\Sigma_\theta(\mathbf{x}_t,t)), \quad p_\theta(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)
$$

- Note that $p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T;\mathbf{0},\mathbf{I})$ (perfect Gaussian noise)

## Diffusion model

### Training

- The diffusion model aims to make $\mu_\theta$ similar to $\tilde{\mu}$.
- $\Sigma_\theta$ is just a fixed value dependent to $t$ (=$\sigma_t^2I$)
    - Values of $\sigma_t^2$ : $\beta_t$, $\tilde{\beta}_t$
    - Both showed similar results
- The final equation goes like this :
    
    $$
    \nabla_\theta||\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha_t}}\epsilon,t)||^2
    $$
    

### Sampling

- When the diffusion model is trained, it can be used in denoising.
    - Regarding $p_\theta(\mathbf{x}_{t-1}\|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mathcal{\mu}_\theta(\mathcal{x}_t,t),\Sigma_\theta(\mathbf{x}_t,t))$, we can predict $\mu_\theta$ using the following equation :
        
        $$
        \mu_\theta = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta)
        $$
        
- The final equation goes like this :
    
    $$
    \mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}}_t}\epsilon_\theta(\mathbf{x}_t, t)) + \sigma_t\mathbf{z} \quad \text{where} \quad \mathbf{z} \sim\mathcal{N}(0,I)
    $$
    

### Overall process
<img src="/assets/images/DDPM_3.png" width="100%" height="100%"> 