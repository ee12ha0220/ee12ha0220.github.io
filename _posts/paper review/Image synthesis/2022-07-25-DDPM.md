---
title: "Denoising Diffusion Probabilistic Models"
# image : ""
date: '2022-07-25'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

<img src="/assets/images/DDPM_1.png" width="100%" height="100%"> 
# Abstract

We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat- urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [https://github.com/hojonathanho/diffusion](https://github.com/hojonathanho/diffusion).

# Introduction

Diffusion probabilistic model(diffusion model)은 Markov chain인 forward process가 주어졌을 때, 그 backward process를 예측하고자 하는 model이다. Forward process가 소량의 Gaussian noise인 경우, backward process역시 Gaussian으로 근사할 수 있기 때문에 비교적 간단하게 model을 학습시킬 수 있다. 

본 논문(DDPM)은 diffusion model을 이용해 high-quality sample을 만드는 것이 가능하고, 다른 generative model보다 좋은 성능을 갖는 경우도 있다고 말한다. 

<!-- In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling -->

<img src="/assets/images/DDPM_2.png" width="100%" height="100%">*DDPM의 대략적인 구조. q가 forward process, p가 backward process 이다.*

# Background

Diffusion model은 latent $\mathbf{x}_1, ..., \mathbf{x}_T$와 forward process $q$로 이루어져 있다. 이때 $q$는 아주 작은 Gaussian noise로, mean과 variance는 특정한 schedule $\beta_1, ..., \beta_T$에 의해 결정된다(\ref{eq1}).

---

$$
q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I}) \label{eq1} \tag{1}
$$

---

Forward process는 어떠한 image $\mathbf{x}_0$에 Gaussian noise $q$를 $T$번 apply하는 것으로 생각할 수 있으며, 최종적인 latent $\mathbf{x}_T$ 역시 Gaussian noise가 된다. 추가적으로 $q$는 markov chain이기 때문에, 아래 식이 성립한다(\ref{eq2}). 

---

$$
q(\mathbf{x}_t|\mathbf{x}_0) = \prod_{i=1}^t q(\mathbf{x}_t|\mathbf{x}_{t-1}) \label{eq2} \tag{2}
$$

---

이를 이용하면 $\mathbf{x}_t$를 $\mathbf{x}_0$를 이용해서 나타낼 수 있다(\ref{eq3}). 

---

$$
\begin{align}
    \mathbf{x}_t &= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\epsilon \\
    &= \sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon \\
    &= ...\\
    &= \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon \label{eq3} \tag{3}
\end{align}
$$

---

이때 $\alpha_t = 1 - \beta_t$,
$\bar{\alpha}_t = \prod_{i=1}^{t}$,
<!-- $\epsilon = \mathcal{N}(0, \mathbf{I})$ 이다.  -->





<!-- $q(\mathbf{x}_t|\mathbf{x}_0)$ 를 계산할 수 있다(\ref{eq2}).

---

$$
q(\mathbf{x}_t|\mathbf{x}_0) = \prod_{i=1}^t q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \alpha_t\mathbf{x}_{t-1} + \beta_t\epsilon \label{eq2} \tag{2}
$$

--- -->



- A diffusion probabilistic model(diffusion model) is a **parameterized Markov chain** trained using variational inference to produce samples matching the data after finite time.
    - [Markov chain](https://brilliant.org/wiki/markov-chains/) : A Markov chain is a stochastic process, but it differs from a general stochastic process in that a Markov chain must be "memory-less." That is, (the probability of) future actions are not dependent upon the steps that led up to the present state.
    - The Markov chain gradually adds noise to the data(diffusion process),  and the aim is to learn its reverse process, which is denoising.
    
<img src="/assets/images/DDPM_2.png" width="100%" height="100%"> 

# Background

## Diffusion process

- The diffusion process is a Markov chain that **gradually adds Gaussian noise** to the data according to a variance schedule $\beta_1,...,\beta_T$ :
    
    $$
    q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I}), \quad q(\mathbf{x}_{1:T}|\mathbf{x}_0) := \prod_{t=1}^Tq(\mathbf{x}_t|\mathbf{x}_{t-1})
    $$
    
    - The term $\mathbf{x}_{1:T}$ means a process from $1$ to $T$.
    - The forward process variances  $\beta_t$ can be learned by reparameterization, or held constant as hyperparameters.
- The original data $\mathbf{x}_0$ gradually loses its feature as the step $t$ becomes larger, and in the end converges to gaussian noise, $\mathbf{x}_T$.

### Reparameterization of diffusion process

- Since the diffusion process is a markov chain, we can obtain $\mathbf{x}_t$ at any arbitrary time step $t$ using a reparameterization trick :
    
    $$
    \mathbf{x}_t = \sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\mathbf{\epsilon}, \quad \alpha_t = 1-\beta_t, \quad\bar\alpha_t = \prod_{i=1}^t\alpha_i, \quad \mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0,I}) \quad
    $$
    
    - This can be obtained by simply defining $\mathbf{x}_t$ recursivly.
    - $\mathbf{x}_0$ can be obtained by modifying the above equation :
        
        $$
        \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}\mathbf{x}_t - \sqrt{\frac{1}{\bar{\alpha}_t}-1}\cdot \epsilon
        $$
        

## Reverse process

- If $\mathbf{x_0}$ is given, the reverse conditional probability is tractable :
    
    $$
    q(\mathbf{x_{t-1}}|\mathbf{x_t},\mathbf{x_0}) = \mathcal{N}(\mathbf{x_{t-1}};\tilde{\mu}(\mathbf{x_t},\mathbf{x_0}),\tilde{\beta}_t\mathbf{}I)
    $$
    
    $$
    \text{where} \quad \tilde{\mu}(\mathbf{x_t},\mathbf{x_0}) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon)\quad\text{and} \quad\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t
    $$

    
- Since obtaining $q(\mathbf{x_{t-1}}\|\mathbf{x_t})$ is very hard(it needs to use the entire dataset), we aim to learn a model $p_\theta$ to approximate it. 

$$
q(\mathbf{x_{t-1}}|\mathbf{x_{t}}) \simeq p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\mathcal{\mu}_\theta(\mathcal{x}_t,t),\Sigma_\theta(\mathbf{x}_t,t)), \quad p_\theta(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)
$$

- Note that $p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T;\mathbf{0},\mathbf{I})$ (perfect Gaussian noise)

## Diffusion model

### Training

- The diffusion model aims to make $\mu_\theta$ similar to $\tilde{\mu}$.
- $\Sigma_\theta$ is just a fixed value dependent to $t$ (=$\sigma_t^2I$)
    - Values of $\sigma_t^2$ : $\beta_t$, $\tilde{\beta}_t$
    - Both showed similar results
- The final equation goes like this :
    
    $$
    \nabla_\theta||\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha_t}}\epsilon,t)||^2
    $$
    

### Sampling

- When the diffusion model is trained, it can be used in denoising.
    - Regarding $p_\theta(\mathbf{x}\_{t-1}\|\mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1};\mathcal{\mu}\_\theta(\mathcal{x}\_t,t),\Sigma\_\theta(\mathbf{x}\_t,t))$, we can predict $\mu_\theta$ using the following equation :
        
        $$
        \mu_\theta = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta)
        $$
        
- The final equation goes like this :
    
    $$
    \mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}}_t}\epsilon_\theta(\mathbf{x}_t, t)) + \sigma_t\mathbf{z} \quad \text{where} \quad \mathbf{z} \sim\mathcal{N}(0,I)
    $$
    

### Overall process
<img src="/assets/images/DDPM_3.png" width="100%" height="100%"> 