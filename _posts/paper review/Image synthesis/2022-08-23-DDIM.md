---
title: "DENOISING DIFFUSION IMPLICIT MODELS"
# image : ""
date: '2022-08-23'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
--- 

# Abstract
Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.

# Introduction
Deep generative model은 다양한 deep learnig domain에서 사용된다. 최근에는 diffusion probabilistic model(DDPM, Ho et al. 2020), noise conditional score networks(NCSN, Song & Ermon 2019)같은 iterative generative model이 GAN에 버금가는 좋은 성능을 보이고 있다. 하지만 이러한 model들의 단점 중 하나는 많은 iteration step(적어도 1000 이상)을 필요로 한다는 것이고, 이는 generation에 긴 시간이 걸린다는 것을 뜻한다. 본 논문에서는 DDPM의 iteration step을 줄이기 위해 ***denoising diffusion implicit model(DDIM)*** 을 제시한다. DDIM에서는 DDPM에서 makov chain으로 정의되었던 forward process를 objective이 같은 non-markovian chain으로 새롭게 design한다. 이를 통해 더 적은 step으로 reconstruction이 가능하게 만들고, 추가적으로 reconstruction을 할 때마다 달라지는 DDPM과는 달리 stable하게 reconstruction을 할 수 있도록 해준다. 

# Variational inference for non-markovian forward process

## Non-markovian forward process

DDIM은 [DDPM](https://ee12ha0220.github.io/posts/DDPM/)위에서 정의된다. DDPM에서는 다음과 같은 forward process를 사용한다(\ref{eq1}).

---

$$
q(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\sqrt{\alpha_t}\mathbf{x}_t, (1-\alpha_t)\mathbf{I}) \label{eq1} \tag{1}
$$

---

이는 오직 $\mathbf{x} _t$에만 의존하는 markov chain이다. 이를 이용하면 $\mathbf{x} _0$에서 $\mathbf{x} _t$를 바로 구할 수 있다(\ref{eq2}). 

---

$$
\begin{align}
\mathbf{x}_t 
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\epsilon \\
&= \sqrt{\alpha_t\alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon\\
&= \ldots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \label{eq2} \tag{2}
\end{align}
$$

---

이때 $\epsilon$은 standard Gaussian distribution이다. 즉, $\mathbf{x} _0$가 주어졌을 때  $\mathbf{x} _t$의 distribution을 다음과 같이 쓸 수 있다(\ref{eq3}). 

---

$$
q(\mathbf{x_t|\mathbf{x}_0}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x_0}, (1-\bar{\alpha}_t)\mathbf{I}) \label{eq3} \tag{3}
$$

---

DDIM은 equation \ref{eq3}을 만족시키는 non-markovian forward process를 사용하고자 한다. 여기에는 사실 $\alpha _t$가 등장하지 않기 때문에, equation \ref{eq3}을 다음과 같이 새롭게 쓰고 시작한다(\ref{eq4}). 

---

$$
q(\mathbf{x_t|\mathbf{x}_0}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t}\mathbf{x_0}, (1-\alpha_t)\mathbf{I}) \label{eq4} \tag{4}
$$

---

Equation 4를 만족시키는 forward process를 얻기 위해, DDIM에서는 다음과 같은 backward process를 사용한다(\ref{eq5}). 

---

$$
q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x_0}}{\sqrt{1-\alpha_t}}, \sigma_t^2\mathbf{I}) \label{eq5} \tag{5}
$$

---

Forward process는 다음 식을 이용해서 구할 수 있다(\ref{eq6}). 

---

$$
q_\sigma(\mathbf{x}_{t}|\mathbf{x}_{t-1},\mathbf{x}_0) = \frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)q_\sigma(\mathbf{x}_{t}|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)} \label{eq6} \tag{6}
$$

---

만약 이때 $\sigma\rightarrow 0$ 이 된다면, $\mathbf{x} _0$과 $\mathbf{x} _{t}$에 deterministic하게 $\mathbf{x} _{t-1}$이 결정되게 된다. 

<img src="/assets/images/DDIM_1.png" width="100%" height="100%">*DDPM(왼쪽)과 달리 DDIM(오른쪽)은 forward process를 $\mathbf{x} _0$를 이용해서 정의한다.*

## Generative process and unified variational inferece objective

DDIM에서는 $q _\sigma(\mathbf{x} _{t-1}\|\mathbf{x} _t,\mathbf{x} _0)$를 이용해 trainable generative process $p ^{(t)} _\theta(\mathbf{x} _{t-1}\|\mathbf{x} _t)$를 정의한다. 간단하게 설명하면 $\mathbf{x} _t$를 이용해 해당하는 $\mathbf{x} _0$를 predict한 다음, 이를 이용해서 $\mathbf{x} _{t-1}$을 얻는 것이다. Equation 4를 이용하면 $\mathbf{x} _t$를 $\mathbf{x} _0 \sim q(\mathbf{x} _0)$,  $\epsilon _t \sim \mathcal{N}(0, I)$를 이용해 나타낼 수 있다. 여기서 다음과 같이  $\mathbf{x} _0$에 대한 정보가 없이 $\epsilon _t$를 predict하려고 시도한다(\ref{eq7}). 

---

$$
\begin{align}
\mathbf{x}_t &= \sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t \\
\mathbf{x}_0 &= (\mathbf{x}_t -\sqrt{1-\alpha_t}\cdot\epsilon_t)\sqrt{\alpha_t} \\
f_\theta^{(t)}(\mathbf{x}_t) :&= (\mathbf{x}_t-\sqrt{1-\alpha_t}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t))/\sqrt{\alpha_t}
\end{align} \label{eq7} \tag{7}
$$

---

그러면 backward process를 다음과 같이 정의할 수 있다(\ref{eq8}). 

---

$$
p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}) = 
\begin{cases}
\mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1), \sigma_1^2\mathbf{I}) &\text{if } t=1 \\
q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,f_\theta^{(t)}(\mathbf{x}_t)) & \text{otherwise}
\end{cases} \label{eq8} \tag{8}
$$

---

# Sampling from generalized generative processes
DDIM은 기본적으로 DDPM과 같은 loss function을 이용해 학습된다. 그렇기 때문에 sampling 방식만 바꿔준다면, DDPM을 기반을 학습된 model을 그대로 사용할 수 있다. 

## Denoising diffusion implicit models
DDIM에서 $\mathbf{x} _{t-1}$은 다음과 같이 나타낼 수 있다(\ref{eq9}). 

---

$$
\mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}}\left( \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t)}{\sqrt{\alpha_t}} \right) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \epsilon_\theta(\mathbf{x}_t) + \sigma_t\epsilon_t \label{eq9} \tag{9}
$$

---

여기서 $\sigma _t$의 값에 따라 $\epsilon _\theta$를 다시 학습하지 않고도 다양한 generative process를 modeling 할 수 있고, 만약 $\sigma _t = \sqrt{(1-\alpha _{t-1})/(1-\alpha _t)} \sqrt{1-\alpha _t/\alpha _{t-1}}$으로 설정하면 DDPM이 된다. 만약 $\sigma _t$의 값을 0으로 설정한다면 $\mathbf{x} _{t-1}$이 deterministic하게 결정되는데, 이를 *denoisiong diffusion implict model(DDIM)* 이라 한다. 

## Accelerated generation process
DDIM에서 사용하는 loss function을 다시 써보면 아래와 같다(\ref{eq10}). 

---

$$
\text{loss} = ||\epsilon_t - \epsilon_\theta(\underbrace{\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon_t}_{\mathbf{x}_t}, t)||^2 \label{eq10} \tag{10} 
$$

---

이때 input으로 주어지는 $\mathbf{x} _t$는 $\mathbf{x} _0$와 $\bar{\alpha} _t$에 의해 결정되는데, 이는 결국 $\bar{\alpha} _t = \prod _{i=1}^t\alpha_t$가 중요한 것이지 각 $\alpha _t$는 달라도 상관없다는 의미를 내포하고 있다. 그렇기 때문에 1에서 $T$까지 모든 $\bar{\alpha} _t$를 사용하는 것이라, 이중 일부만을 사용해서 generation process의 속도를 향상시킬 수 있다. 이런 경우에는 DDPM보다 DDIM이 더 좋은 성능을 보였다. 

# Training details
실험은 DDPM과 DDIM 모두 $T=1000$의 조건 하에 학습한 model을 사용했다. Dataset은 CIFAR 10, CelebA 를 사용했다. 

# Results
더 적은 step을 이용해 reconstruction을 했을 때 DDPM보다 DDIM이 더 좋은 결과를 보였다. 

<img src="/assets/images/DDIM_2.png" width="80%" height="80%">*$\eta$는 variance 부분에 곱해지는 상수로,1일때가 DDPM, 0일때가 DDIM이다. $\hat{\sigma}$의 경우도 DDPM이다. $S$를 full scale(1000)로 했을 때는 DDPM이 성능이 더 좋지만, 작은 $S$를 사용했을 때 DDIM의 성능이 훨씬 좋았다.*

# Conclusion
We have presented DDIMs – an implicit generative model trained with denoising auto-encoding / score matching objectives – from a purely variational perspective. DDIM is able to generate high quality samples much more efficiently than existing DDPMs and NCSNs, with the ability to perform meaningful interpolations from the latent space. The non-Markovian forward process presented here seems to suggest continuous forward processes other than Gaussian (which cannot be done in the original diffusion framework, since Gaussian is the only stable distribution with finite variance). 

Moreover, since the sampling procedure of DDIMs is similar to that of an neural ODE, it would be interesting to see if methods that decrease the discretization error in ODEs, including multi- step methods such as Adams-Bashforth (Butcher & Goodwin, 2008), could be helpful for further improving sample quality in fewer steps (Queiruga et al., 2020). It is also relevant to investigate whether DDIMs exhibit other properties of existing implicit models (Bau et al., 2019).