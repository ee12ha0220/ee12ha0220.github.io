---
title: "Generative Modeling by Estimating Gradients of the Data Distribution"
# image : ""
date: '2022-08-10'
categories: [Paper review, Image synthesis]
# tags: [tag] 
author: saha
math: true
mermaid: true
pin : false
---

# Abstract

We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to
gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.

# Introduction

Generative model은 machine learning의 다양한 분야에서 사용되는데, 대표적으로 log-likelihood를 이용해서 학습을 하는 likelihood-based models와, adversarial training을 이용하는 GAN이 있다. 이 두 model은 뛰어난 성능을 보이지만 한계점도 존재하는데, likelihood based model의 경우에는 normalized probability model을 만들기 위해 특정한 architecture(autoregressive model, flow model)이 강제되고, surrogate loss(VAE의 evidence lower bound)를 사용해야 한다는 한계점이 있다. GAN은 likelihood based model의 문제점들을 어느정도 완화했지만, adversarial training이 불안정하다는 단점이 있다. 

본 논문(SMLD)에서는 log-probability의 gradient인 ***score***라는 개념을 이용한 새로운 generative model을 제안하는데, score matching을 통해 neural network를 학습시킨다. 




Palette는 기본적으로 [DDPM](https://ee12ha0220.github.io/posts/DDPM/)과 동일한 구조를 사용하는데, 달라진 점이라면 input image를 prior로 사용한다는 점이다. Optimization에 사용되는 loss를 수식으로 나타내면 다음과 같다(\ref{eq1}).

---

$$
\mathbb{E}_{(x,y)}\mathbb{E}_{\mathbf{\epsilon}\sim\mathcal{N}(0,I)}\mathbb{E}_\gamma ||f_\theta(\mathbf{x}, \sqrt{\gamma}\mathbf{y}+\sqrt{1-\gamma}\epsilon, \gamma)-\epsilon||^p_p \label{eq1} \tag{1}
$$

---

<img src="/assets/images/palette_1.png" width="100%" height="100%">*Colorization 결과*

<img src="/assets/images/palette_2.png" width="100%" height="100%">*Inpainting 결과*

<img src="/assets/images/palette_3.png" width="100%" height="100%">*Uncropping 결과*

<img src="/assets/images/palette_4.png" width="100%" height="100%">*JPEG restoration 결과*

<img src="/assets/images/palette_5.png" width="100%" height="100%">*Multi-task learning 결과. 맨 오른쪽은 inpainting에 대해서만 학습된 model이다.*



# Conclusion

We present Palette, a simple, general framework for image-to-image translation. Palette achieves strong results on four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG restoration), outperforming strong GAN and regression baselines. Unlike many GAN models, Palette produces diverse and high fidelity outputs. This is accomplished without task-specific customization nor optimization instability. We also present a multi-task Palette model, that performs just as well or better over their task-specific counterparts. Further exploration and investigation of multi-task diffusion models is an exciting avenue for future work. This paper shows some of the potential of image-to-image diffusion models, but we look forward to seeing new applications.
