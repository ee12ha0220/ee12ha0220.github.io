<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS" /><meta name="author" content="saha" /><meta property="og:locale" content="en" /><meta name="description" content="Abstract" /><meta property="og:description" content="Abstract" /><link rel="canonical" href="https://ee12ha0220.github.io/posts/NeRFpp/" /><meta property="og:url" content="https://ee12ha0220.github.io/posts/NeRFpp/" /><meta property="og:site_name" content="Saha’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-13T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@saha" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"saha","url":"https://github.com/ee12ha0220/"},"dateModified":"2022-01-13T00:00:00+09:00","datePublished":"2022-01-13T00:00:00+09:00","description":"Abstract","headline":"NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS","mainEntityOfPage":{"@type":"WebPage","@id":"https://ee12ha0220.github.io/posts/NeRFpp/"},"url":"https://ee12ha0220.github.io/posts/NeRFpp/"}</script><title>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS | Saha's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Saha's Blog"><meta name="application-name" content="Saha's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/images/avatar_2.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Saha's Blog</a></div><div class="site-subtitle font-italic">공부한 것들을 까먹지 말자</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ee12ha0220" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ee12ha0220','kaist.ac.kr'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1641999600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jan 13, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/ee12ha0220/">saha</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1881 words"> <em>10 min</em> read</span></div></div></div><div class="post-content"><h1 id="abstract">Abstract</h1><p>Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi- layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method im- proves view synthesis fidelity in this challenging scenario. Code is available at <a href="https://github.com/Kai-46/nerfplusplus">this link</a>.</p><h1 id="introduction">Introduction</h1><p><a href="https://ee12ha0220.github.io/posts/NeRF/">NeRF</a>는 NVS 분야에서 뛰어난 성능을 보였다. 본 논문에서는 NeRF model의 성능이 좋은 이유에 대한 분석과, outdoor scene같은 unbounded scene에서 NeRF의 성능을 올리는 NeRF++ model을 제시했다.</p><h1 id="shape-radiance-ambiguity">Shape-Radiance Ambiguity</h1><p>NeRF는 view-dependent하게 scene을 generate하기 때문에, 3D shape과 radiance사이의 ambiguity가 만족되지 않으면 degenerate solution이 도출될 가능성이 크다. 즉, input된 training data의 정확한 shape를 학습하지 못한다면, 학습 과정에서는 좋은 성능을 보일 수 있지만 novel view에서의 image는 이상하게 나올 수 있다.</p><p>예를 들면 서로 다른 위치에서 같은 point를 바라보고 있는 2개의 ray를 생각해보자. 이 2개의 ray에서 관측되는 color값은 같은 point를 보고 있지만, object의 surface geometry 때문에 다른 값을 가진다. 하지만 이 두 ray는 viewing direction이 다르기 때문에, 색이 다른 것이 geometry가 달라서가 아니라 viewing direction이 다르기 때문에서라고 학습할 가능성이 있다. 이렇게 되면 전체 model이 incorrect shape을 기준으로 학습되며, 당연하게 novel view에서의 image는 이상하게 나오는 것이다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 50% 50%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_1.png" width="50%" height="50%" data-proofer-ignore> <em>잘못된 shape $\hat{S}$에 맞게 학습되면, 실제로 같은 곳을 바라보고 있는 $C_0$와 $C_1$이 다른 곳을 바라보고 있는 것을 알 수 있다.</em></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_2.png" width="100%" height="100%" data-proofer-ignore> <em>Unit sphere로 shape를 고정하고 학습한 결과, training에서는 좋은 결과를 보이지만 testing에서 novel view에 대해 이상한 결과를 보인다.</em></p><p>굳이 shape를 unit sphere로 고정해놓지 않더라도, 학습을 진행하는 과정에서 NeRF가 올바른 shape를 알아내지 못하고, 이상한 shape에 맞게 학습 될 가능성이 충분히 존재한다. 하지만 실제 NeRF를 학습하면 이러한 degenerate solution이 발생하지 않는다. NeRF++의 저자들은 이를 다음 2가지 이유로 설명한다.</p><ul><li><p>잘못된 shape으로 학습이 되면 그만큼 color가 high-dimensional하게 표현될 가능성이 크다. 왜냐하면 서로 같은 지점을 바라보는 2개의 viewpoint를 생각해 봤을 때, 옳은 shape이라면 그냥 1개의 값만 predict하면 되지만, 잘못된 shape이라면 서로 다른 지점을 바라보는 것으로 간주되어 그만큼 더 복잡한 prediction이 이루어져야 한다. NeRF에서 사용하는 MLP는 그러한 high-dimensional한 prediction을 하기에 더 어렵기 때문에, 이러한 경우가 잘 발생하지 않는다.</p><li><p>NeRF의 MLP 구조를 보면 position $\mathbf{x}$보다 direction $\mathbf{d}$가 MLP layer에 더 늦게 input되는 것을 알 수 있다. $\mathbf{d}$는 그만큼 학습에 큰 영향력을 행사하기 힘들기 때문에, 이로 인한 잘못된 prediction도 잘 발생하지 않는다.</p></ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_2.png" width="100%" height="100%" data-proofer-ignore> <em>NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공되었다.</em></p><p>2번째 이유는 실제로 아주 큰 영향을 미치는데, NeRF의 MLP와 다르게 $\mathbf{d}$가 $\mathbf{x}$와 함께 처음부터 input되는 vanilla MLP의 경우 model의 성능이 현저하게 떨어졌다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_3.png" width="100%" height="100%" data-proofer-ignore> <em>일반적인 MLP를 사용했을 때 성능이 현저하게 떨어진 것을 확인할 수 있다.</em></p><h1 id="handling-unbounded-scene">Handling unbounded scene</h1><p>NeRF에서는 ray를 따라 volume rendering을 해서 최종 color를 구하는데, 이 식은 다음과 같다(\ref{eq1}).</p><hr /> \[C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt, \text{ where } T(t) = \exp\left( -\int_{t_n}^t\sigma(\mathbf{r}(s))ds \right) \label{eq1} \tag{1}\]<hr /><p>이 적분은 ray의 near bound $t_n$과 far bound $t_f$사이에서 sampling을 통해 얻은 sample들을 더하는 방식으로 discrete하게 바껴서 진행된다. 이는 $t_n$과 $t_f$의 차이가 크지 않은 bounded scene에서는 잘 먹히지만, $t_f$가 infinity인 unbounded scene의 경우에는 sampling이 너무 sparse하게 일어나서 좋은 결과가 나오지 않는다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_4.png" width="100%" height="100%" data-proofer-ignore> <em>Bounded scene의 경우 dense한 sample들을 얻을 수 있다.</em></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_5.png" width="100%" height="100%" data-proofer-ignore> <em>Unbounded scene에서는 sample들이 너무 sparse하다.</em></p><p>NeRF++에서는 이를 해결하기 위해 <strong><em>Inverted sphere parametrization</em></strong>을 제시한다. NeRF와 비슷하게 이 역시 2개의 MLP model을 사용하는데, 전체 scene을 foreground과 background으로 나눠서 각각 학습시킨다. Foreground과 background는 input image들의 camera를 모두 감싸는 sphere를 경계로 나누어진다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 50% 50%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_6.png" width="50%" height="50%" data-proofer-ignore> <em>Camera(초록색)를 모두 감싸는 sphere(빨간색)을 기준으로 foreground(sphere 안쪽)과 background(sphere 바깥쪽)이 나눠진다.</em></p><h2 id="foreground"><span class="mr-2">Foreground</span><a href="#foreground" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Foreground는 bounded scene이기 때문에, NeRF과 같은 방식으로 sampling이 이루어진다.</p><h2 id="background"><span class="mr-2">Background</span><a href="#background" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Background는 아직도 unbounded scene이기 때문에, inverted sphere parametrization을 적용해준다. Background에 속한 3D point는 다음과 같이 표현될 수 있다(\ref{eq2}).</p><hr /> \[\mathbf{x} = (x,y,z), r(\mathbf{x}) = \sqrt{x^2+y^2+z^2} \quad \rightarrow \quad \mathbf{x}^\prime = (x/r, y/r, z/r, 1/r) = (x^\prime, y^\prime, z^\prime, 1/r) \label{eq2} \tag{2}\]<hr /><p>즉, 멀리 있는 point를 경계가 되는 sphere에 project 시킨 것으로 생각할 수 있다. 이때 sampling은 $1/r$을 기준으로 일어나게 되는데, $r &gt; 1$이기 때문에 $0 &lt; 1/r &lt; 1$을 만족해 sparse sample들을 dense하게 만드는 효과가 있다. 이때 사실 $(x^\prime, y^\prime, z^\prime)$은 실제 projection과는 다른 점이지만, $\mathbf{x}$가 멀리 떨어진 점이기 때문에 실제 projection과 같다고 근사할 수 있다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 50% 50%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_7.png" width="50%" height="50%" data-proofer-ignore> <em>$(x^\prime, y^\prime, z^\prime)$는 실제 project된 point $a$와는 다르지만, $a$에 비해 계산하는 것이 훨씬 쉽고, $p$가 멀리 있기 때문에 같다고 근사할 수 있기 때문에 사용한다.</em></p><h2 id="volumetric-rendering-of-nerf"><span class="mr-2">Volumetric rendering of NeRF++</span><a href="#volumetric-rendering-of-nerf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>결과적으로 NeRF++의 volumetric rendering은 다음과 같이 다시 쓸 수 있다(\ref{eq3}).</p><hr /> \[\begin{align} \mathbf{C}(\mathbf{r}) = &amp;\underset{\text{foreground}}{\underbrace{\int_{t=0}^{t^\prime}\sigma(\mathbf{o}+t\mathbf{d})\cdot \mathbf{c}(\mathbf{o} + t\mathbf{d}, \mathbf{d})\cdot \exp \left(-\int_{s=0}^t\sigma(\mathbf{o}+s\mathbf{d})ds \right)dt}}\\ &amp;\underset{\text{background}}{\underbrace{+ \exp \left(-\int_{s=0}^{t^\prime}\sigma(\mathbf{o}+s\mathbf{d})ds \right)\cdot\int_{t=t^\prime}^{\infty}\sigma(\mathbf{o}+t\mathbf{d})\cdot \mathbf{c}(\mathbf{o} + t\mathbf{d}, \mathbf{d})\cdot \exp \left(-\int_{s=t^\prime}^t\sigma(\mathbf{o}+s\mathbf{d})ds \right)dt}} \label{eq3}\tag{3} \end{align}\]<hr /><h1 id="training-details">Training details</h1><p>NeRF와 거의 동일하다. 자세한 사항은 <a href="https://arxiv.org/abs/2010.07492">NeRF++ 논문</a>을 참조하길 바란다.</p><h1 id="results">Results</h1><p>NeRF보다 더 발전된 결과를 보였다. 자세한 사항은 <a href="https://arxiv.org/abs/2010.07492">NeRF++ 논문</a>을 참조하길 바란다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 80% 80%'%3E%3C/svg%3E" data-src="/assets/images/NeRFpp_8.png" width="80%" height="80%" data-proofer-ignore> <em>NeRF++의 결과. NeRF보다 더 발전된 것을 확인할 수 있다.</em></p><h1 id="conclusion">Conclusion</h1><p>NeRF++ improves the parameterization of unbounded scenes in which both the foreground and the background need to be faithfully represented for photorealism. However, there remain a number of open challenges. First, the training and testing of NeRF and NeRF++ on a single large-scale scene is quite time-consuming and memory-intensive. Training NeRF++ on a node with 4 RTX 2080 Ti GPUs takes ∼24 hours. Rendering a single 1280x720 image on one such GPU takes ∼30 seconds at test time. Liu et al. (2020) have sped up the inference, but rendering is still far from real-time. Second, small camera calibration errors may impede photorealistic synthesis. Robust loss functions, such as the contextual loss (Mechrez et al., 2018), could be applied. Third, photometric effects such as auto-exposure and vignetting can also be taken into account to increase image fidelity. This line of investigation is related to the lighting changes addressed in the orthogonal work of Martin-Brualla et al. (2020).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>Paper review</a>, <a href='/categories/nvs/'>NVS</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=NERF%2B%2B%3A+ANALYZING+AND+IMPROVING+NEURAL+RADIANCE+FIELDS+-+Saha%27s+Blog&url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRFpp%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=NERF%2B%2B%3A+ANALYZING+AND+IMPROVING+NEURAL+RADIANCE+FIELDS+-+Saha%27s+Blog&u=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRFpp%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRFpp%2F&text=NERF%2B%2B%3A+ANALYZING+AND+IMPROVING+NEURAL+RADIANCE+FIELDS+-+Saha%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">Denoising Diffusion Probabilistic Models</a><li><a href="/posts/SMLD/">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="/posts/DDIM/">DENOISING DIFFUSION IMPLICIT MODELS</a><li><a href="/posts/fn/">각종 코드 정리</a><li><a href="/posts/MLMC/">A Machine Learning Approach for Filtering Monte Carlo Noise</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/NeRF/"><div class="card-body"> <em class="small" data-ts="1641308400" data-df="ll" > Jan 5, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h3><div class="text-muted small"><p> Abstract We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse s...</p></div></div></a></div><div class="card"> <a href="/posts/NeRFmm/"><div class="card-body"> <em class="small" data-ts="1642863600" data-df="ll" > Jan 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NeRF−−: Neural Radiance Fields Without Known Camera Parameters</h3><div class="text-muted small"><p> Abstract This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses or intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recent...</p></div></div></a></div><div class="card"> <a href="/posts/uORF/"><div class="card-body"> <em class="small" data-ts="1643814000" data-df="ll" > Feb 3, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Unsupervised Discovery of Object Radiance Fields</h3><div class="text-muted small"><p> Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/zshshell/" class="btn btn-outline-primary" prompt="Older"><p>ZSH shell 사용하기</p></a> <a href="/posts/post2/" class="btn btn-outline-primary" prompt="Newer"><p>Github Blog에 코드 넣기</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Seeha Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
