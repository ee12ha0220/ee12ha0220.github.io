<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" /><meta name="author" content="saha" /><meta property="og:locale" content="en" /><meta name="description" content="Abstract" /><meta property="og:description" content="Abstract" /><link rel="canonical" href="https://ee12ha0220.github.io/posts/NeRF/" /><meta property="og:url" content="https://ee12ha0220.github.io/posts/NeRF/" /><meta property="og:site_name" content="Saha’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-05T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@saha" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"saha","url":"https://github.com/ee12ha0220/"},"dateModified":"2022-08-25T08:30:02+09:00","datePublished":"2022-01-05T00:00:00+09:00","description":"Abstract","headline":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","mainEntityOfPage":{"@type":"WebPage","@id":"https://ee12ha0220.github.io/posts/NeRF/"},"url":"https://ee12ha0220.github.io/posts/NeRF/"}</script><title>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis | Saha's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Saha's Blog"><meta name="application-name" content="Saha's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/images/avatar_2.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Saha's Blog</a></div><div class="site-subtitle font-italic">공부한 것들을 까먹지 말자</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ee12ha0220" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ee12ha0220','kaist.ac.kr'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1641308400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jan 5, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/ee12ha0220/">saha</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1593 words"> <em>8 min</em> read</span></div></div></div><div class="post-content"><h1 id="abstract">Abstract</h1><p>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate(spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses.We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p><h1 id="introduction">Introduction</h1><p>Novel view synthesis(NVS)는 특정한 scene에 대한 input이 주어졌을 때, 그 scene을 학습하는 것이다. 즉, input들과는 다른 임의의 viewpoint에서 scene을 봤을 때 어떻게 보일지를 예측하는 것을 목표로 한다. 본 논문(NeRF)에서는 MLP를 사용해서 5D input(color + viewing direction)에서 3D view-dependent color을 만들어내는 것을 목표로 한다.</p><p>NeRF의 간단한 과정은 다음과 같다.</p><ol><li>Pixel마다 ray를 쏴서 3D point를 ray를 따라 sampling 한다.<li>Sample된 point와 해당하는 2D viewing direction을 MLP에게 feed해서 color과 density값을 학습한다.<li>구한 color과 density에 volume rendering을 적용해 2D image를 만들어낸다.</ol><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_1.png" width="100%" height="100%" data-proofer-ignore> <em>NeRF의 대략적인 scheme.</em></p><h1 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h1><h2 id="mlp-network"><span class="mr-2">MLP network</span><a href="#mlp-network" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>NeRF에서는 특정한 scene을 3D location $\mathbf{x} = (x,y,z)$와 2D viewing direction $(\theta, \phi)$를 이용해서 나타내고, 이를 MLP network에 input으로 줘서 color $\mathbf{c} = (r,g,b)$와 volume density $\sigma$를 구한다. 이를 식으로 나타내면 다음과 같다(\ref{eq1}).</p><hr /> \[F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma) \label{eq1} \tag{1}\]<hr /><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_2.png" width="100%" height="100%" data-proofer-ignore> <em>NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공된 것을 알 수 있는데, 이는 <a href="https://ee12ha0220.github.io/posts/NeRFpp/">다른논문</a>에서 자세히 설명하고 있다.</em></p><h2 id="volume-rendering"><span class="mr-2">Volume rendering</span><a href="#volume-rendering" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>MLP network을 통해 구한 color $\mathbf{c}(\mathbf{x})$와 volume density $\sigma(\mathbf{x})$는 volume rendering을 통해 실제 color value $C(\mathbf{r})$로 변환된다. Volume rendering은 $\mathbf{r}$을 origin $\mathbf{o}$와 viewing direction $\mathbf{d}$ 을 이용해 $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$로 표현했을 때, 다음과 같이 표현된다(\ref{eq2}).</p><hr /> \[C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt, \text{ where } T(t) = \exp\left( -\int_{t_n}^t\sigma(\mathbf{r}(s))ds \right) \label{eq2} \tag{2}\]<hr /><p>이때 $t_n, t_f$는 scene의 near, far boundary가 된다. 실제로 적분을 하려면 위처럼 continuous 한 식이 아니라, discrete한 형태로 바꿔줘야 한다. 이를 위해서 NeRF에서는 $t_n$과 $t_f$사이에서 stratified sampling을 통해 $N$개의 discrete한 sample을 만든다(\ref{eq3}).</p><hr /> \[t_i \sim \mathcal{U}\left[ t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n) \right] \label{eq3} \tag{3}\]<hr /><p>위 sample들을 이용해 equation \ref{eq2}를 다시 쓰면 다음과 같다(\ref{eq4}).</p><hr /> \[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))\mathbf{c}_i, \text{ where } T_i = \exp\left( -\sum_{j=1}^{i-1}\sigma_j\delta_j \right) \label{eq4} \tag{4}\]<hr /><p>이때 $\delta_i = t_{i+1} - t_i$로, 인접한 sample간의 거리를 의미한다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_3.png" width="100%" height="100%" data-proofer-ignore> <em>NeRF의 학습 과정.</em></p><h1 id="optimizing-nerf">Optimizing NeRF</h1><p>위의 방법대로 NeRF를 학습시켰을 때는 좋은 결과가 나오지 않았다고 한다. NeRF의 저자들은 여러 방법을 통해 NeRF의 성능을 끌여올렸다.</p><h2 id="positional-encoding"><span class="mr-2">Positional encoding</span><a href="#positional-encoding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>MLP network $F_\Theta$는 $xyz\theta\phi$의 5D input만으로는 좋은 성능을 보이지 못한다. 그렇기 때문에 <strong>positional encoding</strong> $\gamma$를 이용해 input의 dimension을 증가시켜 줬다(\ref{eq5}).</p><hr /> \[\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), ..., \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p)) \label{eq5} \tag{5}\]<hr /><p>본 논문에서는 $\gamma(\mathbf{x})$에는 $L=10$을, $\gamma(\mathbf{d})$에는 $L=4$를 사용했다.</p><h2 id="hierarchical-volume-sampling"><span class="mr-2">Hierarchical volume sampling</span><a href="#hierarchical-volume-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Volume rendering을 하는 과정에서 그냥 uniform하게 $N$개의 sample을 만드는 것은 비효율적이다. 왜냐하면 아무 object도 없는 빈 공간일 가능성도 있고, occlude된 region일 수도 있기 때문이다. 그래서 NeRF에서는 더 효율적으로 sampling을 하기 위해 <strong>hierarchical volume sampling</strong>을 사용한다. 하나의 network로 scene을 학습하는 것이 아니라, NeRF에서는 <strong>coarse</strong>와 <strong>fine</strong>이 2개의 network를 사용한다. Coarse network은 uniform한 sample $N_c$들을 이용해 학습되고, 이 결과에 기반해서 다시 한번 sampling이 일어나게 된다. 이때 각 sample이 최종 color에 얼마나 기여하는지에 대한 PDF를 이용해 새로운 sampling을 하는데, 이는 Equation \ref{eq4}를 다음과 같이 다시 쓰면 쉽게 이해할 수 있다(\ref{eq6).</p><hr /> \[\hat{C}_c(\mathbf{r}) = \sum_{i=1}^{N_c}w_ic_i, \quad w_i = T_i(1-\exp(-\sigma_i\delta_i)) \label{eq6}\tag{6}\]<hr /><p>위의 weight들을 $\hat{w}_i = w_i/ \sum _{j=1}^{N_C}w_j$로 normalize하면, 이를 ray를 따른 각 sample의 기여도의 PDF로 생각할 수 있다. 이에 기반해서 sampling된 $N_f$와 $N_C$를 모두 이용해 Fine network 가 학습이 된다. 이는 더 중요한 region에 더 많은 sample들이 있게 해서, NVS의 성능을 올려준다.</p><h1 id="training-details">Training details</h1><h2 id="dataset"><span class="mr-2">Dataset</span><a href="#dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Dataset은 특정 scene에 대한 200장 정도의 RGB image들과 각 image에 대한 camera pose와 intrinsic으로 구성된다. Real data를 사용하는 경우 pose와 intrinsic은 structure-from-motion package인 SFM colmap을 사용해서 구한다.</p><h2 id="loss"><span class="mr-2">Loss</span><a href="#loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Loss에는 coarse와 fine network의 output이 둘다 사용된다(\ref{eq7}).</p><hr /> \[\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}}\left[ \left| \left| \hat{C}_c(\mathbf{r}) - C(\mathbf{r}) \right| \right|^2_2 + \left| \left| \hat{C}_f(\mathbf{r}) - C(\mathbf{r}) \right| \right|^2_2 \right] \label{eq7}\tag{7}\]<hr /><h2 id="evaluation-metric"><span class="mr-2">Evaluation metric</span><a href="#evaluation-metric" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>PSNR, SSIM, LPIPS가 사용되었다.</p><p>더 자세한 사항은 <a href="https://arxiv.org/abs/2003.08934">NeRF 논문</a>를 참조하길 바란다.</p><h1 id="results">Results</h1><p>이전의 연구들에 비해 크게 향상된 성능을 보였다. 더 자세한 내용은 <a href="https://arxiv.org/abs/2003.08934">NeRF 논문</a>를 참조하길 바란다. ]</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_4.png" width="100%" height="100%" data-proofer-ignore> <em>Synthetic data를 사용한 결과.</em></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/NeRF_5.png" width="100%" height="100%" data-proofer-ignore> <em>Real data를 사용한 결과.</em></p><h1 id="conclusion">Conclusion</h1><p>Our work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that represent- ing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations.</p><p>Although we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and ren- der neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>Paper review</a>, <a href='/categories/nvs/'>NVS</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=NeRF%3A+Representing+Scenes+as+Neural+Radiance+Fields+for+View+Synthesis+-+Saha%27s+Blog&url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRF%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=NeRF%3A+Representing+Scenes+as+Neural+Radiance+Fields+for+View+Synthesis+-+Saha%27s+Blog&u=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRF%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FNeRF%2F&text=NeRF%3A+Representing+Scenes+as+Neural+Radiance+Fields+for+View+Synthesis+-+Saha%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">Denoising Diffusion Probabilistic Models</a><li><a href="/posts/SMLD/">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="/posts/DDIM/">DENOISING DIFFUSION IMPLICIT MODELS</a><li><a href="/posts/fn/">각종 코드 정리</a><li><a href="/posts/MLMC/">A Machine Learning Approach for Filtering Monte Carlo Noise</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/NeRFpp/"><div class="card-body"> <em class="small" data-ts="1641999600" data-df="ll" > Jan 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS</h3><div class="text-muted small"><p> Abstract Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded a...</p></div></div></a></div><div class="card"> <a href="/posts/NeRFmm/"><div class="card-body"> <em class="small" data-ts="1642863600" data-df="ll" > Jan 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NeRF−−: Neural Radiance Fields Without Known Camera Parameters</h3><div class="text-muted small"><p> Abstract This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses or intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recent...</p></div></div></a></div><div class="card"> <a href="/posts/uORF/"><div class="card-body"> <em class="small" data-ts="1643814000" data-df="ll" > Feb 3, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Unsupervised Discovery of Object Radiance Fields</h3><div class="text-muted small"><p> Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"><div class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></div><a href="/posts/post1/" class="btn btn-outline-primary" prompt="Newer"><p>TeX 문법 정리</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Seeha Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
