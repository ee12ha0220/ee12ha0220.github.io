<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Unsupervised Discovery of Object Radiance Fields" /><meta name="author" content="saha" /><meta property="og:locale" content="en" /><meta name="description" content="Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be learned in a unsupervised manner. Should explain the image formation process. Should be 3D-aware, capturing geometric and physical object properties. Until now, there is no work satisfying all these characteristics. In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields. During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision. During testing, uORF infers the set of object radiance fields from an single image." /><meta property="og:description" content="Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be learned in a unsupervised manner. Should explain the image formation process. Should be 3D-aware, capturing geometric and physical object properties. Until now, there is no work satisfying all these characteristics. In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields. During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision. During testing, uORF infers the set of object radiance fields from an single image." /><link rel="canonical" href="https://ee12ha0220.github.io/posts/uORF/" /><meta property="og:url" content="https://ee12ha0220.github.io/posts/uORF/" /><meta property="og:site_name" content="Saha’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-03T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Unsupervised Discovery of Object Radiance Fields" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@saha" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"saha","url":"https://github.com/ee12ha0220/"},"dateModified":"2022-02-03T00:00:00+09:00","datePublished":"2022-02-03T00:00:00+09:00","description":"Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be learned in a unsupervised manner. Should explain the image formation process. Should be 3D-aware, capturing geometric and physical object properties. Until now, there is no work satisfying all these characteristics. In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields. During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision. During testing, uORF infers the set of object radiance fields from an single image.","headline":"Unsupervised Discovery of Object Radiance Fields","mainEntityOfPage":{"@type":"WebPage","@id":"https://ee12ha0220.github.io/posts/uORF/"},"url":"https://ee12ha0220.github.io/posts/uORF/"}</script><title>Unsupervised Discovery of Object Radiance Fields | Saha's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Saha's Blog"><meta name="application-name" content="Saha's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/images/avatar_2.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Saha's Blog</a></div><div class="site-subtitle font-italic">공부한 것들을 까먹지 말자</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ee12ha0220" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ee12ha0220','kaist.ac.kr'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Unsupervised Discovery of Object Radiance Fields</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Unsupervised Discovery of Object Radiance Fields</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1643814000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Feb 3, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/ee12ha0220/">saha</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="546 words"> <em>3 min</em> read</span></div></div></div><div class="post-content"><h1 id="introduction">Introduction</h1><ul><li>Object-centric representation is a constant topic of interest in computer vision and machine learning.<li>Such representation should bear three characteristics:<ul><li>Should be learned in a unsupervised manner.<li>Should explain the image formation process.<li>Should be 3D-aware, capturing geometric and physical object properties.</ul><li>Until now, there is no work satisfying all these characteristics.<li>In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields.<li>During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision.<li>During testing, uORF infers the set of object radiance fields from an single image.</ul><h1 id="methods">Methods</h1><h3 id="convolutional-feature-extraction"><span class="mr-2">Convolutional feature extraction</span><a href="#convolutional-feature-extraction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Uses a convolutional net to extract features for the slot attention module.<li>Represents foreground object position and pose in the viewer coordinate system, in order to help learning the 3D object position and generalization.<li>So, feeds pixel coordinates and viewer-space ray direction as additional input channels to the encoder.</ul><h3 id="background-aware-slot-attention"><span class="mr-2">Background-aware slot attention</span><a href="#background-aware-slot-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Adopt <a href="https://ee12ha0220.github.io/posts/OCLSA">Slot Attention module</a> to produce a set of permutation-invariant latent codes.<li>Since the geometry and appearance of the background are usually highly different from those of foreground objects, explicitly separate the foreground objects and background.<li>To achieve this, make a single slot that lie in a different latent space from the other slots, for background features.</ul><h3 id="object-centric-encoding"><span class="mr-2">Object-centric Encoding</span><a href="#object-centric-encoding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Using convolutional feature extraction and background-aware slot attention, infers latent object-centric representations from a single image.<li><p>Given $N$ input features with dimension $D$, the slots are initialized by sampling from two learnable Gaussians. $slot^b$ denotes a single slot for background, and $slots^f$ denotes the slots for foreground objects.</p>\[slot^b \sim \mathcal{N}^b(\mu^b, diag(\sigma^b)) \in \mathbf{R}^{1\times D}, \, slots^f \sim \mathcal{N}^f(\mu^f, diag(\sigma^f)) \in \mathbf{R}^{K \times D}\]<li>The rest are similar to <a href="https://ee12ha0220.github.io/posts/OCLSA">Slot Attention module</a>. <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 90% 90%'%3E%3C/svg%3E" data-src="/assets/images/uORF_1.png" width="90%" height="90%" data-proofer-ignore></ul><h3 id="compositional-neural-rendering"><span class="mr-2">Compositional Neural Rendering</span><a href="#compositional-neural-rendering" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Use a Conditional NeRF $g(\cdot | z)$ that acts like an implicit decoder for each object. $z$ is the generated latent codes.<li><p>To compose individual objects and background into holistic scene, use a scene mixture model that uses density-weighted mean to combine all components.</p>\[\bar{\sigma} = \sum_{i=0}^K\omega_i\sigma_i, \, \bar{\mathbf{c}} = \sum_{i=0}^K\omega_i\mathbf{c}_i, \, \omega_i = \sigma_i/\sum_{j=0}^K\sigma_i\]</ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 50% 50%'%3E%3C/svg%3E" data-src="/assets/images/uORF_2.png" width="50%" height="50%" data-proofer-ignore></p><h3 id="loss-functions"><span class="mr-2">Loss functions</span><a href="#loss-functions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Use reconstruction loss, perceptual loss, and adversarial loss.<li>Since 3D radiance fields are estimated from a single view, there can be uncertainties about the appearance from other views. Therefore, perceptual loss, which is tolerant to mild appearance changes, is used.<li>There can also exist multi-model distribution. Therefore, adversarial loss which can deal with multi-model distributions is used.<li>Reconstruction loss : $\mathcal{L}_{recon} = ||\mathbf{I} - \hat{\mathbf{I}}||^2$ where $\mathbf{I}$ and $\hat{\mathbf{I}}$ denote the ground truth image and rendered image, respectively.<li>Perceptual loss : $\mathcal{L}_{percept} = ||p(\mathbf{I}) - p(\hat{\mathbf{I}})||^2$ where $p$ is a deep feature extractor.<li>Adversarial loss : $\mathcal{L}_{adv} = \mathbb{E}[f(D(\hat{\mathbf{I}}))] + \mathbb{E}[f(-D(\mathbf{I})) + \lambda_R|| \nabla D(\mathbf{I})||^2]$, where $f(t) = -log(1+exp(-t))$</ul><h3 id="coarse-to-fine-progressive-training"><span class="mr-2">Coarse-to-fine Progressive Training</span><a href="#coarse-to-fine-progressive-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Training compositional NeRF requires immense computational cost. To allow training on a higher resolution, coarse-to-fine progressive training is used.<li>In a coarse training stage, uORF is trained on the bilinearly downsampled images to a base resolution.<li>In the following fine training stage, the model is refined by training on patches randomly cropped from images of the higher target resolution.</ul><h3 id="the-overall-process"><span class="mr-2">The overall process</span><a href="#the-overall-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 90% 90%'%3E%3C/svg%3E" data-src="/assets/images/uORF_cover.png" width="90%" height="90%" data-proofer-ignore></p><h1 id="results">Results</h1><ul><li>Refer to <a href="https://arxiv.org/abs/2107.07905">original paper</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>Paper review</a>, <a href='/categories/nvs/'>NVS</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Unsupervised+Discovery+of+Object+Radiance+Fields+-+Saha%27s+Blog&url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FuORF%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Unsupervised+Discovery+of+Object+Radiance+Fields+-+Saha%27s+Blog&u=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FuORF%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FuORF%2F&text=Unsupervised+Discovery+of+Object+Radiance+Fields+-+Saha%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">Denoising Diffusion Probabilistic Models</a><li><a href="/posts/SMLD/">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="/posts/DDIM/">DENOISING DIFFUSION IMPLICIT MODELS</a><li><a href="/posts/fn/">각종 코드 정리</a><li><a href="/posts/MLMC/">A Machine Learning Approach for Filtering Monte Carlo Noise</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/NeRF/"><div class="card-body"> <em class="small" data-ts="1641308400" data-df="ll" > Jan 5, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h3><div class="text-muted small"><p> Abstract We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse s...</p></div></div></a></div><div class="card"> <a href="/posts/NeRFpp/"><div class="card-body"> <em class="small" data-ts="1641999600" data-df="ll" > Jan 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS</h3><div class="text-muted small"><p> Abstract Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded a...</p></div></div></a></div><div class="card"> <a href="/posts/NeRFmm/"><div class="card-body"> <em class="small" data-ts="1642863600" data-df="ll" > Jan 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NeRF−−: Neural Radiance Fields Without Known Camera Parameters</h3><div class="text-muted small"><p> Abstract This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses or intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recent...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/OCLSA/" class="btn btn-outline-primary" prompt="Older"><p>Object-Centric Learning with Slot Attention</p></a> <a href="/posts/fn/" class="btn btn-outline-primary" prompt="Newer"><p>각종 코드 정리</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Seeha Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
