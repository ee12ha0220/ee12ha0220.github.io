<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="DENOISING DIFFUSION IMPLICIT MODELS" /><meta name="author" content="saha" /><meta property="og:locale" content="en" /><meta name="description" content="Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error." /><meta property="og:description" content="Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error." /><link rel="canonical" href="https://ee12ha0220.github.io/posts/DDIM/" /><meta property="og:url" content="https://ee12ha0220.github.io/posts/DDIM/" /><meta property="og:site_name" content="Saha’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-23T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="DENOISING DIFFUSION IMPLICIT MODELS" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@saha" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"saha","url":"https://github.com/ee12ha0220/"},"dateModified":"2022-09-27T17:22:25+09:00","datePublished":"2022-08-23T00:00:00+09:00","description":"Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.","headline":"DENOISING DIFFUSION IMPLICIT MODELS","mainEntityOfPage":{"@type":"WebPage","@id":"https://ee12ha0220.github.io/posts/DDIM/"},"url":"https://ee12ha0220.github.io/posts/DDIM/"}</script><title>DENOISING DIFFUSION IMPLICIT MODELS | Saha's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Saha's Blog"><meta name="application-name" content="Saha's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/images/avatar_2.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Saha's Blog</a></div><div class="site-subtitle font-italic">공부한 것들을 까먹지 말자</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ee12ha0220" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ee12ha0220','kaist.ac.kr'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>DENOISING DIFFUSION IMPLICIT MODELS</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>DENOISING DIFFUSION IMPLICIT MODELS</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1661180400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 23, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/ee12ha0220/">saha</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1573 words"> <em>8 min</em> read</span></div></div></div><div class="post-content"><h1 id="abstract">Abstract</h1><p>Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.</p><h1 id="introduction">Introduction</h1><p>Deep generative model은 다양한 deep learnig domain에서 사용된다. 최근에는 diffusion probabilistic model(DDPM, Ho et al. 2020), noise conditional score networks(NCSN, Song &amp; Ermon 2019)같은 iterative generative model이 GAN에 버금가는 좋은 성능을 보이고 있다. 하지만 이러한 model들의 단점 중 하나는 많은 iteration step(적어도 1000 이상)을 필요로 한다는 것이고, 이는 generation에 긴 시간이 걸린다는 것을 뜻한다. 본 논문에서는 DDPM의 iteration step을 줄이기 위해 <strong><em>denoising diffusion implicit model(DDIM)</em></strong> 을 제시한다. DDIM에서는 DDPM에서 makov chain으로 정의되었던 forward process를 objective이 같은 non-markovian chain으로 새롭게 design한다. 이를 통해 더 적은 step으로 reconstruction이 가능하게 만들고, 추가적으로 reconstruction을 할 때마다 달라지는 DDPM과는 달리 stable하게 reconstruction을 할 수 있도록 해준다.</p><h1 id="variational-inference-for-non-markovian-forward-process">Variational inference for non-markovian forward process</h1><h2 id="non-markovian-forward-process"><span class="mr-2">Non-markovian forward process</span><a href="#non-markovian-forward-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DDIM은 <a href="https://ee12ha0220.github.io/posts/DDPM/">DDPM</a>위에서 정의된다. DDPM에서는 다음과 같은 forward process를 사용한다(\ref{eq1}).</p><hr /> \[q(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\sqrt{\alpha_t}\mathbf{x}_t, (1-\alpha_t)\mathbf{I}) \label{eq1} \tag{1}\]<hr /><p>이는 오직 $\mathbf{x} _t$에만 의존하는 markov chain이다. 이를 이용하면 $\mathbf{x} _0$에서 $\mathbf{x} _t$를 바로 구할 수 있다(\ref{eq2}).</p><hr /> \[\begin{align} \mathbf{x}_t &amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\epsilon \\ &amp;= \sqrt{\alpha_t\alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon\\ &amp;= \ldots \\ &amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \label{eq2} \tag{2} \end{align}\]<hr /><p>이때 $\epsilon$은 standard Gaussian distribution이다. 즉, $\mathbf{x} _0$가 주어졌을 때 $\mathbf{x} _t$의 distribution을 다음과 같이 쓸 수 있다(\ref{eq3}).</p><hr /> \[q(\mathbf{x_t|\mathbf{x}_0}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x_0}, (1-\bar{\alpha}_t)\mathbf{I}) \label{eq3} \tag{3}\]<hr /><p>DDIM은 equation \ref{eq3}을 만족시키는 non-markovian forward process를 사용하고자 한다. 여기에는 사실 $\alpha _t$가 등장하지 않기 때문에, equation \ref{eq3}을 다음과 같이 새롭게 쓰고 시작한다(\ref{eq4}).</p><hr /> \[q(\mathbf{x_t|\mathbf{x}_0}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t}\mathbf{x_0}, (1-\alpha_t)\mathbf{I}) \label{eq4} \tag{4}\]<hr /><p>Equation 4를 만족시키는 forward process를 얻기 위해, DDIM에서는 다음과 같은 backward process를 사용한다(\ref{eq5}).</p><hr /> \[q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_{t-1}}\mathbf{x}_0 + \sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t - \sqrt{\alpha_t}\mathbf{x_0}}{\sqrt{1-\alpha_t}}, \sigma_t^2\mathbf{I}) \label{eq5} \tag{5}\]<hr /><p>Forward process는 다음 식을 이용해서 구할 수 있다(\ref{eq6}).</p><hr /> \[q_\sigma(\mathbf{x}_{t}|\mathbf{x}_{t-1},\mathbf{x}_0) = \frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)q_\sigma(\mathbf{x}_{t}|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)} \label{eq6} \tag{6}\]<hr /><p>만약 이때 $\sigma\rightarrow 0$ 이 된다면, $\mathbf{x} _0$과 $\mathbf{x} _{t}$에 deterministic하게 $\mathbf{x} _{t-1}$이 결정되게 된다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100% 100%'%3E%3C/svg%3E" data-src="/assets/images/DDIM_1.png" width="100%" height="100%" data-proofer-ignore><em>DDPM(왼쪽)과 달리 DDIM(오른쪽)은 forward process를 $\mathbf{x} _0$를 이용해서 정의한다.</em></p><h2 id="generative-process-and-unified-variational-inferece-objective"><span class="mr-2">Generative process and unified variational inferece objective</span><a href="#generative-process-and-unified-variational-inferece-objective" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DDIM에서는 $q _\sigma(\mathbf{x} _{t-1}|\mathbf{x} _t,\mathbf{x} _0)$를 이용해 trainable generative process $p ^{(t)} _\theta(\mathbf{x} _{t-1}|\mathbf{x} _t)$를 정의한다. 간단하게 설명하면 $\mathbf{x} _t$를 이용해 해당하는 $\mathbf{x} _0$를 predict한 다음, 이를 이용해서 $\mathbf{x} _{t-1}$을 얻는 것이다. Equation 4를 이용하면 $\mathbf{x} _t$를 $\mathbf{x} _0 \sim q(\mathbf{x} _0)$, $\epsilon _t \sim \mathcal{N}(0, I)$를 이용해 나타낼 수 있다. 여기서 다음과 같이 $\mathbf{x} _0$에 대한 정보가 없이 $\epsilon _t$를 predict하려고 시도한다(\ref{eq7}).</p><hr /> \[\begin{align} \mathbf{x}_t &amp;= \sqrt{\alpha_t}\mathbf{x}_0 + \sqrt{1-\alpha_t}\epsilon_t \\ \mathbf{x}_0 &amp;= (\mathbf{x}_t -\sqrt{1-\alpha_t}\cdot\epsilon_t)\sqrt{\alpha_t} \\ f_\theta^{(t)}(\mathbf{x}_t) :&amp;= (\mathbf{x}_t-\sqrt{1-\alpha_t}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t))/\sqrt{\alpha_t} \end{align} \label{eq7} \tag{7}\]<hr /><p>그러면 backward process를 다음과 같이 정의할 수 있다(\ref{eq8}).</p><hr /> \[p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}) = \begin{cases} \mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1), \sigma_1^2\mathbf{I}) &amp;\text{if } t=1 \\ q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,f_\theta^{(t)}(\mathbf{x}_t)) &amp; \text{otherwise} \end{cases} \label{eq8} \tag{8}\]<hr /><h1 id="sampling-from-generalized-generative-processes">Sampling from generalized generative processes</h1><p>DDIM은 기본적으로 DDPM과 같은 loss function을 이용해 학습된다. 그렇기 때문에 sampling 방식만 바꿔준다면, DDPM을 기반을 학습된 model을 그대로 사용할 수 있다.</p><h2 id="denoising-diffusion-implicit-models"><span class="mr-2">Denoising diffusion implicit models</span><a href="#denoising-diffusion-implicit-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DDIM에서 $\mathbf{x} _{t-1}$은 다음과 같이 나타낼 수 있다(\ref{eq9}).</p><hr /> \[\mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}}\left( \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t)}{\sqrt{\alpha_t}} \right) + \sqrt{1-\alpha_{t-1} - \sigma^2_t}\cdot \epsilon_\theta(\mathbf{x}_t) + \sigma_t\epsilon_t \label{eq9} \tag{9}\]<hr /><p>여기서 $\sigma _t$의 값에 따라 $\epsilon _\theta$를 다시 학습하지 않고도 다양한 generative process를 modeling 할 수 있고, 만약 $\sigma _t = \sqrt{(1-\alpha _{t-1})/(1-\alpha _t)} \sqrt{1-\alpha _t/\alpha _{t-1}}$으로 설정하면 DDPM이 된다. 만약 $\sigma _t$의 값을 0으로 설정한다면 $\mathbf{x} _{t-1}$이 deterministic하게 결정되는데, 이를 <em>denoisiong diffusion implict model(DDIM)</em> 이라 한다.</p><h2 id="accelerated-generation-process"><span class="mr-2">Accelerated generation process</span><a href="#accelerated-generation-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DDIM에서 사용하는 loss function을 다시 써보면 아래와 같다(\ref{eq10}).</p><hr /> \[\text{loss} = ||\epsilon_t - \epsilon_\theta(\underbrace{\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon_t}_{\mathbf{x}_t}, t)||^2 \label{eq10} \tag{10}\]<hr /><p>이때 input으로 주어지는 $\mathbf{x} _t$는 $\mathbf{x} _0$와 $\bar{\alpha} _t$에 의해 결정되는데, 이는 결국 $\bar{\alpha} _t = \prod _{i=1}^t\alpha_t$가 중요한 것이지 각 $\alpha _t$는 달라도 상관없다는 의미를 내포하고 있다. 그렇기 때문에 1에서 $T$까지 모든 $\bar{\alpha} _t$를 사용하는 것이라, 이중 일부만을 사용해서 generation process의 속도를 향상시킬 수 있다. 이런 경우에는 DDPM보다 DDIM이 더 좋은 성능을 보였다.</p><h1 id="training-details">Training details</h1><p>실험은 DDPM과 DDIM 모두 $T=1000$의 조건 하에 학습한 model을 사용했다. Dataset은 CIFAR 10, CelebA 를 사용했다.</p><h1 id="results">Results</h1><p>더 적은 step을 이용해 reconstruction을 했을 때 DDPM보다 DDIM이 더 좋은 결과를 보였다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 80% 80%'%3E%3C/svg%3E" data-src="/assets/images/DDIM_2.png" width="80%" height="80%" data-proofer-ignore><em>$\eta$는 variance 부분에 곱해지는 상수로,1일때가 DDPM, 0일때가 DDIM이다. $\hat{\sigma}$의 경우도 DDPM이다. $S$를 full scale(1000)로 했을 때는 DDPM이 성능이 더 좋지만, 작은 $S$를 사용했을 때 DDIM의 성능이 훨씬 좋았다.</em></p><h1 id="conclusion">Conclusion</h1><p>We have presented DDIMs – an implicit generative model trained with denoising auto-encoding / score matching objectives – from a purely variational perspective. DDIM is able to generate high quality samples much more efficiently than existing DDPMs and NCSNs, with the ability to perform meaningful interpolations from the latent space. The non-Markovian forward process presented here seems to suggest continuous forward processes other than Gaussian (which cannot be done in the original diffusion framework, since Gaussian is the only stable distribution with finite variance).</p><p>Moreover, since the sampling procedure of DDIMs is similar to that of an neural ODE, it would be interesting to see if methods that decrease the discretization error in ODEs, including multi- step methods such as Adams-Bashforth (Butcher &amp; Goodwin, 2008), could be helpful for further improving sample quality in fewer steps (Queiruga et al., 2020). It is also relevant to investigate whether DDIMs exhibit other properties of existing implicit models (Bau et al., 2019).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>Paper review</a>, <a href='/categories/image-synthesis/'>Image synthesis</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DENOISING+DIFFUSION+IMPLICIT+MODELS+-+Saha%27s+Blog&url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FDDIM%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DENOISING+DIFFUSION+IMPLICIT+MODELS+-+Saha%27s+Blog&u=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FDDIM%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fee12ha0220.github.io%2Fposts%2FDDIM%2F&text=DENOISING+DIFFUSION+IMPLICIT+MODELS+-+Saha%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">Denoising Diffusion Probabilistic Models</a><li><a href="/posts/SMLD/">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="/posts/DDIM/">DENOISING DIFFUSION IMPLICIT MODELS</a><li><a href="/posts/fn/">각종 코드 정리</a><li><a href="/posts/MLMC/">A Machine Learning Approach for Filtering Monte Carlo Noise</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/PixelRNN/"><div class="card-body"> <em class="small" data-ts="1657983600" data-df="ll" > Jul 17, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pixel Recurrent Neural Networks</h3><div class="text-muted small"><p> Abstract Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We presen...</p></div></div></a></div><div class="card"> <a href="/posts/PixelCNNpp/"><div class="card-body"> <em class="small" data-ts="1658674800" data-df="ll" > Jul 25, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS</h3><div class="text-muted small"><p> Abstract PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available here. Our implemen...</p></div></div></a></div><div class="card"> <a href="/posts/DDPM/"><div class="card-body"> <em class="small" data-ts="1659366000" data-df="ll" > Aug 2, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Denoising Diffusion Probabilistic Models</h3><div class="text-muted small"><p> Abstract We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Palette/" class="btn btn-outline-primary" prompt="Older"><p>Palette: Image-to-Image Diffusion Models</p></a> <a href="/posts/DDPM/" class="btn btn-outline-primary" prompt="Newer"><p>Denoising Diffusion Probabilistic Models</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Seeha Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tag/">tag</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
