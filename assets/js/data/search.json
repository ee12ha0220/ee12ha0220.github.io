[ { "title": "Denoising Diffusion Probabilistic Models", "url": "/posts/DDPM/", "categories": "Paper review, Diffusion model", "tags": "tag", "date": "2023-01-14 00:00:00 +0900", "snippet": "AbstractWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat- urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple- mentation is available at https://github.com/hojonathanho/diffusion.IntroductionDiffusion model은 작은 noise를 iteratively하게 더하는 forward process가 주어졌을 때, 그 backward process를 학습하고자 하는 model이다. Denoising Diffusion Probabilistic Models (DDPM)은 diffusion model의 일종으로, forward process를 Gaussian noise로 정의한다.DDPM의 framework. Forward process ($q$)와 backward process ($p$)는 모두 Gaussian distribution으로 modeling된다.Proposed methodForward processDDPM에서 forward process는 다음과 같은 Markov chain으로 정의된다 :\\[q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t;\\sqrt{\\alpha_t}\\mathbf{x}_{t-1}, (1-\\alpha_t)\\mathbf{I})\\]이때 $\\alpha_t$는 pre-defined noise schedule로, distribution의 급격한 변화를 방지하기 위해 1과 가까운 값으로 설정된다. Forward process에 reparameterization trick를 적용하면 $\\mathbf{x}_t$를 $\\mathbf{x}_0$를 이용해서 표현할 수 있다 :\\[\\begin{align}\\mathbf{x}_t &amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon \\\\&amp;= \\sqrt{\\alpha_t\\alpha_{t-1}}\\mathbf{x}_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon\\\\&amp;= \\ldots \\\\&amp;= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon \\\\q(\\mathbf{x}_t|\\mathbf{x}_0) &amp;= \\mathcal{N}(\\mathbf{x}_t;\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_t)\\mathbf{I})\\end{align}\\]이때 $\\epsilon \\sim \\mathcal{N}(0,I)$이고, $\\bar{\\alpha} _t = \\prod _{i=1}^t\\alpha_i$이다. 즉, $\\mathbf{x}_0$와 $t$가 주어지면 쉽게 $\\mathbf{x}_t$를 계산할 수 있다.Inverse forward processInverse forward process는 다음과 같이 정의된다 :\\[q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\mu}(\\mathbf{x_t}, \\mathbf{x}_0), \\tilde{\\sigma}^2(t)\\mathbf{I})\\]이는 다음과 같이 유도할 수 있다 :\\[\\begin{align}q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) &amp;= q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{x}_{0})\\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{0})} \\\\&amp; \\propto \\text{exp}\\left( -\\frac{1}{2}\\left(\\frac{(\\mathbf{x}_{t} - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\&amp;= \\text{exp}\\left( -\\frac{1}{2} \\left( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) \\mathbf{x}^2_{t-1} -(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)\\mathbf{x}_{t-1} + C(\\mathbf{x}_t, \\mathbf{x}_0) \\right) \\right) \\\\ \\\\\\tilde{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0) &amp;= \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_t)}{1-\\bar{\\alpha}_t}\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_t \\right) \\\\\\tilde{\\sigma}^2(t) &amp;= \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} (1-\\alpha_t)\\end{align}\\]Backward processDDPM에서 backward process는 다음과 같이 정의된다 :\\[p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1};\\mu_\\theta(\\mathbf{x}_t, \\alpha_t), \\sigma^2_\\theta(\\mathbf{x}_t, \\alpha_t))\\]Mean($\\mu_\\theta$)과 variance($\\sigma^2_\\theta$)를 학습하기 위해 backward process를 inverse forward process에 근사하게 된다 :\\[\\mu_\\theta(\\mathbf{x}_t, \\alpha_t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(\\mathbf{x}_t, t) \\right), \\,\\sigma_\\theta(\\mathbf{x}_t, \\alpha_t) = \\tilde{\\sigma}(t)\\]Network architectureSelf-attention block이 포함된 U-Net 구조를 사용했다. Network에 들어가기 전에 input에 positonal encoding을 적용해줬고, activation function으로는 SiLU function($x\\cdot\\sigma(x)$)을 사용해줬다.DDPM에서 사용한 network. Self-attention block이 포함된 U-Net 구조를 사용했다.Training processBackward process를 보면 $\\epsilon_\\theta(\\mathbf{x}_t,t)$만 학습하면 된다는 것을 알 수 있다. 즉, DDPM의 loss function은 다음과 같이 쓸 수 있다 :\\[\\mathcal{L}_\\text{DDPM} = \\Vert \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t,t) \\Vert^2_2 = \\Vert \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t,t) \\Vert^2_2\\]학습 과정의 각 step마다 $t \\in [1;T]$를 random하게 골라서 학습하게 된다.Sampling process학습된 backward process를 이용하면 $\\mathbf{x}_T \\sim \\mathcal{N}(0,I)$부터 시작해서 $\\mathbf{x}_0 \\sim p(\\mathbf{x})$까지 sampling 할 수 있다 :\\[\\begin{align}\\text{for t} &amp;\\in [T,\\ldots,1]\\\\\\mathbf{x}_{t-1} &amp;= \\mu_\\theta(\\mathbf{x}_t,t)+\\sigma_\\theta(\\mathbf{x}_t,t)\\epsilon\\\\&amp;= \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(\\mathbf{x}_t, t) \\right) + \\sigma_t\\epsilon\\end{align}\\]" }, { "title": "DENOISING DIFFUSION IMPLICIT MODELS", "url": "/posts/DDIM/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-23 00:00:00 +0900", "snippet": "AbstractDenoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.IntroductionDeep generative model은 다양한 deep learnig domain에서 사용된다. 최근에는 diffusion probabilistic model(DDPM, Ho et al. 2020), noise conditional score networks(NCSN, Song &amp; Ermon 2019)같은 iterative generative model이 GAN에 버금가는 좋은 성능을 보이고 있다. 하지만 이러한 model들의 단점 중 하나는 많은 iteration step(적어도 1000 이상)을 필요로 한다는 것이고, 이는 generation에 긴 시간이 걸린다는 것을 뜻한다. 본 논문에서는 DDPM의 iteration step을 줄이기 위해 denoising diffusion implicit model(DDIM) 을 제시한다. DDIM에서는 DDPM에서 makov chain으로 정의되었던 forward process를 objective이 같은 non-markovian chain으로 새롭게 design한다. 이를 통해 더 적은 step으로 reconstruction이 가능하게 만들고, 추가적으로 reconstruction을 할 때마다 달라지는 DDPM과는 달리 stable하게 reconstruction을 할 수 있도록 해준다.Variational inference for non-markovian forward processNon-markovian forward processDDIM은 DDPM위에서 정의된다. DDPM에서는 다음과 같은 forward process를 사용한다(\\ref{eq1}).\\[q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1};\\sqrt{\\alpha_t}\\mathbf{x}_t, (1-\\alpha_t)\\mathbf{I}) \\label{eq1} \\tag{1}\\]이는 오직 $\\mathbf{x} _t$에만 의존하는 markov chain이다. 이를 이용하면 $\\mathbf{x} _0$에서 $\\mathbf{x} _t$를 바로 구할 수 있다(\\ref{eq2}).\\[\\begin{align}\\mathbf{x}_t &amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon \\\\&amp;= \\sqrt{\\alpha_t\\alpha_{t-1}}\\mathbf{x}_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon\\\\&amp;= \\ldots \\\\&amp;= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon \\label{eq2} \\tag{2}\\end{align}\\]이때 $\\epsilon$은 standard Gaussian distribution이다. 즉, $\\mathbf{x} _0$가 주어졌을 때 $\\mathbf{x} _t$의 distribution을 다음과 같이 쓸 수 있다(\\ref{eq3}).\\[q(\\mathbf{x_t|\\mathbf{x}_0}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x_0}, (1-\\bar{\\alpha}_t)\\mathbf{I}) \\label{eq3} \\tag{3}\\]DDIM은 equation \\ref{eq3}을 만족시키는 non-markovian forward process를 사용하고자 한다. 여기에는 사실 $\\alpha _t$가 등장하지 않기 때문에, equation \\ref{eq3}을 다음과 같이 새롭게 쓰고 시작한다(\\ref{eq4}).\\[q(\\mathbf{x_t|\\mathbf{x}_0}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t}\\mathbf{x_0}, (1-\\alpha_t)\\mathbf{I}) \\label{eq4} \\tag{4}\\]Equation 4를 만족시키는 forward process를 얻기 위해, DDIM에서는 다음과 같은 backward process를 사용한다(\\ref{eq5}).\\[q_\\sigma(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0) = \\mathcal{N}(\\sqrt{\\alpha_{t-1}}\\mathbf{x}_0 + \\sqrt{1-\\alpha_{t-1}-\\sigma_t^2}\\cdot\\frac{\\mathbf{x}_t - \\sqrt{\\alpha_t}\\mathbf{x_0}}{\\sqrt{1-\\alpha_t}}, \\sigma_t^2\\mathbf{I}) \\label{eq5} \\tag{5}\\]Forward process는 다음 식을 이용해서 구할 수 있다(\\ref{eq6}).\\[q_\\sigma(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{x}_0) = \\frac{q_\\sigma(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q_\\sigma(\\mathbf{x}_{t}|\\mathbf{x}_0)}{q_\\sigma(\\mathbf{x}_{t-1}|\\mathbf{x}_0)} \\label{eq6} \\tag{6}\\]만약 이때 $\\sigma\\rightarrow 0$ 이 된다면, $\\mathbf{x} _0$과 $\\mathbf{x} _{t}$에 deterministic하게 $\\mathbf{x} _{t-1}$이 결정되게 된다.DDPM(왼쪽)과 달리 DDIM(오른쪽)은 forward process를 $\\mathbf{x} _0$를 이용해서 정의한다.Generative process and unified variational inferece objectiveDDIM에서는 $q _\\sigma(\\mathbf{x} _{t-1}|\\mathbf{x} _t,\\mathbf{x} _0)$를 이용해 trainable generative process $p ^{(t)} _\\theta(\\mathbf{x} _{t-1}|\\mathbf{x} _t)$를 정의한다. 간단하게 설명하면 $\\mathbf{x} _t$를 이용해 해당하는 $\\mathbf{x} _0$를 predict한 다음, 이를 이용해서 $\\mathbf{x} _{t-1}$을 얻는 것이다. Equation 4를 이용하면 $\\mathbf{x} _t$를 $\\mathbf{x} _0 \\sim q(\\mathbf{x} _0)$, $\\epsilon _t \\sim \\mathcal{N}(0, I)$를 이용해 나타낼 수 있다. 여기서 다음과 같이 $\\mathbf{x} _0$에 대한 정보가 없이 $\\epsilon _t$를 predict하려고 시도한다(\\ref{eq7}).\\[\\begin{align}\\mathbf{x}_t &amp;= \\sqrt{\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\alpha_t}\\epsilon_t \\\\\\mathbf{x}_0 &amp;= (\\mathbf{x}_t -\\sqrt{1-\\alpha_t}\\cdot\\epsilon_t)\\sqrt{\\alpha_t} \\\\f_\\theta^{(t)}(\\mathbf{x}_t) :&amp;= (\\mathbf{x}_t-\\sqrt{1-\\alpha_t}\\cdot\\epsilon_\\theta^{(t)}(\\mathbf{x}_t))/\\sqrt{\\alpha_t}\\end{align} \\label{eq7} \\tag{7}\\]그러면 backward process를 다음과 같이 정의할 수 있다(\\ref{eq8}).\\[p_\\theta^{(t)}(\\mathbf{x}_{t-1}|\\mathbf{x}) = \\begin{cases}\\mathcal{N}(f_\\theta^{(1)}(\\mathbf{x}_1), \\sigma_1^2\\mathbf{I}) &amp;\\text{if } t=1 \\\\q_\\sigma(\\mathbf{x}_{t-1}|\\mathbf{x}_t,f_\\theta^{(t)}(\\mathbf{x}_t)) &amp; \\text{otherwise}\\end{cases} \\label{eq8} \\tag{8}\\]Sampling from generalized generative processesDDIM은 기본적으로 DDPM과 같은 loss function을 이용해 학습된다. 그렇기 때문에 sampling 방식만 바꿔준다면, DDPM을 기반을 학습된 model을 그대로 사용할 수 있다.Denoising diffusion implicit modelsDDIM에서 $\\mathbf{x} _{t-1}$은 다음과 같이 나타낼 수 있다(\\ref{eq9}).\\[\\mathbf{x}_{t-1} = \\sqrt{\\alpha_{t-1}}\\left( \\frac{\\mathbf{x}_t - \\sqrt{1-\\alpha_t}\\epsilon_\\theta(\\mathbf{x}_t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1-\\alpha_{t-1} - \\sigma^2_t}\\cdot \\epsilon_\\theta(\\mathbf{x}_t) + \\sigma_t\\epsilon_t \\label{eq9} \\tag{9}\\]여기서 $\\sigma _t$의 값에 따라 $\\epsilon _\\theta$를 다시 학습하지 않고도 다양한 generative process를 modeling 할 수 있고, 만약 $\\sigma _t = \\sqrt{(1-\\alpha _{t-1})/(1-\\alpha _t)} \\sqrt{1-\\alpha _t/\\alpha _{t-1}}$으로 설정하면 DDPM이 된다. 만약 $\\sigma _t$의 값을 0으로 설정한다면 $\\mathbf{x} _{t-1}$이 deterministic하게 결정되는데, 이를 denoisiong diffusion implict model(DDIM) 이라 한다.Accelerated generation processDDIM에서 사용하는 loss function을 다시 써보면 아래와 같다(\\ref{eq10}).\\[\\text{loss} = ||\\epsilon_t - \\epsilon_\\theta(\\underbrace{\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t}_{\\mathbf{x}_t}, t)||^2 \\label{eq10} \\tag{10}\\]이때 input으로 주어지는 $\\mathbf{x} _t$는 $\\mathbf{x} _0$와 $\\bar{\\alpha} _t$에 의해 결정되는데, 이는 결국 $\\bar{\\alpha} _t = \\prod _{i=1}^t\\alpha_t$가 중요한 것이지 각 $\\alpha _t$는 달라도 상관없다는 의미를 내포하고 있다. 그렇기 때문에 1에서 $T$까지 모든 $\\bar{\\alpha} _t$를 사용하는 것이라, 이중 일부만을 사용해서 generation process의 속도를 향상시킬 수 있다. 이런 경우에는 DDPM보다 DDIM이 더 좋은 성능을 보였다.Training details실험은 DDPM과 DDIM 모두 $T=1000$의 조건 하에 학습한 model을 사용했다. Dataset은 CIFAR 10, CelebA 를 사용했다.Results더 적은 step을 이용해 reconstruction을 했을 때 DDPM보다 DDIM이 더 좋은 결과를 보였다.$\\eta$는 variance 부분에 곱해지는 상수로,1일때가 DDPM, 0일때가 DDIM이다. $\\hat{\\sigma}$의 경우도 DDPM이다. $S$를 full scale(1000)로 했을 때는 DDPM이 성능이 더 좋지만, 작은 $S$를 사용했을 때 DDIM의 성능이 훨씬 좋았다.ConclusionWe have presented DDIMs – an implicit generative model trained with denoising auto-encoding / score matching objectives – from a purely variational perspective. DDIM is able to generate high quality samples much more efficiently than existing DDPMs and NCSNs, with the ability to perform meaningful interpolations from the latent space. The non-Markovian forward process presented here seems to suggest continuous forward processes other than Gaussian (which cannot be done in the original diffusion framework, since Gaussian is the only stable distribution with finite variance).Moreover, since the sampling procedure of DDIMs is similar to that of an neural ODE, it would be interesting to see if methods that decrease the discretization error in ODEs, including multi- step methods such as Adams-Bashforth (Butcher &amp; Goodwin, 2008), could be helpful for further improving sample quality in fewer steps (Queiruga et al., 2020). It is also relevant to investigate whether DDIMs exhibit other properties of existing implicit models (Bau et al., 2019)." }, { "title": "Palette: Image-to-Image Diffusion Models", "url": "/posts/Palette/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-18 00:00:00 +0900", "snippet": "AbstractThis paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.IntroductionComputer vision 분야에서 많은 문제들은 image-to-image translation으로 여겨질 수 있다. 이는 대부분 input image들이 주어졌을 때, output image들의 conditional distribution을 얻는 방법으로 이루어진다. 여러 model들 중에서 특히 Generative adversarial networks(GANs)는 다양한 image-to-image task에 효과적이라고 보고되었지만, GAN은 학습하는 과정이 까다롭다는 큰 단점이 있다. 이와 별개로 Autoregressive Models, VAEs, Normalizing Flows 등 다양한 model들이 소개되었지만, 이들은 GAN과 비교했을 때 좋은 성능을 보이지는 못했다.본 논문(Palette)에서는 최근에 뛰어난 성능올 보인 diffusion model을 이용해서 image-to-image translation을 하고자 한다. Palette에서 다루는 tasks는 colorization, inpainting, uncropping, 그리고 JPEG restoration이다.Image-to-image diffusion modelsPalette에서는 image-to-image diffusion model을 image $\\mathbf{x}$와 $\\mathbf{y}$에 대해 $p(\\mathbf{y}|\\mathbf{x})$ 라고 정의한다. 예를 들어 colorization에서는 $\\mathbf{x}$가 gray scale image, $\\mathbf{y}$가 color image가 된다.Palette는 기본적으로 DDPM과 동일한 구조를 사용하는데, 달라진 점이라면 input image를 prior로 사용한다는 점이다. Optimization에 사용되는 loss를 수식으로 나타내면 다음과 같다(\\ref{eq1}).\\[\\mathbb{E}_{(x,y)}\\mathbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,I)}\\mathbb{E}_\\gamma ||f_\\theta(\\mathbf{x}, \\sqrt{\\gamma}\\mathbf{y}+\\sqrt{1-\\gamma}\\epsilon, \\gamma)-\\epsilon||^p_p \\label{eq1} \\tag{1}\\]이때 $\\gamma$는 DDPM에서 $\\beta$와 같은 의미로, forward process의 schedule을 나타낸다. Training, reconstruction 과정은 DDPM과 동일하다.TasksPalette는 다음의 image-to-image task들을 수행한다. Colorization: Transforms an input grayscale image to a plausible color image. Inpainting: Fills in user-specified masked regions of an image with realistic content. Uncropping: Extends an input image along one or more directions to enlarge the image. JPEG restoration: Corrects for JPEG compression artifacts,restoring plausible image detail. ResultsPalette model은 모든 task에서 뛰어난 성능을 보였다. 그리고 여러가지 task가 섞여 있는 multi-task learning에서도 뛰어난 성능을 보였다. 정확한 수치와 더 specific한 내용은 Palette 논문를 참조하길 바란다.Colorization 결과Inpainting 결과Uncropping 결과JPEG restoration 결과Multi-task learning 결과. 맨 오른쪽은 inpainting에 대해서만 학습된 model이다.ConclusionWe present Palette, a simple, general framework for image-to-image translation. Palette achieves strong results on four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG restoration), outperforming strong GAN and regression baselines. Unlike many GAN models, Palette produces diverse and high fidelity outputs. This is accomplished without task-specific customization nor optimization instability. We also present a multi-task Palette model, that performs just as well or better over their task-specific counterparts. Further exploration and investigation of multi-task diffusion models is an exciting avenue for future work. This paper shows some of the potential of image-to-image diffusion models, but we look forward to seeing new applications." }, { "title": "Image Super-Resolution via Iterative Refinement", "url": "/posts/SR3/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-13 00:00:00 +0900", "snippet": "AbstractWe present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models [17, 48] to conditional image generation and performs super-resolution through a stochastic iterative denoising process. Output generation starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.IntroductionSingle-image super-resolution은 input된 low-resolution image와 동일한 quality의 high-resolution image를 generate하는 것이다. 이는 colorization, in-painting, de-blurring등과 같은 image-to-image translation task의 범주에 속한다. 이러한 문제들과 마찬가지로 single-image super-resolution 역시 high-resolution image의 distribution이 일반적인 parametric distribution(예를 들면, multivariate Gaussian)으로 표현되기가 힘들기 때문에 이 문제 역시 어려운 문제가 된다. Autoregressive models, VAEs, Normalizing Flows(NFs), 그리고 GANs같은 deep-generative model들은 이러한 문제들을 해결하는 데에 좋은 성능을 보이지만, 이들은 너무 heavy하거나 성능 부분에 약간의 issue가 있거나, 학습을 하는 것이 너무 어려운 등 각종 문제들이 있다.본 논문(SR3)에서는 이를 해결하고자 Super-Resolution via Repeated Refinement 을 제시한다. 이는 최근에 뛰어난 성능을 보인 DDPM과 비슷하지만, 사용되는 U-Net의 구조에 변화를 줘서 conditional generation에 적합하게 변화시켰다.Conditional Denoising Diffusion modelConditional denoising diffusion model은 source image $\\mathbf{x}$와 target image $\\mathbf{y}$에 대해, $p(\\mathbf{y}|\\mathbf{x})$의 parametric approximation을 알아내는 것을 목표로 한다. 이 과정에서 stochastic iterative refinement가 사용되는데, 이는 처음 distribution에 아주 작은 noise를 순차적으로 줘서 원하는 distribution으로 보내는 것이다. 즉, pure noise $\\mathbf{y}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$에서 시작해서, 학습된 conditional transition distribution $p _\\theta(\\mathbf{y} _{t-1}|\\mathbf{y} _{t}, \\mathbf{x})$를 통해 $(\\mathbf{y} _{T-1}, \\mathbf{y} _{T-2}, …, \\mathbf{y} _{0})$ 의 과정을 거쳐 $\\mathbf{y} _{0} \\sim p(\\mathbf{y}|\\mathbf{x})$를 얻는 것을 목표로 한다. 학습 과정에서 intermediate image들의 distribution($\\mathbf{y} _t$)는 주어진 target image $\\mathbf{y} _0$에서 시작해 작은 Gaussian noise들인 forward process $q(\\mathbf{y} _t|\\mathbf{y} _{t-1})$를 순차적으로 가해줘서 얻을 수 있다.Conditional diffusion model의 구조. Forward process $q$를 이용해 여러 intermediate image들을 만들어내고, 이들을 이용해 backward process $p$를 학습한다.DDPM과 동일하게 forward와 backward process가 진행되기 때문에, 같은 수학적 공식을 따른다. 다만 conditional한 diffusion model이기 때문에, source image $\\mathbf{x}$도 포함해서 network가 만들어진다.SR3의 전체 algorithm. DDPM과 동일하지만, network에 $\\mathbf{x}$가 포함되어 있다.SR3 model architectureScoreSDE의 변화를 가져와서 사용했는데, Residual block을 BigGAN의 것으로 교체하고, skip connection을 $1/\\sqrt{2}$로 rescale하고, 더 많은 residual block을 사용했다. Low-resolution input $\\mathbf{x}$는 bicubic interpolation을 이용해서 upsample 된 후에, $\\mathbf{y} _t$와 concatenate되어 U-Net으로 전달된다.SR3 U-Net의 구조. Input 단계에서 $\\mathbf{x}$ $\\mathbf{y} _t$가 concatenate된다.Training detailsFlickr-Faces-HQ(FFHQ), CelebA-HQ, ImageNet 1K dataset을 이용해서 학습되었다. Evaluation metric에는 PSNR과 SSIM이 사용되었고, 2-alternative forced-choice(2AFC)를 기반으로 측정된 Fool rate도 사용했다. 더 자세한 내용은 SR3 paper를 참조하길 바란다.Results이전의 연구들보다 훨씬 더 향상된 성능을 보였다. 추가적으로, 여러 SR3 model을 이어서 몇 번의 upsampling을 통한 cascaded high-resolution image synthesis를 선보였는데, 한번에 high-resolution image를 만들어내는 것보다 더 적은 parameter로 좋은 성능을 낼 수 있다고 한다. 더 자세한 내용은 SR3 paper를 참조하길 바란다.Super-resolution task에 대한 결과.여러 SR3 model을 이어 cascaded high-resolution image synthesis를 한 결과.ConclusionBias is an important problem in all generative models.SR3 is no different, and suffers from bias issues. While intheory, our log-likelihood based objective is mode covering(e.g., unlike some GAN-based objectives), we believeit is likely our diffusion-based models drop modes. We observedsome evidence of mode dropping, the model consistentlygenerates nearly the same image output during sampling(when conditioned on the same input). We also observedthe model to generate very continuous skin texture inface super-resolution, dropping moles, pimples and piercingsfound in the reference. SR3 should not be used forany real world super-resolution tasks, until these biases arethoroughly understood and mitigated.In conclusion, SR3 is an approach to image super-resolutionvia iterative refinement. SR3 can be used in a cascadedfashion to generate high resolution super-resolution images, as well as unconditional samples when cascadedwith a unconditional model. We demonstrate SR3 onface and natural image super-resolution at high resolutionand high magnification ratios. SR3 achieves a human fool rateclose to 50%, suggesting photo-realistic outputs." }, { "title": "Generative Modeling by Estimating Gradients of the Data Distribution", "url": "/posts/SMLD/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-10 00:00:00 +0900", "snippet": "AbstractWe introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding togradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.IntroductionGenerative model은 machine learning의 다양한 분야에서 사용되는데, 대표적으로 log-likelihood를 이용해서 학습을 하는 likelihood-based models와, adversarial training을 이용하는 GAN이 있다. 이 두 model은 뛰어난 성능을 보이지만 한계점도 존재하는데, likelihood based model의 경우에는 normalized probability model을 만들기 위해 특정한 architecture(autoregressive model, flow model)이 강제되고, surrogate loss(VAE의 evidence lower bound)를 사용해야 한다는 한계점이 있다. GAN은 likelihood based model의 문제점들을 어느정도 완화했지만, adversarial training이 불안정하다는 단점이 있다.본 논문(SMLD)에서는 log-probability의 gradient인 ‘score’ 라는 개념을 이용한 새로운 generative model을 제안하는데, score matching을 통해 neural network를 학습시킨다.Score based generative modelingProbability density $p(\\mathbf{x})$에 대해, score은 $\\nabla _\\mathbf{x} \\log p(\\mathbf{x})$로 정의할 수 있다. Score network $\\mathbf{s} _\\mathbf{\\theta}$는 $p _\\text{data} (\\mathbf{x})$의 score를 예측하는 것을 목표로 한다. 여기에는 score matching과 Langevin dynamics의 2가지 개념이 사용된다.Score matching for score estimationScore estimation의 기본적인 objective은 $\\frac{1}{2} \\mathbb{E} _{p _\\text{data}}[ || \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x}) - \\nabla _\\mathbf{x} \\log p _\\text{data} (\\mathbf{x})) || ^2 _2 ]$를 minimize 하는 것이다. 이는 Song et al. 2019에 의해 다음 식으로 다시 쓸 수 있다(\\ref{eq1}).\\[\\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\left[ \\text{tr}(\\nabla_\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})) + \\frac{1}{2} ||\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})||^2_2 \\right] \\label{eq1} \\tag{1}\\]이때 $\\nabla _\\mathbf{x} \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x})$는 $\\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x})$의 Jacobian을 의미한다. 하지만 image같은 high-dimensional data를 사용하는 경우에 $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x}))$의 계산이 너무 복잡하기 때문에, 이를 그대로 사용하는 것에는 무리가 있다. 이를 해결하기 위해 다음과 같은 방법들이 사용될 수 있다.Denoising score matchingDenoising score matching은 $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x}))$항의 계산을 통째로 없애버리는 방법이다. Data point $\\mathbf{x}$를 pre-defined noise distribution $q _\\sigma (\\tilde{\\mathbf{x}}|\\mathbf{x})$를 이용해 $q _\\sigma (\\tilde{\\mathbf{x}}) = \\int q _\\sigma (\\tilde{\\mathbf{x}}|\\mathbf{x})p _\\text{data}(\\mathbf{x})\\text{d}\\mathbf{x}$으로 perturb 시키고, 이 perturb된 data distribution의 score를 estimate 한다. 그러면 objective은 다음과 같이 쓸 수 있다(\\ref{eq2}).\\[\\frac{1}{2}\\mathbb{E}_{q _\\sigma (\\tilde{\\mathbf{x}}|\\mathbf{x})p_\\text{data}(\\mathbf{x})} [ || \\mathbf{s}_\\mathbf{\\theta}(\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q _\\sigma (\\tilde{\\mathbf{x}}|\\mathbf{x}) ||^2_2 ] \\label{eq2} \\tag{2}\\]이때 $q _\\sigma (\\tilde{\\mathbf{x}}|\\mathbf{x})$는 알고 있는 값이기 때문에, $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x}))$를 계산하지 않고 equation \\ref{eq2}를 바로 풀 수 있다. 이렇게 구해진 optimal한 $\\mathbf{s} _\\mathbf{\\theta}(\\tilde{\\mathbf{x}})$는 $\\nabla _\\mathbf{x} \\log q _\\sigma (\\mathbf{x})$와 같아지게 되고, $q _\\sigma(\\mathbf{x}) \\approx p _\\text{data} (\\mathbf{x})$를 만족할정도로 작은 noise를 사용하면 원래 objective 역시 풀 수 있다.Sliced score matchingSliced score matching은 $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x}))$를 쉽게 계산하기 위해 random projection을 사용하는 방법이다. Song et al. 2019에 따르면 trace를 $\\mathbf{v}^\\intercal \\nabla _\\mathbf{x}\\mathbf{s} _\\mathbf{\\theta}(\\mathbf{x})\\mathbf{v}$로 근사할 수 있다고 한다. 이때 $\\mathbf{v}$는 그냥 random한 vector이다. 이 경우에 objective은 다음과 같다(\\ref{eq3}).\\[\\mathbb{E}_{p_\\mathbf{v}}\\mathbb{E}_{p_\\text{data}}\\left[ \\mathbf{v}^\\intercal \\nabla_\\mathbf{x}\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})\\mathbf{v} + \\frac{1}{2}|| \\mathbf{x}_\\mathbf{\\theta}(\\mathbf{x}) ||^2_2 \\right] \\label{eq3} \\tag{3}\\]이 방법은 perturbed data를 사용하는 denoising score matching과는 다르게 unperturbed data를 사용한다는 이점이 있지만, 그만큼 더 많은 계산량을 필요로 한다.Sampling with Langevin dynamicsLangevin dynamics를 사용하면 score function $\\nabla _\\mathbf{x} \\log p(\\mathbf{x})$만을 사용해서 $p(\\mathbf{x})$를 따르는 sample을 얻을 수 있다(\\ref{eq4}).\\[\\tilde{\\mathbf{x}}_t = \\tilde{\\mathbf{x}}_{t-1} + \\frac{\\epsilon}{2} \\nabla_\\mathbf{x} \\log p(\\tilde{\\mathbf{x}}_{t-1}) + \\sqrt{\\epsilon} \\mathbf{z}_t \\label{eq4} \\tag{4}\\]이때 $\\mathbf{z} _t \\sim \\mathcal{N}(0, I)$이고, $\\epsilon$은 fixed step size이다. $\\epsilon \\rightarrow 0$, $T \\rightarrow 0$으로 갈 때 $\\tilde{\\mathbf{x}} _T$의 distribution은 $p(\\mathbf{x})$와 같아지게 된다.Challenges of score-based generative modelingNoise conditional score networks: learning and inferencePalette는 기본적으로 DDPM과 동일한 구조를 사용하는데, 달라진 점이라면 input image를 prior로 사용한다는 점이다. Optimization에 사용되는 loss를 수식으로 나타내면 다음과 같다(\\ref{eq1}).Colorization 결과Inpainting 결과Uncropping 결과JPEG restoration 결과Multi-task learning 결과. 맨 오른쪽은 inpainting에 대해서만 학습된 model이다.ConclusionWe present Palette, a simple, general framework for image-to-image translation. Palette achieves strong results on four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG restoration), outperforming strong GAN and regression baselines. Unlike many GAN models, Palette produces diverse and high fidelity outputs. This is accomplished without task-specific customization nor optimization instability. We also present a multi-task Palette model, that performs just as well or better over their task-specific counterparts. Further exploration and investigation of multi-task diffusion models is an exciting avenue for future work. This paper shows some of the potential of image-to-image diffusion models, but we look forward to seeing new applications." }, { "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS", "url": "/posts/ScoreSDE/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-08 00:00:00 +0900", "snippet": "AbstractCreating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024 \\times 1024$ images for the first time from a score-based generative model.IntroductionProbabilistic generative model로, data에 작은 noise를 계속 줘서 corrupt시킨 다음, 이 corruption의 reverse process를 학습해 clean image를 얻는 것을 목표로 한다. Score matching with Langevin dynamics(SMLD) 은 각 noise scale에서의 score를 estimate하고, Langevin dynamics를 이용해서 decreasing noise scale의 sequence에서 sampling을 해서 clean image를 얻는다. Denoising diffusion probabilistic model(DDPM)은 noise corruption의 backward process를 적용하기 위한 probabilistic model을 학습시킨다. DDPM도 SMLD와 마찬가지로 특정 noise scale에서 일종의 score를 계산하기 때문에, 이 두 model은 모두 score-based generative model의 범주에 들어간다. 본 논문(ScoreSDE)에서는 score-based generative model을 일반화하기 위해 stochastic differential equations(SDEs)를 사용한다.기존의 방법들처럼 data를 유한한 noise distribution들을 이용해 perturb시키는 것이 아니라, ScoreSDE에서는 diffusion process에 알맞은 연속적인 SDE를 사용한다. 이는 기존의 방법들처럼 data와 상관 없이 시간이 지나면 random noise로 보내고, reverse SDE도 구할 수 있다.ScoreSDE의 key idea.추가적으로, 적절한 SDE를 선택하면 SMLD와 DDPM도 ScoreSDE에 포함될 수 있고, 더 좋은 architecture과 sampling algorithm을 이용해 기존의 연구들보다 더 좋은 결과를 얻을 수 있었다고 한다.BackgroundDenoising score matching with Langevin Dynamics(SMLD)Data distribution $p_{data}$에 대해, perturbation kernel $p_\\sigma(\\tilde{\\mathbf{x}}|\\mathbf{x}) := \\mathcal{N}(\\tilde{\\mathbf{x}};\\mathbf{x},\\sigma^2 \\mathbf{I})$, noise scale distribution $p_\\sigma(\\tilde{\\mathbf{x}}) := \\int p_{data}(\\mathbf{x})p_\\sigma(\\tilde{\\mathbf{x}}|\\mathbf{x})d\\mathbf{x}$를 정의할 수 있다. 이는 noise scale의 sequence $\\sigma_{min} = \\sigma_1 &lt; \\sigma_2 &lt; … &lt; \\sigma_N = \\sigma_{max}$에 따라 결정되는데, $\\sigma_{min}$은 $p_{\\sigma_{min}}(\\mathbf{x}) \\approx p_{data}(\\mathbf{x})$ 을 만족할 정도로 작은 값이고, $\\sigma_{max}$는 $p_{\\sigma_{max}}(\\mathbf{x}) \\approx \\mathcal{N}(\\mathbf{x};\\mathbf{0},\\sigma_{max}^2 \\mathbf{I})$를 만족할 정도로 크다. 학습은 Noise Conditional Score Network(NCSN) $\\mathbf{s}_\\theta(\\mathbf{x}, \\sigma)$를 다음과 같이 weighted sum of denoising score matching을 이용해 학습시켰다(\\ref{eq1}).\\[\\mathbf{\\theta}^\\star = \\underset{\\mathbf{\\theta}}{\\text{argmin}} \\sum_{i=1}^N \\sigma_i^2 \\mathbb{E}_{p_{data}(\\mathbf{x})} \\mathbb{E}_{p_{\\sigma_i}(\\tilde{\\mathbf{x}}|\\mathbf{x})} || \\mathbf{s}_{\\mathbf{\\theta}}(\\tilde{\\mathbf{x}}, \\sigma_i) - \\nabla_{\\tilde{\\mathbf{x}}} \\log p_{\\sigma_i}(\\tilde{\\mathbf{x}} | \\mathbf{x}) ||^2_2 \\label{eq1} \\tag{1}\\]충분한 양의 data와 model capacity가 주어진다면, optimal score-based model $\\mathbf{s}_{\\mathbf{\\theta}^\\star}(\\mathbf{x}, \\sigma)$는 $\\nabla _{\\mathbf{x}} \\log p _{\\sigma}(\\mathbf{x})$ 와 거의 모든 부분에서 일치하게 될 것이다. Sampling 단계에서는 Langevin MCMC를 $M$번 시행에서 $p _{\\sigma_i}(\\mathbf{x})$를 sequential하게 얻었다(\\ref{eq2}).\\[\\mathbf{x}_i^m = \\mathbf{x}_i^{m-1} + \\epsilon_i \\mathbf{s}_{\\mathbf{\\theta}^\\star}(\\mathbf{x}_i^{m-1}, \\sigma_i) + \\sqrt{2\\epsilon_i}\\mathbf{z}_i^m, \\quad m = 1,2,..., M \\label{eq2} \\tag{2}\\]이때 $\\epsilon_i&gt;0$는 step size이고, $\\mathbf{z}_i^m$는 standard normal이다.Denoising Diffusion Probabilistic Models(DDPM)Data distribution $p_{data}$에 대해, discrete Markov chain $(\\mathbf{x}_0, \\mathbf{x}_1, …, \\mathbf{x}_N)$을 $p(\\mathbf{x} _i | \\mathbf{x} _{i-1}) = \\mathcal{N}(\\mathbf{x} _i;\\sqrt{1-\\beta _i}\\mathbf{x} _{i-1}, \\beta _i\\mathbf{I})$ 의 형태로 정의하고, 결과적으로 $p _{\\alpha _i}(\\mathbf{x}_i | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x} _i;\\sqrt{\\alpha _i}\\mathbf{x} _{i-1}, (1 - \\alpha _i)\\mathbf{I})$ 가 된다. 이때 $\\beta _i$는 corruption을 위한 0과 1 사이의 schedule이고, $\\alpha _i = \\prod _{j=1} ^i (1 - \\beta _j)$ 이다. SMLD와 비슷하게 $p _{\\alpha _i}(\\tilde{\\mathbf{x}}) := \\int p _{data}(\\mathbf{x})p _{\\alpha _i}(\\tilde{\\mathbf{x}}|\\mathbf{x})d\\mathbf{x}$로 쓸 수 있고, Markov chain 의 reverse direction을 $p _\\mathbf{\\theta}(\\mathbf{x} _{i-1} | \\mathbf{x} _{i}) = \\mathcal{N} (\\mathbf{x} _{i-1} ; \\frac{1}{\\sqrt{1-\\beta _i}}(\\mathbf{x} _{i} + \\beta _i \\mathbf{s} _\\mathbf{\\theta} (\\mathbf{x} _{i}, i)), \\beta _i \\mathbf{I})$ 로 reparameterize 할 수 있다. 학습은 evidence lower bound(ELBO) loss를 이용해 진행된다(\\ref{eq3}).\\[\\mathbf{\\theta}^\\star = \\underset{\\mathbf{\\theta}}{\\text{argmin}} \\sum_{i=1}^N (1 - \\alpha_i) \\mathbb{E}_{p_{data}(\\mathbf{x})} \\mathbb{E}_{p_{\\alpha_i}(\\tilde{\\mathbf{x}}|\\mathbf{x})} || \\mathbf{s}_{\\mathbf{\\theta}}(\\tilde{\\mathbf{x}}, i) - \\nabla_{\\tilde{\\mathbf{x}}} \\log p_{\\alpha_i}(\\tilde{\\mathbf{x}} | \\mathbf{x}) ||^2_2 \\label{eq3} \\tag{3}\\]Sampling은 $\\mathbf{x}_N \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$에서 부터 reverse Markov chain을 이용해 진행된다(\\ref{eq4}).\\[\\mathbf{x}_{i-1} = \\frac{1}{\\sqrt{1-\\beta_i}}(\\mathbf{x}_{i} + \\beta_i \\mathbf{s}_{\\mathbf{\\theta}^\\star}(\\mathbf{x}_i,i)) + \\sqrt{\\beta_i} \\mathbf{z}_i, \\quad i = N, N-1, ..., 1 \\label{eq4} \\tag{4}\\]전체적으로 SMLD와 비슷한 느낌으로 진행되는 것을 알 수 있다.Score-Based Generative Modeling with SDEsPerturbing Data with SDEsScoreSDE의 목적은 data distribution $p_0$와 prior distribution $p_T$에 대해, continuous time variable $t \\in [0,T]$로 indexing이 가능하고 $\\mathbf{x}(0) \\sim p_0$, $\\mathbf{x}(T) \\sim p_T$를 만족하는 diffusion process $[\\mathbf{x}(t)]_ {t=0}^ T$를 construct 하는 것이다. 이는 일반인 SDE의 solution을 이용해서 modeling 할 수 있다(\\ref{eq5}).\\[\\text{d}\\mathbf{x} = \\mathbf{f}(\\mathbf{x},t) \\text{d}t + g(t)\\text{d}\\mathbf{w} \\label{eq5} \\tag{5}\\]이때 $\\mathbf{w}$는 standard Wiener process(Brownian motion), $\\mathbf{f} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$는 $\\mathbf{x}(t)$의 drift coefficient, $\\mathbf{g} : \\mathbb{R} \\rightarrow \\mathbb{R}$은 $\\mathbf{x}(t)$의 diffusion coefficient이다. 추가적으로, $\\mathbf{x}(t)$의 probability density를 $p _t(\\mathbf{x})$로, $\\mathbf{x}(s)$에서 $\\mathbf{x}(t)$로 가는 transition kernel을 $p _{st}(\\mathbf{x}(t) | \\mathbf{x}(s))$로 정의한다. 이때 $p _T$는 $p _0$에 대한 아무런 정보도 담고 있지 않은 distribution(예를 들면 Gaussian)으로 설정된다.Generating Samples by Reversing the SDESampling은 Reverse-time diffusion equation models에 의거한 reverse-time SDE를 이용해 실행할 수 있다(\\ref{eq6}).\\[\\text{d}\\mathbf{x} = [\\mathbf{f}(\\mathbf{x},t) - g(t)^2 \\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x})]\\text{d}t + g(t)\\text{d}\\bar{\\mathbf{w}} \\label{eq6} \\tag{6}\\]이때 $\\bar{\\mathbf{w}}$는 $\\mathbf{w}$과 마찬가지로 standard Wiener process이다.Estimating Scores for the SDE$\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x})$ 를 estimate하기 위해, time-dependent score-based model $\\mathbf{s} _\\mathbf{\\theta} (\\mathbf{x}, t)$를 학습시킨다. 이는 equation\\ref{eq1} \\ref{eq3}의 일반화된 형태이다(\\ref{eq7}).\\[\\mathbf{\\theta}^\\star = \\underset{\\mathbf{\\theta}}{\\text{argmin}} \\mathbb{E}_t \\left( \\lambda_t \\mathbb{E}_{\\mathbf{x}(0)} \\mathbb{E}_{\\mathbf{x}(t)|\\mathbf{x}(0)} || \\mathbf{s}_{\\mathbf{\\theta}}(\\mathbf{x}(t), t) - \\nabla_{\\mathbf{x}(t)} \\log p_{0t}(\\mathbf{x}(t) | \\mathbf{x}(0)) ||^2_2 \\right) \\label{eq7} \\tag{7}\\]이때 $\\lambda : [0,T] \\rightarrow \\mathbb{R} _{&gt;0}$는 positive weighting function이고, $\\mathbf{x}(0) \\sim p _0(\\mathbf{x})$, $\\mathbf{x}(t) \\sim p _{0t}(\\mathbf{x}(t) | \\mathbf{x}(0))$이다. SMLD와 DDPM의 경우에는, $\\lambda \\propto 1/\\mathbb{E}[|| \\nabla _{\\mathbf{x}(t)} \\log p _{0t} (\\mathbf{x}(t) | \\mathbf{x}(0)) || ^2 _2]$ 으로 설정되었다.Equation \\ref{eq7}를 풀려면 transition kernel $p _{0t}(\\mathbf{x}(t) | \\mathbf{x}(0))$에 대한 정보가 있어야 한다. $\\mathbf{f}$가 affine한 경우에 transition kernel은 항상 Gaussian이 되며, mean과 variance는 존재하는 여러 방법들을 이용해 구할 수 있다고 한다. 더 일반적인 경우는 Kolmogorov’s forward equation를 풀면 된다고 한다.Modeling SMLD, DDPM using ScoreSDE$N$개의 noise scale을 사용할 때, SMLD의 각 perturbation kernel $p _{\\sigma _i}(\\mathbf{x} | \\mathbf{x} _0)$는 다음과 같이 쓸 수 있다(\\ref{eq8}).\\[\\mathbf{x}_i = \\mathbf{x}_{i-1} + \\sqrt{\\sigma_i^2 - \\sigma_{i-1}^2} \\mathbf{z}_{i-1}, \\quad i = 1,2,..., N \\label{eq8} \\tag{8}\\]이때 $N \\rightarrow \\infty$가 되면, $\\sigma _i$는 함수 $\\sigma(t)$가 되고, $\\mathbf{z} _i$는 $\\mathbf{z}(t)$, Markov chain $\\mathbf{x} _i$는 continuous stochastic process $\\mathbf{x}(t)$가 된다. $\\mathbf{x}(t)$는 다음 SDE를 이용해서 쓸 수 있다(\\ref{eq9}).\\[\\text{d}\\mathbf{x} = \\sqrt{\\frac{\\text{d}[\\sigma^2(t)]}{\\text{d}t}} \\text{d} \\mathbf{w} \\label{eq9} \\tag{9}\\]이와 마찬가지로 DDPM도 각 perturbation kernel 을 다음과 같이 쓸 수 있다(\\ref{eq10}).\\[\\mathbf{x}_i = \\sqrt{1-\\beta_i} \\mathbf{x}_{i-1} + \\sqrt{\\beta_i} \\mathbf{z}_{i-1}, \\quad i = 1,2,..., N \\label{eq10} \\tag{10}\\]$N \\rightarrow \\infty$가 되면, 이 역시 다음 SDE를 이용해서 쓸 수 있다(\\ref{eq11}).\\[\\text{d}\\mathbf{x} = -\\frac{1}{2} \\beta(t)\\mathbf{x} \\text{d}t + \\sqrt{\\beta(t)} \\text{d}\\mathbf{w} \\label{eq11} \\tag{11}\\]더 자세한 사항은 ScoreSDE 논문의 Appendix B를 참고하길 바란다.Solving the Reverse SDE내용이 정말 방대하다. ScoreSDE 논문을 참고하는 것이 좋을 것 같다.Architecture improvementsScore SDE 역시 DDPM과 비슷한 U-Net 구조를 사용한다. 여기에 추가적으로 아래 변경점들이 존재한다. StyleGAN-2에서 사용한 image upsampling, downsampling을 사용했다. Skip connection들을 $1/\\sqrt{2}$로 rescale 했다. 이는 Progressive GAN, StyleGAN, StyleGAN-2등 여러 GAN model에서 효과적이었다고 한다. Residual block을 BigGAN의 것으로 교체했다. 한 resolution 당 block의 개수를 2개에서 4개로 증가시켰다.Controllable GenerationScoreSDE의 framework은 $p _0$에서 data sample을 만들어내는 것 뿐만 아니라, $p _t(\\mathbf{y} | \\mathbf{x}(t))$를 알고있다면 $p _0(\\mathbf{x}(0) | \\mathbf{y})$에서도 data sample을 만들어낼 수 있다. 이는 다음 reverse-time SDE를 풀면 얻을 수 있다(\\ref{eq12}).\\[\\text{d}\\mathbf{x} = \\left\\{ \\mathbf{f}(\\mathbf{x},t) - g(t)^2[\\nabla _\\mathbf{x} \\log p_t(\\mathbf{x}) + \\nabla_{\\mathbf{x}} \\log p_t (\\mathbf{y} | \\mathbf{x})] \\right\\} \\text{d}t + g(t)\\text{d}\\bar{\\mathbf{w}} \\label{eq12} \\tag{12}\\]Result자세한 사항은 ScoreSDE 논문 를 참고하길 바란다.Class-conditional samples(왼쪽)와 inpainting, colorization같은 image-to-image task(오른쪽)에 모두 좋은 성능을 보인다.ConclusionWe presented a framework for score-based generative modeling based on SDEs. Our work enables abetter understanding of existing approaches, new sampling algorithms, exact likelihood computation,uniquely identifiable encoding, latent code manipulation, and brings new conditional generationabilities to the family of score-based generative models.While our proposed sampling approaches improve results and enable more efficient sampling, theyremain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifyingways of combining the stable learning of score-based generative models with the fast sampling ofimplicit models like GANs remains an important research direction. Additionally, the breadth ofsamplers one can use when given access to score functions introduces a number of hyper-parameters.Future work would benefit from improved methods to automatically select and tune these hyperparameters,as well as more extensive investigation on the merits and limitations of various samplers." }, { "title": "Denoising Diffusion Probabilistic Models", "url": "/posts/DDPM/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-08-02 00:00:00 +0900", "snippet": "AbstractWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat- urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.IntroductionDiffusion probabilistic model(diffusion model)은 Markov chain인 forward process가 주어졌을 때, 그 backward process를 예측하고자 하는 model이다. Forward process가 소량의 Gaussian noise인 경우, backward process역시 Gaussian으로 근사할 수 있기 때문에 비교적 간단하게 model을 학습시킬 수 있다.본 논문(DDPM)은 diffusion model을 이용해 high-quality sample을 만드는 것이 가능하고, 다른 generative model보다 좋은 성능을 갖는 경우도 있다고 말한다.DDPM의 대략적인 구조. q가 forward process, p가 backward process 이다.Diffusion modelForward processDiffusion model은 latent $\\mathbf{x}_1, …, \\mathbf{x}_T$와 forward process $q$로 이루어져 있다. 이때 $q$는 아주 작은 Gaussian noise로, mean과 variance는 특정한 schedule $\\beta_1, …, \\beta_T$에 의해 결정된다(\\ref{eq1}).\\[q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t;\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\label{eq1} \\tag{1}\\]Forward process는 어떠한 image $\\mathbf{x}_0$에 Gaussian noise $q$를 $T$번 apply하는 것으로 생각할 수 있으며, 최종적인 latent $\\mathbf{x}_T$ 역시 Gaussian noise가 된다. 추가적으로 $q$는 markov chain이기 때문에, 아래 식이 성립한다(\\ref{eq2}).\\[q(\\mathbf{x}_t|\\mathbf{x}_0) = \\prod_{i=1}^t q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) \\label{eq2} \\tag{2}\\]이를 이용하면 $\\mathbf{x}_t$를 $\\mathbf{x}_0$를 이용해서 나타낼 수 있다(\\ref{eq3}).\\[\\begin{align} \\mathbf{x}_t &amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon \\\\ &amp;= \\sqrt{\\alpha_t \\alpha_{t-1}}\\mathbf{x}_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon \\\\ &amp;= ...\\\\ &amp;= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon \\label{eq3} \\tag{3}\\end{align}\\]이때 $\\alpha_t = 1 - \\beta_t$, $\\bar{\\alpha}_t = \\prod _{i=1}^{t}$, $\\epsilon = \\mathcal{N}(0, \\mathbf{I})$ 이다.Backward processForward process $q$를 이용해서 backward process $p$를 예측하는것이 DDPM의 목표이다. 이때 $q$와 마찬가지로 $p$도 Markov chain이다(\\ref{eq4}).\\[p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}) := \\mathcal{N}(\\mathbf{x}_{t}; \\mathbf{\\mu}_{\\theta}(\\mathbf{x}_{t},t), \\mathbf{\\Sigma}_{\\theta}(\\mathbf{x}_{t}, t)), \\quad p_\\theta(\\mathbf{x}_{0}|\\mathbf{x}_{T}) = \\prod_{t=1}^Tp_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}) \\label{eq4} \\tag{4}\\]Learning backward process$p$는 Negative log likelihood의 upper bound를 이용해 학습시킬 수 있다(\\ref{eq5}).\\[\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_{0})] \\le \\mathbb{E}_q\\left[-\\log p_\\theta(\\mathbf{x}_{T}) - \\sum_{t \\ge 1}\\log\\frac{p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right] := L \\label{eq5} \\tag{5}\\]위 식을 간단하게 하면, 다음과 같은 식을 얻을 수 있다(\\ref{eq6}).\\[L = \\mathbb{E}_q\\left[ \\underset{L_T}{\\underbrace{D_{KL}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p(\\mathbf{x}_T))}} + \\sum_{t&gt;1} \\underset{L_{t-1}}{\\underbrace{D_{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)||p(\\mathbf{x}_{t-1}|\\mathbf{x}_t))}} \\underset{L_0}{\\underbrace{-\\log p_\\theta(\\mathbf{x}_0|\\mathbf{x}_1)}}\\right] \\label{eq6} \\tag{6}\\]위 식들의 자세한 유도과정은 DDPM 논문의 appendix를 참고하길 바란다.$L_T$DDPM에서는 $\\beta_t$를 학습시키지 않고 고정된 값을 사용한다. 그렇기 때문에 forward process $q$에는 learnable parameter가 존재하지 않고, $p(\\mathbf{x}_T)$도 Gaussian noise이기 때문에 $L_T$는 상수가 된다.$L_{T-1}, …, L_1$수식 \\ref{eq4}를 보면, backward process 의 mean과 variance는 모두 learnable 하다. 하지만 DDPM에서는 $p$의 variance $\\sigma^2$를 $q$와 동일하다고 놓고 mean만 학습을 시켰다. 이러한 경우에 Loss term $L_{t-1}$은 다음과 같이 정리할 수 있다(\\ref{eq7}).\\[L_{t-1} = \\mathbb{E}_q \\left[ \\frac{1}{2\\sigma_t^2} || \\tilde{\\mathbf{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t)||^2 \\right] + C \\label{eq7} \\tag{7}\\]이때 $C$는 상수부분으로, 학습할 때 무시할 수 있다. Forward process($q$)에 대한 정보를 알고 있기 때문에, $\\tilde{\\mu}_t$ 는 다음과 같이 구할 수 있다.\\[\\begin{align} q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) &amp;= q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1},\\mathbf{x}_{0})\\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{0})} \\\\ &amp;= \\text{exp}\\left( -\\frac{1}{2}\\left(\\frac{(\\mathbf{x}_{t} - \\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ &amp;= \\text{exp}\\left( -\\frac{1}{2}\\left(\\frac{\\mathbf{x}_{t}^2 - 2\\sqrt{\\alpha_t}\\mathbf{x}_{t}\\mathbf{x}_{t-1}+ \\alpha_t\\mathbf{x}_{t-1}^2}{\\beta_t} + \\frac{\\mathbf{x}_{t-1}^2 -2\\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{t-1}\\mathbf{x}_{0} + \\bar{\\alpha}_{t-1}\\mathbf{x}_{0}^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{\\mathbf{x}_{t}^2 - 2\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_{t}\\mathbf{x}_{0} + \\bar{\\alpha}_t\\mathbf{x}_{0}^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ &amp;= \\text{exp}\\left( -\\frac{1}{2} \\left( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) \\mathbf{x}^2_{t-1} -(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)\\mathbf{x}_{t-1} + C(\\mathbf{x}_t, \\mathbf{x}_0) \\right) \\right)\\end{align}\\]$C(\\mathbf{x} _t, \\mathbf{x} _0)$는 $\\mathbf{x} _{t-1}$을 포함하지 않는 부분으로, 무시해도 지장이 없다. 위 식은 Gaussian distribution의 형태이기 때문에, $\\tilde{\\mu}_t$는 다음과 같이 표현된다(\\ref{eq8}).\\[\\tilde{\\mu}_t = \\frac{\\left( \\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 \\right)}{\\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}} \\right)} = \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}\\mathbf{x}_0 \\label{eq8}\\tag{8}\\]Equation \\ref{eq3}을 이용해 위 식을 간단하게 하면, 다음과 같은 식을 얻을 수 있다(\\ref{eq9}).\\[\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha}_t}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) \\label{eq9}\\tag{9}\\]결과적으로 equation \\ref{eq7} 을 다시 쓰면 아래와 같이 쓸 수 있다(\\ref{eq10}).\\[L_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\sigma_t^2}||\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) - \\mathbf{\\mu}_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t)||^2 \\right] \\label{eq10} \\tag{10}\\]$\\epsilon$은 Gaussian noise 이다. 여기서 DDPM의 저자들은 $\\mathbf{\\mu}_\\theta$ 를 $\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon _\\theta(\\mathbf{x}_t, t) \\right)$ 로 parameterize 했는데, 이는 $p$와 $q$의 mean term을 서로 비슷하게 만들어주기 위함이다. 그러면 결과적으로 아래와 같은 최종 loss function을 얻을 수 있다(\\ref{eq11}).\\[L_{t-1} = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{\\beta^2_t}{2\\sigma^2_t\\alpha_t(1-\\bar{\\alpha}_t)} ||\\epsilon - \\epsilon _\\theta(\\sqrt{\\bar{\\alpha}\\mathbf{x}_0} + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon, t)||^2 \\right] \\label{eq11} \\tag{11}\\]실제로 학습을 할 때는, 앞의 상수부분 $\\frac{\\beta^2_t}{2\\sigma^2_t\\alpha_t(1-\\bar{\\alpha}_t)}$ 를 지우고 하는 것이 실험적으로 더 좋은 결과를 보여줬다고 한다.Reconstructing $\\mathbf{x}_0$Reconstruction은 $\\mathbf{x}_T$에서 시작해서 순차적으로 $\\mathbf{x}_0$ 까지 generate 하는 방식으로 진행된다(\\ref{eq12}).\\[\\mathbf{x}_{t-1} = \\mathbf{\\mu}(\\mathbf{x}_t, t) + \\sigma_t\\mathbf{z} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon _\\theta(\\mathbf{x}_t, t) \\right) + \\sigma_t\\mathbf{z} \\label{eq12} \\tag{12}\\]Overall algorithm위에서 설명한 training과 reconstruction의 algorithm은 아래와 같다.Training detailsDatasetCIFAR10, CelebA-HQ, LSUN dataset을 사용했다. 모든 image는 $256\\times256$ 크기로 변환되어서 사용되었다.Training parameters$T = 1000$, $\\beta_1 = 10^{-4}, …, \\beta_T = 0.02$ 의 linear schedule로 실험을 진행했다. Model로는 PixenCNN++와 비슷한 구조의 U-Net을 사용했다.Results이전의 연구들을 완전히 outperform하는 것이지만, 뒤쳐지지 않는 성능을 보여준다.더 자세한 설명은 DDPM 논문을 참고하길 바란다.ConclusionWe have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression.Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in othertypes of generative models and machine learning systems." }, { "title": "PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS", "url": "/posts/PixelCNNpp/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-07-25 00:00:00 +0900", "snippet": "AbstractPixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available here. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.IntroductionPixelRNN 논문에서 소개된 PixelCNN은 image를 위한 tractable generative model이다. 이는 image $\\mathbf{x}$의 모든 sub-pixel $x_i$에 대한 pdf $p(\\mathbf{x}) = \\prod_i p(x_i|x_{&lt;i})$ 로 정의된다. 본 논문(PixelCNN++)에서는 PixelCNN에 몇 가지 modification을 가해서 그 성능을 향상시켰다.Modifications to PixelCNNDiscretized logistic mixture likelihoodPixelCNN은 sub-pixel의 conditional distribution(color channel)을 256-way softmax로 modeling 한다. RGB color의 범위가 0~255이기 때문에, 이 사이의 값으로 discretize 하는 것이다. 이는 model에 flexibility를 제공해주지만, 동시에 memory 측면에서 많은 resource를 사용하게 된다. 그리고 discretize된 value $c$가 실제로 $c-1$과 $c+1$중 어디에 가까운지 알 수 없기 때문에, 이를 추가적으로 학습시켜야 해서 학습 속도가 정말 느리다. 그래서 PixelCNN++에서는 discretized pixel value의 conditional probability를 계산하는 다른 mechanism을 제시했다. Continuous distribution인 latent color intensity $\\nu$가 있다고 가정하고, 이를 가장 가까운 자연수로 round하는 방법을 사용하는데, 이를 logistic distribution의 mixture으로 modeling 한다(\\ref{eq1}).\\[\\begin{align} \\nu &amp;\\sim \\sum_{i=1}^K\\pi_i\\text{logistic}(\\mu_i, s_i) \\\\ P(x|\\pi, \\mu, s) &amp;= \\sum_{i=1}^K \\pi_i [\\sigma((x+0.5-\\mu_i)/s_i) - \\sigma((x-0.5-\\mu_i)/s_i)]\\end{align} \\label{eq1} \\tag{1}\\]이때 $\\sigma()$는 logistic sigmoid function이다. Edge case인 0과 255에서는 각각 $x-0.5$를 $-\\infty$로, $x+0.5$를 $\\infty$로 바꿨다.Conditioning on whole pixelsPixelCNN에서는 RGB 3개의 값을 분리시켜서 generative model을 만든다. 이는 굉장히 general한 structure을 보장해주지만, 동시에 model을 더 복잡하게 만든다. PixelCNN++에서는 color channel간의 dependency는 deep learning을 통해 알아내야 할 정도로 복잡하지 않다고 말하며, 각 channel의 mean이 다른 channel value와 linearly depend한다고 가정해 RGB값을 한번에 modeling 했다.Using downsamplingPixelCNN에서는 작은 receptive field에서 CNN을 사용해준다. 이는 local dependency를 고려하기에는 좋지만, long range structure을 modeling하기에는 적합하지 않다. 그래서 PixelCNN++에서는 input image를 downsampling해서 receptive field를 증가시켜 줬다. 하지만 이는 정보의 손실을 불러일으킬 수 있는데, short-cut connection을 추가해서 이 문제를 해결했다.Network architecture$32 \\times 32$크기의 input에 대해, PixelCNN++는 5개의 ResNet layer을 6 block만큼 사용한다. 1번과 2번, 그리고 2번과 3번 block 사이에는 strided convolution을 통한 subsampling이 이루어지고, 4번과 5번, 그리고 5번과 6번 block 사이에는 transpose된 strided convolution을 통한 upsampling이 이루어진다. 이 과정에서 정보 손실을 막기위해, 1번과 6번, 2번과 5번, 그리고 3번과 4번 block 사이에 short-cut connection을 추가했다. 이는 UNet과 비슷한 구조이다.PixelCNN++의 network 구조.추가적으로 regularization을 위한 dropout layer을 사용했다.Training detailsCIFAR-10 dataset을 사용했으며, PixelCNN과 마찬가지로 NLL loss을 사용했다.ResultsPixelCNN을 비롯한 이전의 연구들보다 더 좋은 성능을 발휘했다. 더 자세한 사항은 PixelCNN++ 논문을 참조하길 바란다.NLL을 bits per sub-pixel로 나타낸 표이다. PixelCNN++가 가장 성능이 좋음을 알 수 있다.왼쪽 : Class conditional generated images, 오른쪽 : Real CIFAR-10 imagesShort-cut connection을 사용하는 것이 성능이 훨씬 좋음을 알 수 있다.ConclusionWe presented PixelCNN++, a modification of PixelCNN using a discretized logistic mixture likelihood on the pixels among other modifications. We demonstrated the usefulness of these modifications with state-of-the-art results on CIFAR-10. Our code is made available here and can easily be adapted for use on other data sets." }, { "title": "Pixel Recurrent Neural Networks", "url": "/posts/PixelRNN/", "categories": "Paper review, Image synthesis", "tags": "", "date": "2022-07-17 00:00:00 +0900", "snippet": "AbstractModeling the distribution of natural images isa landmark problem in unsupervised learning.This task requires an image model that is atonce expressive, tractable and scalable. Wepresent a deep neural network that sequentiallypredicts the pixels in an image along the twospatial dimensions. Our method models the discreteprobability of the raw pixel values and encodesthe complete set of dependencies in theimage. Architectural novelties include fast two dimensionalrecurrent layers and an effective useof residual connections in deep recurrent networks.We achieve log-likelihood scores on naturalimages that are considerably better than theprevious state of the art. Our main results alsoprovide benchmarks on the diverse ImageNetdataset. Samples generated from the model appearcrisp, varied and globally coherent.IntroductionGenerative image modeling은 여러 방면에서 뛰어난 성능을 보이지만, high-dimensional한 image들의 distribution을 알아내는 것은 어려운 일이다. 추가적으로 generative modeling은 tractable하면서 scalable한 model을 만드는 것을 목표로 하는데, 기존의 연구들은 stochastic latent variable을 사용하는 VAE같이 scalability는 잘 챙겼지만 tractability는 잘 챙기지 못하는 모습을 보여준다. 본 논문(PixelRNN)에서는 RNN을 이용해서 image내의 pixel의 관계를 학습해 tractability를 얻고자 한다. 구체적으로, LSTM을 row방향으로 적용하는 ‘Row LSTM’, diagonal 방향으로 적용하는 ‘Diagonal BiLSTM’, 그리고 masked CNN을 사용하는 ‘PixelCNN’을 소개한다.Main ideaPixelRNN에서는 이전 context를 기반으로, 현재 pixel이 될 수 있는 값의 conditional distribution을 구하는 것을 목표로 한다. $n\\times n$크기의 image $\\mathbf{x}$에 대해, probability $p(\\mathbf{x})$는 다음과 같이 정의된다(\\ref{eq1}).\\[p(\\mathbf{x}) = \\prod_{i=1}^{n^2}p(x_i|x_1, ..., x_{i-1}) \\label{eq1} \\tag{1}\\]$p(x_i|x_1, …, x_{i-1})$ 는 pixel $x_1, …, x_{i-1}$의 값이 주어졌을 때, 다음 pixel $x_i$의 probability로 해석할 수 있다.PixelRNN의 main idea. 이전 pixel들의 값이 주어졌을 때, 현재 pixel 값의 probability를 알고자 한다.Pixel Recurrent Neural NetworksPixerRNN에서 사용한 model에는 Row LSTM, Diagonal BiLSTM, PixelCNN의 3가지가 있다.PixelRNN에서 사용한 3가지 model.Row LSTMRow LSTM은 위에서부터 순서대로 row의 feature을 계산하는 unidirectional layer이다. 각 pixel에 대해 pixel 윗부분에 존재하는 삼각형 부분의 context를 계산하게 되는데, LSTM layer의 한 step은 다음과 같이 쓸 수 있다(\\ref{eq2}).\\[\\begin{align} [\\mathbf{o}_i, \\mathbf{f}_i, \\mathbf{i}_i, \\mathbf{g}_i] &amp;= \\sigma(\\mathbf{K}^{ss}\\circledast\\mathbf{h}_{i-1} + \\mathbf{K}^{is}\\circledast \\mathbf{x}_i) \\\\ \\mathbf{c}_i &amp;= \\mathbf{f}_i \\odot \\mathbf{c}_{i-1} + \\mathbf{i}_i \\odot \\mathbf{g}_i \\\\ \\mathbf{h}_i &amp;= \\mathbf{o}_i \\odot \\tanh(\\mathbf{c}_i)\\end{align} \\label{eq2} \\tag{2}\\]Row LSTM은 삼각형 모양의 receptive field를 갖고 있기 때문에, 전체 context를 참고하는 것은 불가능하다.Diagonal BiLSTMDiagonal BiLSTM은 computation 과정에서 이전의 context를 모두 참고하기 위해 고안되었으며, input map을 row마다 한칸씩 차이나게 skew시켜서 LSTM을 진행해 줬다. 그러면 각 column에 있는 pixel들은 그보다 왼쪽 위에 있는 pixel들을 참고하면 이전에 등장한 모든 context를 참고할 수 있게되고, computation을 column 순서대로 하면 되기 때문에 순서가 꼬이는 일도 없다.Diagonal BiLSTM에서는 이 그림처럼 input map을 skew하게 된다. 이전 column에 대한 정보만으로 각 pixel 이전의 모든 context를 참고할 수 있다.PixelCNNLSTM layer은 이론적으로 unbounded dependency range를 갖고 있다. 하지만 이 때문에 각 computational cost도 같이 올라간다. 그렇기 저자들은 receptive field는 넓게 하지만 unbounded하지는 않은 CNN을 사용하는 ‘PixelCNN’을 제시했다. 이 과정에서 CNN에 mask를 씌워 미래 pixel의 값을 참고하지 않도록 했다.Multi-Scale PixelRNNMulti-Scale PixelRNN은 unconditional PixelRNN과 추가적인 conditional PixelRNN으로 이루어져 있다. 먼저 unconditional PixelRNN을 이용해 original image에서 subsample된 $s\\times s$크기의 smaller image를 만들어낸다. 그 다음 이를 additional input으로 받아 conditional PixelRNN을 이용해 원래 크기의 image를 만든다. 이때 처음 생성된 image는 upsample되어 input으로 사용된다.Diffusion probabilistic model(diffusion model)은 Markov chain인 forward process가 주어졌을 때, 그 backward process를 예측하고자 하는 model이다. Forward process가 소량의 Gaussian noise인 경우, backward process역시 Gaussian으로 근사할 수 있기 때문에 비교적 간단하게 model을 학습시킬 수 있다.Multi-scale PixelRNN. 하늘색 pixel이 unconditional PixelRNN으로 만든 small image이다.Training detailsMNIST, CIFAR-10, ImageNet dataset을 이용해서 실험을 진행했다. Loss로는 Generative model에서 많이 사용하는 negative log likelihood(NLL)를 사용했다. 더 자세한 사항은 PixelRNN paper을 참조하길 바란다.Reuslt이전의 연구들보다 좋은 성능을 보였다. Diagonal BiLSTM이 가장 좋은 성능을 보였지만, Row LSTM과 PixelCNN도 좋은 결과를 보였다. 더 자세한 사항은 PixelRNN paper을 참조하길 바란다.ConclusionIn this paper we significantly improve and build upon deep recurrent neural networks as generative models for natural images. We have described novel two-dimensional LSTM layers: the Row LSTM and the Diagonal BiLSTM, that scale more easily to larger datasets. The models were trained to model the raw RGB pixel values. We treated the pixel values as discrete random variables by using a softmax layer in the conditional distributions. We employed masked convolutions to allow PixelRNNs to model full dependencies between the color channels. We proposed and evaluated architectural improvements in these models resulting in PixelRNNs with up to 12 LSTM layers.We have shown that the PixelRNNs significantly improve the state of the art on the MNIST and CIFAR-10 datasets. We also provide new benchmarks for generative image modeling on the ImageNet dataset. Based on the samples and completions drawn from the models we can conclude that the PixelRNNs are able to model both spatially local and long-range correlations and are able to produce images that are sharp and coherent. Given that these models improve as we make them larger and that there is practically unlimited data available to train on, more computation and larger models are likely to further improve the results." }, { "title": "A Machine Learning Approach for Filtering Monte Carlo Noise", "url": "/posts/MLMC/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-05-10 00:00:00 +0900", "snippet": "AbstractThe most successful approaches for filtering Monte Carlo noise use feature-based filters (e.g., cross-bilateral and cross non-local means filters) that exploit additional scene features such as world positions and shading normals. However, their main challenge is finding the optimal weights for each feature in the filter to reduce noise but preserve scene detail. In this paper, we observe there is a complex relationship between the noisy scene data and the ideal filter parameters, and propose to learn this relationship using a nonlinear regression model. To do this, we use a multilayer perceptron neural network and combine it with a matching filter during both training and testing. To use our framework, we first train it in an offline pro- cess on a set of noisy images of scenes with a variety of distributed effects. Then at run-time, the trained network can be used to drive the filter parameters for new scenes to produce filtered images that approximate the ground truth. We demonstrate that our trained net- work can generate filtered images in only a few seconds that are superior to previous approaches on a wide range of distributed effects such as depth of field, motion blur, area lighting, glossy reflections, and global illumination.IntroductionMonte Carlo denoising using filters초창기 Monte Carlo denoising은 대부분 filter을 이용해서 진행되었다. 그중에서 feature-based filter는 다양한 feature 정보를 guidance로 filter을 만드는 방법이고, 이는 뛰어난 성능을 보였다. 하지만 이러한 filter의 weight를 정하는 일은 시간도 오래 걸리고, 여러 assumption을 바탕으로 정해졌기 때문에 성능에도 한계가 있었다. 본 논문에서는 여러 feature과 해당하는 filter weight에는 어떠한 복잡한 관계가 있다고 말하며, 이를 machine learning을 통해 알아내고자 한다.A new learning framework for MC filtering본 논문의 목적은 적은 spp로 rendering된 noisy image가 주어졌을 때, 높은 spp로 rendering된 것과 유사한 noise-free image를 만들어내는 것을 목표로 한다. Filtered image $\\hat{\\mathbf{c}} = \\left( \\hat{c}_r, \\hat{c}_g, \\hat{c}_b \\right)$는 다음과 같이 구할 수 있다(\\ref{eq1}).\\[\\hat{\\mathbf{c}}_i = \\frac{\\sum_{j \\in \\mathcal{N}(i)}d_{i,j}\\bar{\\mathbf{c}}_j}{\\sum_{j \\in \\mathcal{N}(i)}d_{i,j}} \\label{eq1} \\tag{1}\\]이때 $\\mathcal{N}(i)$는 pixel $i$의 neighborhood를 의미하고, $d_{i,j}$는 $i$와 그 neighbor $j$ 사이의 filter weight를 의미한다. 이전의 연구들에서는 denoising 성능을 올리기 위해 cross-bilateral filter같은 특수한 filter을 사용했는데, 이 경우 $d_{i,j}$는 다음과 같이 정의된다(\\ref{eq2}).\\[d_{i,j} = \\text(exp)\\left[ -\\frac{||\\bar{\\mathbf{p}}_i - \\bar{\\mathbf{p}}_j||^2}{2\\alpha_i^2}\\right] \\times \\text{exp}\\left[ -\\frac{D(\\bar{\\mathbf{c}}_i, \\bar{\\mathbf{c}}_j)}{2\\beta_i^2} \\right] \\times \\prod_{k=1}^K \\text{exp}\\left[ -\\frac{D_k(\\bar{\\mathbf{f}}_{i,k}, \\bar{\\mathbf{f}}_{j,k})}{2\\gamma_{k,i}^2} \\right] \\label{eq2} \\tag{2}\\]이때 $\\bar{\\mathbf{p}}_{i}$는 pixel $i$의 screen space position, $\\bar{\\mathbf{f}} _{i,k}$ 는 $k$번째 scene feature, $\\alpha _i^2, \\beta _i^2, \\gamma _{k,i}^2$는 각각 $i$의 spatial, color, $k^\\text{th}$ feature에 대한 variance, $D$, $D_k$는 color와 feature간의 거리를 측정하기 위한 특수한 함수들이다. 결국 $\\alpha _i, \\beta _i, \\gamma _{k,i}$에 따라 $d _{i,j}$가 바뀌기 때문에, 이들의 값을 잘 예측하는 것이 관건이고, 이전의 feature-based filter을 사용하는 연구들은 모두 이 변수들의 값을 예측하는 방식에 차이가 있다.본 논문에서는 다음과 같이 이 filtering process를 더 general하게 접근한다(\\ref{eq3}).\\[\\hat{\\mathbf{c}}_i = h(\\bar{\\mathbf{s}}_{\\mathcal{N}(i)}, \\mathbf{\\theta}_i), \\quad \\text{where} \\quad \\bar{\\mathbf{s}}_{\\mathcal{N}(i)} = \\underset{j \\in \\mathcal{N}(i)}{\\bigcup} \\bar{\\mathbf{s}}_j \\label{eq3} \\tag{3}\\]이때 $\\bar{\\mathbf{s}}_{\\mathcal{N}(i)}$ 는 pixel $i$의 neighborhood에 있는 primary feature들의 모음이다. 예를 들어 앞서 소개한 cross-bilateral filter의 경우에는 $\\mathbf{\\theta}_i$가 $(\\alpha, \\beta, \\gamma_1, …, \\gamma_K)$의 총 $K+2$개의 parameter을 갖고 있다고 생각할 수 있다. 궁극적으로, ground truth와 가장 오차가 적은 result를 만들어내는 optimal한 $\\mathbf{\\theta}$를 목표로 한다(\\ref{eq4}).\\[\\mathbf{\\theta}_i^* = \\underset{\\mathbf{\\theta}_i}{\\text{argmin }}E(h(\\bar{\\mathbf{s}}_{\\mathcal{N}(i)}, \\mathbf{\\theta}_i), \\mathbf{c}_i) \\label{eq4} \\tag{4}\\]본 연구에서는 $\\mathbf{\\theta}_i^*$ 와 유사한 filter parameter $\\hat{\\mathbf{\\theta}}_i$ 를 얻기 위해 secondary features $\\mathbf{x}_i = (x_1, x_2, …, x_N)$와 function $\\mathcal{G}$를 이용한다(\\ref{eq5}).\\[\\hat{\\mathbf{\\theta}}_i = \\mathcal{G}(\\mathbf{x}_i), \\quad \\mathcal{G}^* = \\underset{\\mathcal{G}}{\\text{argmin }}E(h(\\bar{\\mathbf{s}}_{\\mathcal{N}(i)}, \\mathcal{G}(\\mathbf{x}_i)), \\mathbf{c}_i) \\label{eq5} \\tag{5}\\]이때 function $\\mathcal{G}$를 MLP를 이용해서 표현한다.사용된 MLP의 구조.Primary features본 논문에서는 screen position(2), color(3), world position(3), shading norma(3), texture values for the first and second intersections(각각 3), direct illumination visibility(1)의 총 18차원 7개의 primary feature을 사용한다. Primary feature들에 해당하는 distance function은 아래와 같다(\\ref{eq6}, \\ref{eq7}).\\[D(\\bar{\\mathbf{c}}_i, \\bar{\\mathbf{c}}_j) = \\frac{||\\bar{\\mathbf{c}}_i-\\bar{\\mathbf{c}}_j||^2}{\\psi_i^2 + \\psi_j^2 + \\zeta} \\label{eq6} \\tag{6}\\]이때 $\\psi_i$, $\\psi_j$는 pixel $i$, $j$의 color std이고, $\\zeta = 10^{-10}$는 0으로 나누는 것을 방지하기 위한 작은 상수이다.\\[D_k(\\bar{\\mathbf{f}}_{i,k}, \\bar{\\mathbf{f}}_{j,k}) = \\frac{||\\bar{\\mathbf{f}}_{i,k}-\\bar{\\mathbf{f}}_{j,k}||^2}{\\max(\\psi_{k,i}^2, \\delta)} \\label{eq7} \\tag{7}\\]이때 $\\psi_{k,i}$는 pixel $i$의 $k$번째 feature의 std이고, $\\delta = 10^{-4}$는 0으로 나누는 것을 방지하기 위한 작은 값이다.Secondary features각 pixel마다 primary feature에 기반한 secondary feature을 계산해서 사용한다. 본 논문의 section 3.3에 아주 자세하게 설명이 되어 있으니 이를 참고하길 바란다.Overall framework전체 framework.Training detailsLoss function본 논문에서는 일반적인 MSE가 아니라 relative mean squared error(RelMSE)를 사용했다(\\ref{eq8}).\\[E_i = \\frac{n}{2} \\sum_{q \\in \\{r,g,b\\}} \\frac{(\\hat{c}_{i,q} - c_{i,q})^2}{c_{i,q}^2 + \\epsilon}\\]$\\epsilon = 0.01$은 0으로 나누는 것을 방지하기 위한 작은 상수이다. 이는 사람의 visual system이 어두운 부분에서의 오차에 더 민감하기 때문에 사용했다고 한다.더 자세한 사항은 본 논문을 참조하길 바란다.Result이전의 연구들에 비해 크게 발전된 결과를 얻을 수 있었다.이전의 연구들에 비해 크게 발전된 것을 볼 수 있다.더 자세한 사항은 본 논문을 참조하길 바란다.ConclusionWe have presented a machine learning approach to reduce noise inMonte Carlo (MC) rendered images. In order to model the complexrelationship between the ideal filter parameters and a set of featuresextracted from the input noisy samples, we use a multilayer perceptron(MLP) neural network as a nonlinear regression model. Toeffectively train the network, we combine the MLP network witha filter such that the standard MLP takes in a set of secondary featuresextracted from a local neighborhood at each pixel and outputsa set of filter parameters. These parameters and the noisy samplesare given as inputs to the filter to generate a filtered pixel that iscompared to the ground truth pixel during training. We train ourproposed system on a set of scenes with a variety of distributedeffects and then test it on different scenes containing motion blur,depth of field, area lighting, glossy reflections, and global illumination.Our results show that this simple approach demonstratesvisible improvement over existing state-of-the-art methods." }, { "title": "Interactive Monte Carlo Denoising using Affinity of Neural Features", "url": "/posts/AffinityMC/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-04-30 00:00:00 +0900", "snippet": "AbstractHigh-quality denoising of Monte Carlo low-sample renderings remains a critical challenge for practical interactive ray tracing. We present a new learning-based denoiser that achieves state-of-the-art quality and runs at interactive rates. Our model processes individual path-traced samples with a lightweight neural network to extract per-pixel feature vectors. The rest of our pipeline operates in pixel space. We define a novel pairwise affinity over the features in a pixel neighborhood, from which we assemble dilated spatial kernels to filter the noisy radiance. Our denoiser is temporally stable thanks to two mechanisms. First, we keep a running average of the noisy radiance and intermediate features, using a per-pixel recursive filter with learned weights. Second, we use a small temporal kernel based on the pairwise affinity between features of consecutive frames. Our experiments show our new affinities lead to higher quality outputs than techniques with comparable computational costs, and better high-frequency details than kernel-predicting approaches. Our model matches or outperfoms sota offline denoisers in the low-sample count regime (2–8 samples per pixel), and runs at interactive frame rates at 1080p resolution.introduction대부분의 sota MC denoiser들은 large kernel-predicting network 을 사용한다. Off-line 상황에서는 이들의 computational cost가 큰 문제가 되지 않지만, interactive application에서 사용하기에는 무거운 감이 없지않아 있다. 그래서 빠른 denoiser들은 아직도 hand-crafted filter이나 compact neural network를 통해 quality를 희생해서 performance를 챙긴다. 추가적으로, video animation의 경우에는 denoising artifact들이 극대화되어 flickering같은 문제가 발생할 가능성도 있다.본 논문에서는 이를 해결하고자 lightweight neural network를 사용해 per-sample information을 low-dimensional per pixel feature vector로 바꿔서 학습을 진행한다. 여기에서 feature간의 novel pairwise affinity라는 개념을 사용해 neighboring per-pixel radiance value들의 weight를 정해준다.Denoising with learned pairwise affinityInteractive application에서는 낮은 spp로 rendering된 image를 사용하기 때문에, temporal stability를 유지하는 것이 정말 힘들다. 본 논문은 recursive filter을 사용해 frame간에 sample information의 평균을 구하고, denoising kernel을 frame들에 쌍으로 적용해서 이를 해결하고자 한다. 전체 framework은 아래에서 확인할 수 있다.전체 frameworkInput path-traced sample featuresSample에 대한 정보를 사용하면 pixel 정보만 사용하는 것에 비해 denoising의 성능이 좋아지지만, 그만큼의 computational overhead가 생기게 된다. 본 논문에서는 이 문제를 해결하기 위해 sample의 정보를 filtering weight를 만드는 데에는 사용하지만, filter 자체는 pixel radiance에 적용한다. Rendering 하는 과정에서 sample당 18차원의 feature vector $\\mathbf{r}_{xyst}$를 저장한다. 여기에는 diffuse radiance(3), specular radiance(3), normal(3), depth(1), roughness(1), albedo(3)와 추가적인 4 binary variables ‘emissive’, ‘metallic’, ‘transmissive’, ‘specular-bounce’가 포함되어 있다.Mapping samples to per-pixel featuresSample space에서의 process를 최소화 하기 위해, per sample feature $\\mathbf{r}_ {xyst}$ 는 얕은 fully connected network을 통해 per pixel embedding $\\mathbf{e}_{xyt}$으로 변화하게 된다(\\ref{eq1}).\\[\\mathbf{e}_{xyt} = \\frac{1}{S}\\sum_{s=1}^{S}\\text{FC}(\\mathbf{r}_{xyst}) \\label{eq1} \\tag{1}\\]Spatio-temporal feature propagation만들어진 per-pixel embedding $\\mathbf{e}_ {xyt}$는 lightweight U-net을 통해 process된다. U-net은 현재 frame 뿐만 아니라 이전의 frame에 대한 embedding을 받아서 feature vector $\\mathbf{f}^k_{xyt}$와 여러 scalar들을 생성한다(\\ref{eq2}).\\[\\text{UNet}(\\mathbf{e}_{xyt}, \\mathcal{W}_t\\bar{\\mathbf{e}}_{xy,t-1}) = \\left( \\mathbf{f}^k_{xyt}, a^k_{xyt}, b^k_{xyt}\\right), b^k_{xyt}, \\lambda^k_{xyt} \\label{eq2} \\tag{2}\\]이때 k는 각 kernel을 의미하고, 총 $K$개의 dilated spatial kernel이 만들어지게 된다. Kernel들은 affinity feature $\\mathbf{f}^k_{xyt}$들 사이의 distance에 scaling factor $a^k_{xyt}$를 곱해서 만들어지고, $c^k_{xyt}$는 kernel의 central weight가 된다. $b^k_{xyt}$는 인접한 frame 사이의 feature affinity를 조절하기 위한 parameter이고, $\\lambda^k_{xyt}$는 인접한 frame의 기여도를 정하는 parameter이다. $\\mathcal{W}_t$는 frame $t-1$를 frame $t$로 reproject하는 warping factor 이고, $\\bar{\\mathbf{e}} _{xyt}$는 pixel embedding들의 temporal accumulation으로, 위의 parameter을 이용해 다음과 같이 정의된다(\\ref{eq3}).\\[\\begin{cases}\\bar{\\mathbf{e}}_{xy0} = \\mathbf{e} _{xy0} \\\\\\bar{\\mathbf{e}}_{xyt} = (1-\\lambda _{xyt})\\mathbf{e} _{xyt} + \\lambda _{xyt}\\mathcal{W}_t\\mathbf{e} _{xy,t-1}\\end{cases} \\label{eq3} \\tag{3}\\]Spatial kernels from pairwise affinitySpatial filtering kernel은 다음과 같이 정의된다(\\ref{eq4}).\\[w^k_{xyuvt} = \\begin{cases}c^k{xyt} &amp;\\text{if } x=u \\text{ and } y=v, \\\\\\text{exp}\\left( -a^k_{xyt}||\\mathbf{f}^k_{xyt} - \\mathbf{f}^k_{uvt}||^2_2 \\right) &amp;\\text{otherwise.}\\end{cases} \\label{eq4} \\tag{4}\\]Kernel의 center weight를 다른 parameter로 정해주는 이유는 outlier handling에 도움이 되기 때문이다. 만약 어떠한 pixel의 값이 MC noise 측면에서 outlier이라고 판단되면, $c^k_{xyt}$를 0과 가깝게 만들어서 그 pixel이 영향을 억제시킬 수 있다.Outlier pixel의 경우, kernel의 center weight를 다른 parameter를 이용해서 정하면 이를 효과적으로 억제할 수 있다.Temporally-stable kernel based denoisingFiltering을 하기에 앞서, noisy radiance는 frame에 따라 일정한 가중치를 두고 accumulate 된다. 이는 특정한 motion이 있는 부분 등에 많은 도움을 주고, 전체적인 temporal stability를 향상시킨다고 한다(\\ref{eq5}).\\[\\begin{cases}\\bar{\\mathbf{L}}_{xy0} = \\mathbf{L}_{xy0} \\\\\\bar{\\mathbf{L}}_{xyt} = (1-\\lambda_{xyt})\\mathbf{L}_{xyt} + \\lambda_{xyt}\\mathcal{W}_t\\bar{\\mathbf{L}}_{xyt}\\end{cases} \\label{eq5} \\tag{5}\\]이때 $\\lambda_{xyt}$는 앞서 U-Net을 통해 얻어진 parameter이다(\\ref{eq2}).Filtering은 아래와 같이 이루어진다(\\ref{eq6}).\\[\\mathbf{L}^{(k)}_{xyt} = \\frac{\\sum_{u,v}w^k\\mathbf{L}^{(k-1)}_{uvt}}{\\epsilon + \\sum_{u,v}w^k_{xyuv}} \\label{eq6} \\tag{6}\\]이때 $\\epsilon = 10^{-10}$ 는 0으로 나누는 것을 방지하기 위한 작은 상수이다.마지막 filtering step에서는 현재 frame과 바로 이전 frame의 feature간의 affinity를 측정하는 temporal kernel을 사용한다. Kernel의 weight은 다음과 같다(\\ref{eq7}).\\[\\omega_{xyuvt} = \\text{exp}\\left( -b_{xyt}||\\mathbf{f}^K_{xyt} - \\mathcal{W}_t\\mathbf{f}^K_{uv,t-1}||^2_2 \\right) \\label{eq7} \\tag{7}\\]최종 output은 다음과 같이 얻을 수 있다(\\ref{eq8}).\\[\\mathbf{O}_{xyt} = \\frac{\\sum_{u,v}w^K_{xyuv}\\mathbf{L}^{(K-1)}_{uvt} + \\sum_{u^{'},v^{'}}\\omega_{xyu^{'}v^{'}}\\mathcal{W}_t\\mathbf{O}_{u^{'}v^{'},t-1}}{\\epsilon + \\sum_{u,v}w^K_{xyuv} + \\sum_{u^{'},v^{'}}\\omega_{xyu^{'}v^{'}}} \\label{eq8} \\tag{8}\\]Training detailsDatasetSBMC의 scene generator을 사용해서 dataset을 만들었다. 여기에 추가적으로 camera translation, rotation에 기반한 motion을 추가해서 넣었다.Loss전체 loss function은 다음과 같다(\\ref{eq9}).\\[\\mathcal{L} = \\mathcal{L}_\\text{recons} + 0.25\\cdot\\mathcal{L}_\\text{temporal}+10^{-5}\\cdot\\mathcal{L}_\\text{reg} \\label{eq9} \\tag{9}\\]Reconstruction loss($\\mathcal{L}_ \\text{recons}$), temporal loss($\\mathcal{L}_ \\text{temporal}$)에는 SMAPE가 사용되었고, $\\mathcal{L}_ \\text{reg}$는 $L_2$ regularization loss이다.더 자세한 정보는 AffinityMC 논문을 참조하길 바란다.Result이전의 kernel based 연구들보다 더 빠른 속도를 보였고, 성능을 조금 내려놓은 interactive 연구들보다는 속도가 느렸지만 kernel based 연구들보다 성능이 좋을 정도로 성능 면에서 차별점을 보였다. 더 자세한 정보는 AffinityMC 논문을 참조하길 바란다.속도와 성능을 종합해봤을 때 상당히 좋은 결과를 보인다.성능 면에서 많은 improvement가 있었다.ConclusionWe have presented a novel method for denoising Monte Carlo renderings at interactive speeds with quality on-par with off-line denoisers. We use an efficient network to aggregate relevant per-sample features into temporally-stable per-pixel features. Pairwise affinity between these features are used to predict dilated 2D kernels that are iteratively applied to the input radiance to produce the final denoised result. We show our model can spatially adjust the kernels to effectively smooth out noise and preserve fine details. We further demonstrate how to incorporate the spatially-warped content from previous frames to produce a temporally consistent result." }, { "title": "Adversarial Monte Carlo Denoising with Conditioned Auxiliary Feature Modulation", "url": "/posts/AdvMC/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-04-24 00:00:00 +0900", "snippet": "AbstractDenoising Monte Carlo rendering with a very low sample rate remains a ma- jor challenge in the photo-realistic rendering research. Many previous works, including regression-based and learning-based methods, have been explored to achieve better rendering quality with less computational cost. However, most of these methods rely on handcrafted optimization objectives, which lead to artifacts such as blurs and unfaithful details. In this paper, we present an adversarial approach for denoising Monte Carlo rendering. Our key insight is that generative adversarial networks can help denoiser networks to produce more realistic high-frequency details and global illumination by learning the distribution from a set of high-quality Monte Carlo path tracing images. We also adapt a novel feature modulation method to utilize auxiliary features better, including normal, albedo and depth. Compared to previous state-of-the-art methods, our approach produces a better reconstruction of the Monte Carlo integral from a few samples, performs more robustly at different sample rates, and takes only a second for megapixel images.IntroductionDeep learning에 기반한 Monte Carlo denoising은 이전의 방법들에 비해 크게 발전된 성능을 보였다. 하지만 이들은 MSE, MAPE loss같은 handcraft optimization objective을 이용해서 진행된다. 실제로 이 paper기준 sota methods들도 noise가 많은 영역을 잘 처리하지 못하고, 그저 loss 값이 낮아지게 over-smoothed된 결과를 만들어냈다. 이는 denoised image가 원래 image의 fine details를 잃어버리게 한다.여러 method를 이용해 denoised된 image. 이전의 방법들은 fine details들이 많이 손실된 것을 확인할 수 있다.본 논문에서는 image의 fine details들을 최대한 유지하면서 denoising을 하기 위해 generative adversarial networks(GANs)의 아이디어를 사용했다. GAN은 특정 image들의 data distribution을 학습해 novel realistic image들을 만들어내는 데에 뛰어난 성능을 보인다. 본 논문의 저자들은 MC denoising도 이와 비슷하게 realistic한 image를 만드는 것을 목표로 하고, rendering을 통해 생성되어서 texture, camera information, lighting condition등의 다양한 정보들이 있기 때문에 일반적인 image generation보다 유리한 조건이라고 말한다. 추가적으로 본 논문에서는 Wasserstein distance를 활용해 generative model을 학습시켰는데, 이는 KL, JS divergence보다 더 좋은 성능을 보였다고 한다.CNN based MC denoising의 한계CNN을 사용한 deep learning based MC denoising은 noisy input과 denoised output사이의 관계를 효과적으로 예측할 수 있다. 하지만 이에는 다음과 같은 한계점들이 존재한다. 사용하는 loss function이 perceptual한 면에서 더 많은 정보를 제공할 수 있어야 한다. 본 논문 기준 최근의 연구들에서는 L1, L2 loss같은 image-space metric을 사용하는데, 이는 blurry result을 만들어낸다고 한다. 본 논문에서는 이를 해결하고자 더 general loss function을 사용하는 adversarial mechanism을 제시했다. 사용하는 auxiliary feature들이 더 효율적으로 사용되어야 한다. 본 논문 이전의 연구들에서는 그저 noisy color과 concatenation을 통해 input으로 사용되었지만, 본 논문에서는 더 발전된 auxiliary feature modulation을 제안한다. Adversarial MC denoisingGAN과 마찬가지로, adversarial mc denosing은 denosing network과 critic network을 통해 이루어진다. 이때 denoising network에서는 앞서 언급한 auxiliary feature modulation이 사용된다.FrameworkKPCN과 동일하게 noisy color를 diffuse, specular component으로 나누어서 학습을 진행한다. Denosing network은 다음과 같이 정의된다(\\ref{eq1}).\\[c_{out} = G(c_{in}, b_{feat}) \\label{eq1} \\tag{1}\\]이때 $c$는 color(diffuse 혹은 specular), $b_{feat}$는 auxiliary buffer를 의미한다. $G$의 parameter $\\theta_G$는 다음과 같이 critic network $D$를 이용해서 학습된다(\\ref{eq2}).\\[\\underset{\\theta_G}{\\text{min}}\\,\\underset{\\theta_D}{\\text{max}}\\, D(G(c_{in}, b_{feat}),c_{gt}) \\label{eq2} \\tag{2}\\]전체 framework의 구조는 아래와 같다.전체 adversarial frameworkAuxiliary buffer conditioned modulation이전의 filter based denoising에서부터 auxiliary feature을 계속해서 사용해 왔지만, neighborhood에 대한 정보 없이 한 pixel 내에서 color과 auxiliary feature의 correlation을 이용하자니 과한 assumption이 필요했고, 많은 visual artifact가 발생했다. 최근의 learning based denoising에서는 이를 해결하고자 CNN을 이용해 neighborhood에서 정보를 gather하는 방식을 사용했지만, 이는 color와의 concatenation으로 network에 feed 된 것이기 때문에 전체 과정 중 early stage에만 영향을 줄 수 있었다. 본 논문에서는 conditioned feature modulation(CFM)을 이용해 이러한 문제들을 해결하고자 한다. 여기에는 conditional biasing과 scaling이 포함되는데, CFM operation은 다음과 같이 정의된다(\\ref{eq3}).\\[\\mathbf{CFM}(L_{in}) = \\mathbf{\\gamma}(\\hat{b}_{feat}) \\odot L_{in} \\oplus \\mathbf{\\beta}(\\hat{b}_{feat}) \\label{eq3} \\tag{3}\\]이때 $\\hat{b}_ {feat}$ 는 EncoderNet을 통해 변환된 auxiliary feature를 의미하고, $\\mathbf{\\gamma}$, $\\mathbf{\\beta}$는 학습가능한 scaling, shifting matrices이다. 즉, auxiliary feature에 dependent하게 noisy color를 scale, shift해서 denoised image를 얻어내는 것을 목표로 한다.Denosing and criticising in an adversarial approachLearning-based denoising model들은 high-frequency noisy region같은 복잡한 상황에서 좋은 성능을 보여주지 못했다. 본 논문의 저자들은 실험을 통해 network structure보다 loss function이 결과에 더 큰 영향을 미친다는 것을 발견했다고 한다. 그리고 denoising task는 ill-posed problem이기 때문에, L1 loss, L2 loss같은 pixel-wise loss를 사용하면 정확한 denoised된 값을 얻는게 아니라 over-smoothed된 값을 얻을 가능성이 높다. 그렇기 때문에 본 논문에서는 human perception과 비슷한 metric을 사용하는 critic을 통해 이 문제를 해결하고자 한다. Loss를 구하는 과정에서 일반적인 L1 loss와 critic network에서 얻어지는 adversarial loss을 합해서 구했는데, critic은 Wasserstein-GAN을 기반으로 만들어졌다고 한다.Training detailsDataset저자들의 commercial renderer로 rendering된 1000장의 indoor scene을 사용했고, 900장을 training에, 100장을 validation에 사용했다. Testing을 할 때는 이전 연구들과의 비교를 위해 KPCN에서 사용한 tungsten renderer로 rendering한 image들을 사용했다. 실험과정에서는 image를 $128\\times 128$ 크기의 patch들로 나눠서 학습을 했다고 한다.Commercial renderer로 만들어진 image.Tungsten renderer로 만들어진 image.Result이전의 연구들에 비해서 전반적으로 더 좋은 결과를 보였다. 신기한 것은 더 높은 spp에서 학습된 model을 이용해 더 낮은 spp를 denoise하는 데에도 괜찮은 성능을 보였다는 점이다. 더 자세한 설명은 AdvMC 논문을 참조하길 바란다.32spp로 rendering된 image를 denoising한 결과. 이전 연구들에 비해 성능이 좋은 것을 확인할 수 있다.32spp에서 학습된 model로 4spp image를 denoising한 결과. 이것 역시 이전 연구들에 비해 성능이 좋다.ConclusionWe have presented the first adversarial learning framework for the Monte Carlo denoising problem and have achieved state-of-the-art results with improved perceptual quality. Moreover, our framework sheds light on exploring the relationship between auxiliary features and noisy images by neural networks. Comprehensive evaluations have demonstrated the effectiveness and efficiency of our framework over previous works." }, { "title": "많이 사용하는 loss, Evaluation metrics", "url": "/posts/Evaluation-metrics/", "categories": "coding", "tags": "", "date": "2022-04-20 00:00:00 +0900", "snippet": "LossMSE(Mean Squared Error)MSE는 그 이름을 보면 무엇을 의미하는지 알 수 있는데, 오차(error)의 제곱의 평균을 의미한다. 이를 수식으로 쓰면 다음과 같다.\\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{Y}_i - Y_i)^2\\]Evaluation metricsPSNR(Peak Signal-to-Noise Ratio)PSNR은 image의 비교에서 자주 사용되는 metric이다. 한글로 번역하면 ‘최대 신호 대 잡음비’가 되지만, 잘 와닿는 것 같지는 않다. PSNR의 수식은 아래와 같다.\\[\\text{PNSR} = 10\\cdot\\log_{10}\\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\\]이때 MAX는 가능한 pixel의 최댓값으로, 일반적인 RGB image의 경우 255가 된다. 수식 상으로는 PSNR이 클수록 두 image가 서로 비슷해야 하지만, 인간이 시각적으로 느끼는 품질 차이와는 다르기 때문에 그 반대인 경우가 있을 수도 있다.SSIM(Structural Similarity Index Map)SSIM은 수치적인 오차가 아니라 인간의 시각에서의 차이를 고려하기 위해 만든 metric이다. SSIM의 수식은 아래와 같다.\\[\\text{SSIM}(x,y) = l(x,y)^\\alpha\\cdot c(x,y)^\\beta\\cdot s(x,y)^\\gamma\\]이때 l은 luminance, c는 contrast, s는 structural의 의미를 갖고 있다. 이 값들은 아래와 같이 정의된다.\\[\\begin{align} &amp;l(x,y) = \\frac{2\\mu_x\\mu_y + C_1}{\\mu_x^2+\\mu_y^2+C_1} \\\\ &amp;c(x,y) = \\frac{2\\sigma_x\\sigma_y + C_2}{\\sigma_x^2+\\sigma_y^2+C_2} \\\\ &amp;s(x,y) = \\frac{\\sigma_{xy}+C_3}{\\sigma_x\\sigma_y+C_3}\\end{align}\\]이때 $\\mu_x, \\mu_y, \\sigma_x, \\sigma_y, \\sigma_{xy}$ 는 각각 image $x$, $y$의 mean, std, 그리고 cross-covariance를 의미한다. $C_1, C_2, C_3$는 특정한 상수로, 아래와 같이 정의된다.\\[\\begin{align} &amp;C_1 = (0.01\\cdot L)^2 \\\\ &amp;C_2 = (0.03\\cdot L)^2 \\\\ &amp;C_3 = C_2/2\\end{align}\\]$L$은 dynamic range value로, 일반적인 RGB image에서는 255가 된다. 위의 값들을 다 대입하면 최종 SSIM 식은 아래와 같이 나온다.\\[\\text{SSIM}(x,y) = \\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\\]LPIPSIS(Inception score)IS는 generative model에서 자주 사용하는 evaluation metric으로, ImageNet에 pretrained된 model인 inception-v3를 사용해서 GAN에 의해 생성된 image를 분류한다. Label $y$, image $x$에 대해 IS는 다음과 같이 정의된다.\\[\\text{IS} = \\text{exp}(\\mathbb{E}_{x\\sim p_{data}}D_{KL}(p(y|x)||p(y)))\\]FID(Frechet Inception Distance)FID는 IS의 여러 한계를 극복하기 위해 제시된 evaluation metric이다. 이 역시 ImageNet에 pretrained된 inception-v3 model을 사용하는데, 이때 마지막 layer을 제거하고 그 바로 전 layer의 activation을 사용한다. 이 activation의 mean과 covariance를 구해서 분포 사이의 distance를 이용해 구할 수 있다.\\[\\text{FID} = d^2 = ||\\mu_1 - \\mu_2||^2 + \\text{TR}(C_1+C_2-2C_1C_2)\\]CAPD" }, { "title": "Neural Denoising with Layer Embeddings", "url": "/posts/LBMC/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-04-17 00:00:00 +0900", "snippet": "AbstractWe propose a novel approach for denoising Monte Carlo path traced images, which uses data from individual samples rather than relying on pixel aggregates. Samples are partitioned into layers, which are filtered separately, giving the network more freedom to handle outliers and complex visibility. Finally the layers are composited front-to-back using alpha blending. The system is trained end-to-end, with learned layer partitioning, filter kernels, and compositing. We obtain similar image quality as recent state-of-the-art sample based denoisers at a fraction of the computational cost and memory requirements.IntroductionMonte Carlo denoising는 적은 spp로 만들어진 noisy image를 denoise하는 것을 목표로 한다. KPCN에서는 이에 machine learning과 noisy image에 적용할 수 있는 kernel을 만든다는 아이디어를 제공했고, SBMC에서는 noisy image만을 이용하는 것이 아니라 rendering에 사용되는 각 sample을 활용하는 per-sample denoiser에 대한 아이디어를 제공했다.Per-sample denoiser은 뛰어난 성능을 보이지만, computationally expensive하기 때문에 real-time rendering에 사용되기에 힘들다는 문제가 있다. 추가적으로 kernel을 각 sample에 적용하기 때문에, sample의 수가 많아지면 denoising task가 쉬워져야하는 것과는 역설적으로 consume하는 resource가 많아진다. 본 논문(LBMC)에서는 neural network를 사용해 각 sample에 대한 compact representation을 학습시켜 전반적인 computational cost를 줄이고자 한다.이를 위해 LBMC는 각 sample을 layer로 partition하는 것을 배우는 novel architecture을 사용한다. Kernel은 sample 단위가 아니라 만들어진 layer단위로 이루어지게 되고, 이는 SBMC와 비슷한 유익을 누리게 해준다. 이 방법은 sample의 개수보다는 layer의 개수에 영향을 받기 때문에 더 효율적으로 denoising을 진행할 수 있고, 실제로 layer의 개수가 2개일 때도 sample-based denoiser의 이점을 모두 가질 수 있다고 한다.Architecture앞부분의 구조는 SBMC와 비슷한 구조를 갖고 있고, 뒷부분에 layer embedding stage가 있다. SBMC와 비교했을 때 UNet을 이용해 input feature의 개수를 74개에서 20개로 줄인다. 전체적인 구조는 아래의 5개 stage로 나눌 수 있다. Sample reducer : Input sample radiance, feature의 차원을 줄여 sample embedding을 만든다. U-net : Sample embedding을 이용해 context feature을 만든다. Sample partitioner : Sample embedding을 2개 이상의 layer에 splat해서 layer embedding을 만든다. Splatting weight은 sample embedding과 context feature, 그리고 FCN을 사용해서 만들어진다. Layer filtering : Layer embedding, context features 그리고 FCN을 사용해 kernel을 만들어 각 layer에 적용시킨다. Compositing : 각 layer에서 생성된 결과를 composite해서 최종적인 결과를 생성한다.Sample embedding과 context feature을 만드는 과정은 SBMC와 유사하기 때문에 설명을 생략하겠다. LBMC model의 architecturePer-layer processingFully connected network을 이용해 sample별 weight $w^l_{xys}$ 만들고, 이를 통해 sample embedding을 각 layer에 splat 해준다. 이때 sample당 weight의 합은 1이 된다. 각 layer마다 weight sum $w^l_{xy}$, layer occupancy $n^l_{xy}$(sample이 어느 layer에 속해 있는지에 대한 정보), weighted sum of radiance $L^l_{xy}$, weighted sum of embeddings $E^l_{xy}$를 track 해준다(\\ref{eq1}).\\[\\begin{align} &amp;w^l_{xy} = \\sum_s w^l_{xys} \\\\ &amp;n^l_{xy} = \\sum_s \\sum_{k=0}^{l-1}w^k_{xys} \\\\ &amp;L^l_{xy} = \\sum_s w^l_{xys}L_{xys} \\\\ &amp;E^l_{xy} = \\sum_s w^l_{xys}E_{xys} \\label{eq1} \\tag{1}\\end{align}\\]Kernel이 만들어지면 $L, w, n$에 적용되어 새로운 값을 생성하게 된다(\\ref{eq2}).\\[\\begin{align} &amp;\\bar{L}^l_{xy} = L^l_{xy} * K^l_{xyuv} \\\\ &amp;\\bar{w}^l_{xy} = w^l_{xy} * K^l_{xyuv} \\\\ &amp;\\bar{n}^l_{xy} = n^l_{xy} * K^l_{xyuv} \\label{eq2} \\tag{2}\\end{align}\\]Compositing각 Layer에서 나온 결과를 composite해서 최종 결과를 만든다. 먼저 radiance와 layer weight을 normalize 해주고, 이들을 이용해 최종 pixel color $o_{xy}$를 계산해준다(\\ref{eq3}).\\[\\begin{align} &amp;\\hat{L}^l_{xy} = \\frac{\\bar{L}^l_xy}{\\bar{n}^l_{xy}},\\quad \\alpha^l_{xy} = \\frac{\\bar{w}^l_xy}{\\bar{n}^l_{xy}} \\\\ &amp;o_xy = \\hat{L}^0_{xy} + \\sum_{l=1}^{N} \\hat{L}^l_{xy} \\prod_{j=0}^{l-1}(1-\\alpha^j_{xy}) \\label{eq3} \\tag{3}\\end{align}\\]Dataset and training학습에는 4352장의 $256\\times256$, 5 bounces, Russian Roulette disabled, 8spp rendered image를 사용했고 ground truth image는 4096spp rendered image를 사용했다.Symmetric mean absolute percentage error(SMAPE)라는 metric을 사용했는데, HDR image를 denoise하는 데 stable한 metric이라는 report가 있었다고 한다. Reference $\\mathbf{r}$, denoised image $\\mathbf{d}$에 대해 수식은 다음과 같다(\\ref{eq4}).\\[SMAPE(\\mathbf{r}, \\mathbf{d}) = \\frac{1}{3N}\\sum_{p \\in N} \\sum_{c \\in C} \\frac{|\\mathbf{d}_{p,c} - \\mathbf{r}_{p,c}|}{|\\mathbf{d}_{p,c}| + |\\mathbf{r}_{p,c}| + \\epsilon} \\label{eq4} \\tag{4}\\]더 자세한 정보는 LBMC 논문을 참조하길 바란다.ResultLBMC는 previous work와 비교했을 때 성능 면에서는 큰 차이를 보이지 않는다. 하지만 computational cost 측면에서는 훨씬 더 효율적인 모습을 보인다. 특히 SBMC와 유사한 SampleSplat와 비교해 보면 computational cost 측면에서 상당히 많이 향상된 것을 볼 수 있다. 더 자세한 설명은 LBMC 논문을 참조하길 바란다.성능 면에서는 previous work과 비슷한 모습을 보여준다.Computational cost부분에는 상당히 많은 improvement가 있었다.ConclusionWe have presented a layer-based denoising algorithm that produces image quality comparable to per-sample denoising, both visually and in image quality metrics, while being almost as efficient as denoisers working on accumulated pixel values. We denoise 1080p images at interactive rates on contemporary GPUs.We observe similar robustness against outlier samples, a smoother look, and better handling of out-of-focus regions as first shown by SBMC. In general, it seems beneficial to give the network the flexibility to apply more than one kernel per pixel. In practice, we see most benefits already with two layers.When comparing PIXELGATHER and SAMPLESPLAT in our evaluation, the differences are fairly subtle, and smaller than what we had anticipated. This may be an effect of our reduced input feature count (20 instead of 74 floats) compared to SBMC, and differences in training data and test set. In scenarios where runtime performance is critical, it remains an open question if it is worth the added cost of incorporating per-sample information in machine learning denoisers, both in terms of the additional bandwidth usage requirements and the added arithmetics of per-sample or per-layer kernels. Extreme firelies are less common in real-time rendering settings with short ray paths and smooth approximations of global illumination.Still, we argue that a layered denoising architecture is a flexible, scalable tool to exploit per-sample data. Our architecture learns to partition samples into layers, learns unique filter kernels per layer and alpha composite the filtered layers, all trained end-to-end. We hope this research will inspire future progress in denoising for both offline and real-time rendering.In future work, we hope to apply similar ideas to deep compositing workflows. We also want to extend the layer denoising approach to the temporal domain, by denoising sequences, similar to recent work [CKS∗ 17, VRM∗ 18, HMP∗ 20]. We believe a layered representation can be beneficial to more robustly handle disocclusion and ghosting effects." }, { "title": "학습을 멈췄는데도 GPU가 돌아가는 현상 해결법", "url": "/posts/tip1/", "categories": "coding", "tags": "", "date": "2022-04-12 00:00:00 +0900", "snippet": "Deep learning 실험을 하다 보면 도중에 실험을 멈춰야 하는 경우가 상당히 많이 나온다. 더 좋은 방법이 있을 수도 있겠지만, 나는 ‘control + c’ 커멘드를 이용해서 실험을 멈춘다.대부분의 경우 이렇게 하면 실험이 잘 멈추고, GPU 사용도 멈추게 된다. 하지만 가끔 실험은 멈췄는데, GPU는 계속 사용중인 경우가 발생한다.python 실행은 멈췄지만.. GPU를 확인해보면 하나가 계속 사용중이라고 뜬다.당연히 이러한 상태가 된 GPU로는 다른 실험을 돌릴 수 없다.이 문제를 해결하려면 직접 돌아가는 python process를 찾아서, 강제종료 시키면 된다. ps -ef | grep pythonTerminal에 위 명령을 입력하면, 현재 실행되고 있는 모든 python process를 보여준다. 이중 종료되었어야 할 process를 찾아서 ‘kill’ 명령어로 종료시키면 된다.이런 식으로 종료되지 않은 process를 찾을 수 있다. 3162, 3164 process를 kill하면 된다. kill -9 'pid'하지만 실행되고 있는 python process가 상당히 많을 것이기 때문에 어떤 것을 종료시켜야 될지 모를 수도 있는데, 나는 보통 작업하고있는 가상환경(pyenv)에 해당하는 process를 찾아서 종료시킨다.처음 이 상황을 겪었을 때는 당연히 python 프로그램과 함께 GPU사용도 멈췄을 것이라고 생각했는데, 다른 실험을 돌렸을 때 계속 OOM(out of memory)이 떠서 애꿎은 코드에서만 계속 잘못된 부분을 찾았던 기억이 있다. 뭔가 더 스마트한 방법이 있을 것 같기는 하지만, 그래도 이러한 정보를 알아두면 언젠간 도움이 될 수도 있을 것 같다." }, { "title": "Sample-based Monte Carlo Denoising using a Kernel-Splatting Network", "url": "/posts/SBMC/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-04-08 00:00:00 +0900", "snippet": "AbstractDenoising has proven to be useful to efficiently generate high-quality Monte Carlo renderings. Traditional pixel-based denoisers exploit summary statistics of a pixel’s sample distributions, which discards much of the samples’ information and limits their denoising power. On the other hand, sample based techniques tend to be slow and have difficulties handling general transport scenarios. We present the first convolutional network that can learn to denoise Monte Carlo renderings directly from the samples. Learning the mapping between samples and images creates new challenges forthe network architecture design: the order of the samples is arbitrary, and they should be treated in a permutation invariant manner. To address these challenges, we develop a novel kernel-predicting architecture that splats individual samples onto nearby pixels. Splatting is a natural solution to situations such as motion blur, depth-of-field and many light transport paths, where it is easier to predict which pixels a sample contributes to, rather than a gather approach that needs to figure out, for each pixel, which samples (or nearby pixels) are relevant. Compared to previous state-of-the-art methods, ours is robust to the severe noise of low-sample count images (e.g. 8 samples per pixel) and yields higher-quality results both visually and numerically. Our approach retains the generality and efficiency of pixel-space methods while enjoying the expressiveness and accuracy of the more complex sample-based approaches.IntroductionKPCN 논문에서 설명했듯이, Monte Carlo denoising은 적은 sample로 만들어진 noisy image를 이용해 clean image를 얻는 기법이다. 여기에는 크게 두가지의 종류가 있다고 하는데, rendering된 image를 이용하는 pixel-based method와, 실제로 image를 만들 때 사용된 sample에 대한 정보를 사용하는 sample-based method가 있다. 이전의 work에서는 대부분 pixel-based method를 사용했으며, noisy pixel color에 추가적으로 depth, normal, albedo등의 정보를 사용해서 denoisng을 한다. 본 논문, SBMC 에서는 sample-based method를 사용해 denoising을 수행하며, 이는 motion blur같은 특수한 상황에서 상당히 좋은 성능을 보인다. Ground truth image와 denoised image를 비교해 봤을 때, sample-based method가 motion blur을 더 잘 표현한다.Permutation invariance in Neural NetworksSample에 대한 정보를 사용하면, neural network에 여러개의 input이 들어가게 된다. 실제 Monte carlo rendering에서 image를 만드는 과정을 생각해보면, input의 순서에 따라 network의 결과값이 달라지면 안된다.Sample-based denoising networkSBMC는 sample-based method이기 때문에 pixel $(x,y)$ 에 대한 $s$개의 sample (noisy radiance $L_{xys}$, auxiliary features $f_{xys}$)를 input으로 받는다. Sample의 개수가 고정되어 있지 않기 때문에 기존의 single-pass feedforward neural network은 적합하지 않고, RNN 같은 경우에는 permutation-invariant하지 않기 때문에 적합하지 않다. 그래서 본 논문에서는 여러 per-sample non-linear processing들의 spatial information을 CNN을 통해 sharing하는 novel architecture을 소개한다.Previous work [Bako et al. 2017; Vogels et al. 2018]과 마찬가지로, SBMC도 noisy image에 apply 될 수 있는 kernel을 만드는 것을 목표로 한다. 하지만 pixel단위가 아닌 sample단위이기 때문에, 각 sample이 근처에 있는 pixel에 얼마나 contribute 하는지를 알아내는 것을 목표로 한다고 생각할 수 있다. 이 때문에 SBMC에서는 일반적인 gathering kernel이 아니라 splatting kernel, 즉 sample에 대한 정보를 주변 pixel에 splat하는 kernel을 사용한다(\\ref{eq1}).\\[I_{uv} = \\frac{\\sum_{x,y,s}K_{xyuvs}L_{xys}}{\\sum_{x,y,s}K_{xyuvs}}\\label{eq1} \\tag{1}\\]이때 $s$는 pixel $(x,y)$에 존재하는 각 sample, $K$는 predict하고자 하는 kernel, $I_{uv}$는 근처에 있는 pixel $(u,v)$의 denoised된 값을 의미한다. $K$는 KPCN과 마찬가지로 $21\\times21$의 크기로 사용한다.Sample embeddings and context featuresPermutation invariance를 만족시키기 위해, SBMC에서는 per-sample feature extraction과 spatial information sharing을 분리시킨다. 이때 individual sample embedding과 per-pixel context feature 라는 개념이 쓰인다.먼저 fully connected layer을 이용해 각 sample마다 embedding을 만든다. 이때 각 sample은 서로에게 영향을 주지 않기 때문에, sample의 순서에 영향을 받지 않는다. 즉, permutation invariance를 갖게된다(\\ref{eq2}).\\[E^0_{xys} = \\text{FC}(L_{xys},f_{xys};\\theta^0_E) \\label{eq2} \\tag{2}\\]그 후 각 sample들 사이의 정보 교환을 위해 $E^0$를 이용해 per-pixel context feature $C^0$를 계산해준다(\\ref{eq3}).\\[C^0_{xy} = \\underset{s}{\\text{reduce_mean}}(E^0_{xys}) \\label{eq3} \\tag{3}\\]그냥 평균을 취하는 것이기 때문에, permutation invariance가 유지된다. 추가적으로 $C^0$는 sample의 개수와 상관없이 같은 차원을 갖기 때문에 CNN을 사용할 수 있게 된다.이렇게 만들어진 $C^0$는 U-Net을 통해 같은 차원의 또다른 context feature $C^1$으로 변환되고, 이는 sample embedding $E^0$과 함께 fully connected layer에 feed되어 새로운 sample embedding $E^1$을 생성한다(\\ref{eq4}).\\[C^1 = \\text{UNet}(C^0;\\theta^0_C) \\quad E^1_{xys} = \\text{FC}(E^0_{xys},C^1_{xy}; \\theta^1_{E})\\label{eq4} \\tag{4}\\]이 과정을 3번정도 반복해서 만들어진 sample embedding이 kernel prediction에 사용되는데, 이는 자신에 대한 정보 뿐만 아니라 neighborhood의 정보도 담고 있기 때문에 coherent한 결과를 낼 수 있도록 해준다.Per-sample splatting kernelsSplatting kernel은 앞서 생성한 sample embedding과 fully connected network을 이용해서 만들어진다(\\ref{eq5}).\\[K_{xyuvs} = \\text{FC}(E^n_{xys}, C^n_{xy};\\theta_K) \\label{eq5} \\tag{5}\\]이때 $u,v$는 $x,y$의 neighbor pixel을 의미한다. 이때 gather kernel이 아니라 splatting kernel을 사용하는 이유는, sample을 기준으로 만든 embedding을 이용해 kernel을 만들기 때문에 sample의 view에서 작업을 수행하는 것이 permutation invariance를 유지하기 쉽기 때문이다. 더 자세한 내용은 아래 figure과 함께 설명되어 있다.앞서 kernel을 생성할 때 현재 pixel에 있는 sample embedding만을 사용하기 때문에, 이 상황처럼 a와 b의 index가 바뀌면 a에 b를 고려하며 학습된 잘못된 weight가 들어가게 된다. Splatting kernel을 사용할 경우 이를 방지할 수 있다.Overall structure and algorithmSBMC model의 전체적인 구조SBMC model의 algorithmTraining detailsSBMC에서는 $128\\times128$ 해상도의 rendered image 300,000장을 학습에 사용했고, 동일한 방식으로 1,000장을 rendering해서 validation에 사용했다. Ground truth image의 경우에는 4096 spp로 rendering된 image를 사용했다. 더 자세한 설명은 SBMC 논문을 참조하길 바란다.ResultSBMC는 previous work와 비교했을 때 좋은 성능을 보여줬으며, 특히 motion blur이나 depth of field등 auxiliary buffer에 noise가 큰 경우에 특출나게 좋은 성능을 보여줬다. 더 자세한 설명은 SBMC 논문을 참조하길 바란다.Depth of field 상황에서 KPCN보다 확연하게 뛰어난 성능을 보여준다.ConclusionWe propose a new convolutional neural network for denoising Monte Carlo renderings. The key innovations that explain the suc- cess of our method are the use of samples rather than pixel statistics and splatting rather than gathering. For this, we introduce a new kernel-splatting architecture that is invariant to input permutations and accepts arbitrary numbers of samples. We show that our approach is robust to severe, heavy-tailed noise in low sample count settings and excels at rendering scenes with distributed effects such as depth-of-field, achieving significantly-reduced error on our extensive tests." }, { "title": "Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings", "url": "/posts/KPCN/", "categories": "Paper review, MC denoising", "tags": "", "date": "2022-04-02 00:00:00 +0900", "snippet": "AbstractRegression-based algorithms have shown to be good at denoising Monte Carlo (MC) renderings by leveraging its inexpensive by-products (e.g., feature buffers).However, when using higher-order models to handle complex cases, these techniques often overfit to noise in the input. For this reason, supervised learning methods have been proposed that train on a large collection of reference examples, but they use explicit filters that limit their denoising ability. To address these problems, we propose a novel, supervisedlearning approach that allows the filtering kernel to be more complex and general by leveraging a deep convolutional neural network (CNN) architecture.In one embodiment of our framework, the CNN directly predicts the final denoised pixel value as a highly non-linear combination of the input features.In a second approach, we introduce a novel, kernel-prediction network which uses the CNN to estimate the local weighting kernels used tocompute each denoised pixel from its neighbors. We train and evaluate our networks on production data and observe improvements over state-of-the artMC denoisers, showing that our methods generalize well to a variety of scenes. We conclude by analyzing various components of our architecture and identify areas of further research in deep learning for MC denoising.IntroductionMonte Carlo denoising보다 더 정확한 image를 생성하기 위해, 과거에 사용하던 REYES-style micropolygon architecture[Cook et al. 1987]에서 physically-based Monte Carlo (MC) path tracing [Kajiya 1986] 으로 많이 넘어오고 있다. 이 방법은 ray tracing을 사용하며, rendering equation의 적분을 쉽게 하기 위해 Monte Carlo integration을 사용한다(\\ref{eq1}).\\[L_o(p,\\omega_o) = \\int_{\\Omega^{+}} L_i(p, \\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)\\mathbf{d}\\omega_i \\approx \\frac{1}{N}\\sum^{N}_{i=1}\\frac{L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)}{p(\\omega_i)} \\label{eq1} \\tag{1}\\]Monte Carlo integration은 unbiased estimator이기 때문에, sample의 개수($N$)이 많다는 전제 하에 ground truth로 수렴하게 된다. 하지만 많은 spp(sample per pixel)을 사용하면 그만큼 시간이 오래 걸리게 되는데, 그래서 적은 spp로 noisy image를 뽑아내고, 여기에 denoising을 통해 clean image를 얻어내는 Monte Carlo denoising 이 연구되고 있다.Previous worksMonte Carlo renderer에서 만들어진 image의 각 pixel 값 $\\mathbf{x}_p \\in\\mathbb{R}^{3+D}$는 pixel당 RGB color을 나타내는 $c_p$, surface normal, depth, albedo, 그리고 이들의 variance정보를 담고 있는 D auxiliary features $\\mathbf{f}_p$ 로 이루어져 있다. MC denoising에서 filtered color $\\hat{c}_p$ 는 pixel $p$의 neighborhood $\\mathcal{N}_p$에 속하는 per-pixel vector들인 $\\mathbf{X}_p$와 CNN network를 이용해서 얻어지며, 이를 ground truth color $\\bar{c}_p$ 와 가깝게 만드는 weight을 찾는 것을 목표로 한다(\\ref{eq2}).\\[\\hat{c}_p = g(\\mathbf{X}_p;\\hat{\\theta}_p), \\quad\\hat{\\theta}_p = \\underset{\\theta}{\\text{argmin }} \\mathcal{l}(\\bar{c}_p, \\hat{c}_p) \\label{eq2} \\tag{2}\\]하지만 이 과정에서 ground truth 값을 얻을 수 없기 때문에, 기존의 MC denoising 방법들은 $\\mathcal{N}_p$에 속하는 다른 값들을 reference로 weight를 정했다(\\ref{eq3}).\\[\\hat{c}_p = {\\hat{\\theta}}_p^\\top \\phi(\\mathbf{x}_p), \\quad \\hat{\\theta}_p = \\underset{\\theta}{\\text{argmin}} \\sum_{q\\in\\mathcal{N}(p)} (\\mathbf{c}_q - {\\theta}_p^\\top \\phi(\\mathbf{x}_q))\\omega({\\mathbf{x}_p, \\mathbf{x}_q}) \\label{eq3} \\tag{3}\\]이때 $\\omega({\\mathbf{x}_p, \\mathbf{x}_q})$ 는 일종의 regression kernel로, noise로 인해 심하게 변한 value들을 무시하게 해준다.점점 더 복잡한 $\\phi$ 를 사용하는 방법들이 시도되었지만, 이는 특정 image에 over-fitting될 가능성이 높았기 때문에 그 한계를 보였다.그래서 Kalanari et al.[2015] 에서는 이를 해결하기 위해 supervised learning을 도입했는데, over-fitting을 막기 위해 전체 dataset을 $N$개의 patch들로 나눠서 그 평균을 이용해 학습을 했다(\\ref{eq4}).\\[\\hat{\\theta} = \\underset{\\theta}{\\text{argmin}} \\frac{1}{N}\\sum^{N}_{i=1}\\mathcal{l}(\\bar{c}_i, g(\\mathbf{X}_i;\\theta)) \\label{eq4} \\tag{4}\\]하지만 이 경우에도 결국에는 고정된 filter을 사용했기 때문에, 표현에 한계가 존재했다. 더 flexible한 g를 사용하기 위해 이 논문에서는 Convolutional Neural Network(CNN)을 도입했다.Deep convolutional denoisingNetwork architecture이 논문에서는 parameter의 개수를 적게 하기 위해서 fully connected layer을 사용하지 않고 오직 CNN만을 사용했다. 이는 over-fitting의 위험을 줄여줄 뿐만 아니라, training, inference speed를 모두 빠르게 해준다.각 layer사이의 activation function에는 ReLU가 사용되었고, 마지막에만 identity function이 사용되었다.Reconstruction methodsFunction g의 output에 따라 direct-prediction(DPCN) 과 kernel-prediction(KPCN) 으로 나뉠 수 있다.먼저 DPCN은 말 그대로 clean image자체를 얻는 것을 목표로 한다. 논문에 따르면 이 방법은 좋은 성능을 보였지만, optimization 과정에서 converge 속도가 엄청 느렸다고 한다.반면에 KPCN은 각 neighborhood에 맞는 특정한 kernel을 얻는 것을 목표로 한다. Kernel의 weight들은 마지막에 softmax function을 통해 normalize되는데, 이는 최종 clean image의 각 pixel의 값이 색의 범위를 벗어나지 않도록 해주고, gradient를 안정화시켜서 converge 속도 향상에 도움을 준다고 한다.Diffuse/Specular decompositionMC denoising을 거친 결과물은 over-blurring되는 경우가 많이 있는데, 이는 image에 존재하는 noise들의 원인과 특성이 다 제각기이기 때문에, denoising하는 데에 있어서 conflict를 일으키기 때문이다. 그래서 이 논문에서는 image를 diffuse, specular components으로 나눠서 이 현상을 줄이고자 했다.Diffuse components는 오차의 범위가 크지 않기 때문에 color preprocessing 없이도 좋은 결과를 보였다고 한다. 실제 실험에서는 noisy albedo 부분을 제거해줘서 irradiance 부분만 CNN에게 전달해줬다(\\ref{eq5}).\\[\\tilde{\\mathbf{c}}_\\text{diffuse} = {\\mathbf{c}}_{\\text{diffuse}} / (\\mathbf{f}_\\text{albedo} + \\epsilon) \\label{eq5} \\tag{5}\\]이때 $\\epsilon$은 0으로 나누는 것을 막기 위한 상수이다.Specular components는 오차의 범위가 컸기 때문에, log scale로 만들어줘서 이를 완화시켰다(\\ref{eq6}).\\[\\tilde{\\mathbf{c}}_\\text{specular} = \\log(1 + \\mathbf{c}_{\\text{specular}}) \\label{eq6} \\tag{6}\\]Clean image로 restoration하는 과정에서는 아래의 역변환을 거쳤다(\\ref{eq7}).\\[\\hat{c} = (\\mathbf{f}_\\text{albedo} + \\epsilon) * \\hat{\\mathbf{c}}_\\text{diffuse} + \\text{exp}(\\hat{\\mathbf{c}}_\\text{specular})-1 \\label{eq7} \\tag{7}\\]Training9개의 convolution layer을 사용했으며, 각 layer은 100개의 channel과 $5\\times5$ 크기의 kernel을 갖고 있다. 마지막 output kernel의 크기는 $21\\times21$ 로, 최종 layer의 channel 수는 $21^2$이 된다. Input data는 $1280\\times1280$ 크기의 image로, diffuse color + variance(4), specular color + variance(4), diffuse color derivative(6), specular color derivative(6), normals + variance(4), normals derivative(6), albedo + variance(4), albedo derivative(6), depth + variance(2), depth derivative(2)로 총 44개의 channel이 있다. 앞서 설명했듯이 diffuse, specular로 나눠서 2개의 model을 학습하기 때문에, 각 model에는 34개의 channel이 input으로 들어가게 된다.ResultRefer to original paper" }, { "title": "iNeRF: Inverting Neural Radiance Fields for Pose Estimation", "url": "/posts/iNeRF/", "categories": "Paper review, NVS", "tags": "", "date": "2022-02-18 00:00:00 +0900", "snippet": "AbstractWe present iNeRF, a framework that performsmesh-free pose estimation by “inverting” a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effectivefor the task of view synthesis — synthesizing photorealisticnovel views of real-world scenes or objects. In this work,we investigate whether we can apply analysis-by-synthesis viaNeRF for mesh-free, RGB-only 6DoF pose estimation – givenan image, find the translation and rotation of a camera relativeto a 3D object or scene. Our method assumes that no objectmesh models are available during either training or test time.Starting from an initial pose estimate, we use gradient descentto minimize the residual between pixels rendered from a NeRFand pixels in an observed image. In our experiments, we firststudy 1) how to sample rays during pose refinement for iNeRFto collect informative gradients and 2) how different batch sizesof rays affect iNeRF on a synthetic dataset. We then showthat for complex real-world scenes from the LLFF dataset,iNeRF can improve NeRF by estimating the camera poses ofnovel images and using these images as additional trainingdata for NeRF. Finally, we show iNeRF can perform category levelobject pose estimation, including object instances not seenduring training, with RGB images by inverting a NeRF modelinferred from a single view.Introduction6 degree of freedom(6DoF) pose estimation은 다양한 분야에서 폭넓게 쓰인다. 최근에는 differentiable rendering을 기반으로 한 pose estimation이 뛰어난 성능을 보였지만, 이는 high-quality watertight 3D model을 필요로 하기 때문에 학습이 어렵고, 일반적인 object에만 적용될 수 있다는 단점이 있다. 본 논문(iNerF)에서는 novel view synthesis(NVS) 분야에서 뛰어난 성능을 보인 NeRF를 이용해 pose estimation을 하고자 한다. iNeRF는 특정한 scene의 image, pose, 그리고 NeRF를 통해 학습된 그 scene의 3D model을 input으로 받는데, image와 NeRF model을 통해 generate된 image를 비교하며 정확한 pose를 찾는 ‘analysis-by-synthesis’방법을 사용한다.BackgroundNeRF–는 NeRF를 기반으로 만들어졌기 때문에, NeRF에 대한 내용을 확인하는 것을 추천한다.iNeRF modeliNeRF는 그 이름에서 알 수 있듯이 학습된 NeRF model을 “invert”해서 특정 image의 pose를 알아내고자 한다. 즉, NeRF model의 weight $\\Theta$와 image $I$가 주어졌을 때, camera pose $T$를 얻고자 한다(\\ref{eq1}).\\[\\hat{T} = \\underset{T \\in \\text{SE}(3)}{\\text{argmin }} \\mathcal{L}(T|I, \\Theta) \\label{eq1} \\tag{1}\\]이때 사용한 loss function은 NeRF와 동일하다. 하지만 이는 6DoF space에서 convex하지 않고, 전체 NeRF rendering이 computationally expensive 하기 때문에 약간의 수정이 필요하다.iNeRF의 전체 framework.Gradient-Based SE(3) OptimizationEquation \\ref{eq1}에 gradient based optimization을 적용할 때, estimated pose $\\hat{T}_i$는 여전히 SE(3) manifold안에 놓여 있어야 한다. 그렇기 때문에 $\\hat{T}_i$를 exponential coordinate로 parameterize해서 학습을 한다(\\ref{eq2}).\\[\\begin{align} \\hat{T}_i = &amp;e^{[\\mathcal{S}_i]\\theta_i}\\hat{T}_0 \\\\ \\text{where} \\quad &amp;e^{[\\mathcal{S}_i]\\theta_i} = \\left[ \\begin{matrix} e^{[\\omega]\\theta} &amp; K(\\mathcal{S}, \\theta) \\\\ 0 &amp; 1 \\end{matrix} \\right]\\end{align} \\label{eq2} \\tag{2}\\]이때 $\\mathcal{S} = [\\omega, \\nu]^T$ 는 skew axis, $\\theta$는 magnitude, $[\\omega]$는 $\\omega$의 $3\\times3$ skew-symmetric matrix, $K(\\mathcal{S}, \\theta) = (I\\theta+(1-\\cos\\theta)[\\omega] + (\\theta - \\sin\\theta)[\\omega]^2)\\nu$ 이다. 그러면 equation \\ref{eq1}은 다음과 같이 다시 쓸 수 있다(\\ref{eq3}).\\[\\hat{\\mathcal{S}\\theta} = \\underset{\\mathcal{S}\\theta \\in \\mathbb{R}^6}{\\text{argmin }}\\mathcal{L}(e^{[\\mathcal{S}]\\theta}T_0|I,\\Theta) \\label{eq3} \\tag{3}\\]Sampling raysImage에 크기에 해당하는 모든 pixel에 ray를 쏴서 volumetric rendering을 하고, 여기에 back propagation을 진행하는 것은 너무 computationally heavy 하다. 그렇기 때문에 iNeRF에서는 특정한 sampling strategy를 사용해서 전체 ray중 일부만 선택해 학습을 진행했다. 실제로 $640\\times 480$크기의 image에서 2048개의 ray만 사용을 해서 학습 속도와 GPU memory 사용량을 크게 줄였다. 사용한 sampling strategy는 다음과 같다. Random sampling : 말 그대로 무작위로 $M$개의 ray를 골라서 사용한다. 하지만 이런 식으로 만들어진 대부분의 sample은 pose estimation에는 도움이 되지 않는 flat, textureless region에 해당했다고 한다. Interest point sampling : Image alignment에서 하는 것과 비슷하게, interest point detector을 이용해 interest point를 찾아서 해당하는 ray를 사용한다. 혹시 ray에 수가 모자르다면, 남은 ray들 가운데에서 random sampling을 통해 수를 맞춰준다. 하지만 오직 interest point만 고려하기 때문에 local minima에 빠지기 쉽다. Interest region sampling : Interest point sampling이 local minima에 빠지는 형상을 완화하기 위해, point가 아니라 “Interest Region”을 찾아내어 거기에서 sampling을 해준다. Interest point detector에서 interest point를 찾아내면, 거기에 $I$번의 $5\\times 5$ morphological dilation을 적용해 영역을 넓혀줬다. 사용된 다양한 sampling strategy. Random 같은 경우에는 아무런 정보가 없는 공간에 많은 sample들이 위치하고 있는 것을 확인할 수 있다.Self-Supervising NeRF with iNeRFiNeRF는 pose estimation 뿐만 아니라 NeRF의 성능을 올리는 데에도 기여할 수 있다. NeRF model을 학습시킨 뒤, iNeRF를 이용해 train image들 중 pose를 모르는 image들의 pose를 알아내고, 다시 NeRF를 학습시키는 일종의 semi-supervised learning을 할 수 있다.Training detailsSynthetic dataset, LLFF dataset, ShapeNet-SRN Cars, Sim2Real Cars의 정말 많은 dataset상에서 실험을 진행했다. 더 자세한 사항은 iNeRF 논문을 참조하길 바란다.ResultsPose estimation이 성공적으로 이루어지고, self-supervising NeRF에서도 가능성을 보였다. 더 자세한 사항은 iNeRF 논문을 참조하길 바란다.복잡한 model을 사용하지 않고도 성공적으로 pose estimation을 해낸다.전체 dataset을 사용한 것과 일부분만 사용한것, 일부분 + iNeRF을 사용한 것에 대한 비교이다. iNeRF를 사용하면 확실히 일부만 사용한 것보다는 좋은 결과가 나오는 것을 알 수 있다.ConclusionWe have presented iNeRF, a framework for mesh-free,RGB-only pose estimation that works by inverting a NeRFmodel. We have demonstrated that iNeRF is able to performaccurate pose estimation using gradient-based optimization.We have thoroughly investigated how to best construct minibatchesof sampled rays for iNeRF and have demonstrated itsperformance on both synthetic and real datasets. Lastly, wehave shown how iNeRF can perform category-level objectpose estimation and track pose for novel object instances with an image conditioned generative NeRF model." }, { "title": "Moving in a 360 World : Synthesizing Panoramic Parallaxes from a Single Panorama(OmniNeRF)", "url": "/posts/OmniNeRF/", "categories": "Paper review, NVS", "tags": "", "date": "2022-02-10 00:00:00 +0900", "snippet": "Introduction Synthesizing novel views provides immersive 3D experiences, but the needed cost of computing power is very high in both time and capacity. While many techniques are proposed to synthesize novel views by taking the perspective image(s) as the input, prior work rarely considers the panorama image as a single source for modeling and rendering. Perspective images can be obtained easily. But to construct the whole scene, very dense samples are required. Panorama images have more data of a scene in a single image, so using less images will be achievable. In this paper, they propose OmniNeRF, which uses a single panorama image with depth value to synthesize a novel view of the whole scene. Also in reconstruction, training with merely a single image is apparently not sufficient. So they proposed a method to create images form different views from a single image.Related work This paper is based on NeRFMethodsInput data They use a single panorama image(omnidirectional image), with depth value. (RGB-D panorama image)Generating training samples Generating novel views using only one image is very challenging. In previous works, multi-view data with known camera parameters were mostly used. This paper adopted a similar process, by simulating multi-view images from the single RGB-D panorama image. All pixels are mapped to a uniform sphere by their 2D coordinates. The coordinate center will be the current camera position, namely the ray origin. The ray direction means a unit vector from the center to the sphere. A novel panoramic view can be obtained by moving the camera to a new position and sampling on a new unit sphere. To get the actual training image, these points have to projected back to the original pose. Visibility In transforming the image to a different pose, a ray from a translated camera view has a chance to “see through” the sparse points of an obstacle and reach a point that was visible to the original view(but not to the current view). To handle this, median filter is applied to the depth map of the translated view, and pixels with depth value larger then the local minimum depth multiplied by a tolerance ratio are filtered out. Regressing with gradient Since the given training data are not dense enough to cover all areas of the scene, rendering images at new positions forces the model to predict some regions that the model has never seen before. Use additional loss term to make the model learn to interpolate from one pixel to another according to ray origin and direction information." }, { "title": "각종 코드 정리", "url": "/posts/fn/", "categories": "coding", "tags": "", "date": "2022-02-10 00:00:00 +0900", "snippet": "GIT ### git config 관련 git config --global user.name &lt;user_name&gt; # 유저 이름 등록 git config --global user.email &lt;user_email&gt; # 유저 email 등록 git config --list # git config 전체 list 확인 git config credential.helper store # git 관련해서 id, password 기억하기 ### branch 관련 git pull origin &lt;branch&gt; # 특정 branch에서 pull git clone -b &lt;branch&gt; --single-branch &lt;repo url&gt; # 특정 branch에서 clone git branch &lt;branch&gt; # 새로운 branch 생성 git checkout &lt;branch&gt; # 특정 branch로 이동 ### 기타 rm -r .git # git 연결 끊기Tmux터미널에서 코드를 실행할 때, 때로는 한번 실행하고 끝나는 것이 아니라 계속 실행시켜 놔야 하는 경우가 있다(예를 들면 Deep learning). 하지만 실수로 터미널을 닫거나, 컴퓨터가 꺼지거나, 서버에서 연결이 끊기는 등 여러 불상사로 인해 코드의 실행이 멈추는 경우가 비일비재하다. Tmux는 background에서 터미널 실행을 계속 해주는 프로그램으로, 이러한 경우를 방지할 수 있다. Tmux를 사용하면 여러 터미널 session을 만들 수 있고, session과의 연결을 끊어도 background에서 그 session이 계속 돌아가게 된다. ### 설치 sudo apt-get install tmux ### session 관련 tmux new -s &lt;name&gt; # 새로운 session 생성 tmux kill-session -t &lt;name&gt; # session 지우기 tmux attach -t &lt;name&gt; # session에 연결 'ctrl + b -&gt; d' # session 연결 끊기 ### 기타 'ctrl + b -&gt; [' # tmux session내에서 스크롤 가능하게 하기. esc를 누르면 다시 꺼진다. PyenvPyenv는 pip 기반의 python package를 관리해주는 tool로, 여러 python 버전을 설치해서 각자 적용시킬 수 있다. Conda와 비슷한 역할을 하지만, conda가 조금 무겁고 비효율적이라는 것을 어디서 주워들어서 pyenv를 사용한다. ### 설치 sudo apt install libssl-dev build-essential libffi-dev libbz2-dev zlib1g-dev libreadline-dev libsqlite3-dev liblzma-dev curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash ### 기본 명령어 pyenv versions # 설치된 python version, virtualenvs 를 확인한다 pyenv install --list # 설치 가능한 python 버전 확인 pyenv install &lt;version&gt; # 원하는 버전 설치 pyenv uninstall &lt;version&gt; # 특정 버전의 python 제거 pyenv virtualenv &lt;version&gt; &lt;name&gt; # 특정 버전의 python을 사용해서 특정한 virtualenv를 만든다 pyenv activate &lt;name&gt; # virtualenv에 연결 pyenv deactivate # virtualenv 연결 해제Pyenv를 설치 후, .bashrc나 .zshrc에 아래 내용을 추가해 줘야 한다.export PYENV_ROOT=\"$HOME/.pyenv\"export PATH=\"$PYENV_ROOT/bin:$PATH\"eval \"$(pyenv init -)\"export PYENV_VIRTUALENV_DISABLE_PROMPT=1Pyenv virtualenv를 제거하고 싶을 때, 단순히 uninstall하면 그 log가 남아서, 같은 이름의 새로운 virutalenv를 만들고 싶어도 log에 있는 package들이 그대로 다시 설치된다. 그래서 직접 pyenv 폴더가 있는 경로로 가서 삭제해줘야 한다. ‘~/.pyenv/versions’으로 이동하면 pyenv virtualenv들과 설치된 python version이 있는데, 아래 항목들을 삭제해 줘야 한다. pyenv virtualenv 폴더(~/.pyenv/versions에 존재함) virtualenv에 해당하는 python version 내부에 존재하는 log(~/.pyenv/versions/[python_version]에 존재함)PythonParserParser은 여러 argument를 사용할 때 이를 parsing을 통해 한꺼번에 함수로 전달해주는 역할을 한다. ### setup from argparse import ArgumentParser def init_parser(parser): parser.add_argument(\"name\", type=t, default=d) ... parser = ArgumentParser() init_parser(parser) args = parser.parser_args() ### 특정 argument 접근 arg = args[\"name\"] ### type의 종류 int, float, str action = \"store_true\" # bool nargs = \"+\", type=int, default = [1] # listTorchtorch.expand Tensor을 반복할 때 사용 a = torch.rand(1,2,3) b = a.expand(4,-1,-1) # 첫 번째 차원을 4로 확장. -1은 원래 차원으로 그대로 남기라는 의미이다. print(b.shape) # [4,2,3]torch.unsqueeze Tensor의 차원 확장이 필요한 경우 사용 a = torch.rand(2,2) a = a.unsqueeze(0) # [1,2,2] a = a.unsqueeze(2) # [1,2,1,2] a = a.unsqueeze(-1) # [1,2,1,2,1]register_buffer Network에 포함된 layer의 일종 Optimizer로 update 되지 않음 GPU에서 동작 가능 state_dict로 확인 가능 중간에 학습시키지 않을 layer나 parameter을 넣을 때 사용한다. self.register_buffer('arg_name', torch.zeros(1)) # self.arg_name에 torch.zeros(1)이 할당된다. a[…] Tensor에 접근 할 때, 앞에 있는 차원(혹은 뒤에 있는 차원)을 전부 무시한다. ex : a[…, 1] -&gt; 마지막 차원의 [1] 위치에 해당하는 tensor" }, { "title": "Unsupervised Discovery of Object Radiance Fields", "url": "/posts/uORF/", "categories": "Paper review, NVS", "tags": "", "date": "2022-02-03 00:00:00 +0900", "snippet": "Introduction Object-centric representation is a constant topic of interest in computer vision and machine learning. Such representation should bear three characteristics: Should be learned in a unsupervised manner. Should explain the image formation process. Should be 3D-aware, capturing geometric and physical object properties. Until now, there is no work satisfying all these characteristics. In this paper, they propose Object Radiance fields(uORF), which infers a set of object-centric latent codes through a slot-based encoder, and use them to represent a 3D scene as a composition of radiance fields. During training, such radiance fields are neurally rendered in multiple views, with reconstruction losses in pixel space as supervision. During testing, uORF infers the set of object radiance fields from an single image.MethodsConvolutional feature extraction Uses a convolutional net to extract features for the slot attention module. Represents foreground object position and pose in the viewer coordinate system, in order to help learning the 3D object position and generalization. So, feeds pixel coordinates and viewer-space ray direction as additional input channels to the encoder.Background-aware slot attention Adopt Slot Attention module to produce a set of permutation-invariant latent codes. Since the geometry and appearance of the background are usually highly different from those of foreground objects, explicitly separate the foreground objects and background. To achieve this, make a single slot that lie in a different latent space from the other slots, for background features.Object-centric Encoding Using convolutional feature extraction and background-aware slot attention, infers latent object-centric representations from a single image. Given $N$ input features with dimension $D$, the slots are initialized by sampling from two learnable Gaussians. $slot^b$ denotes a single slot for background, and $slots^f$ denotes the slots for foreground objects.\\[slot^b \\sim \\mathcal{N}^b(\\mu^b, diag(\\sigma^b)) \\in \\mathbf{R}^{1\\times D}, \\, slots^f \\sim \\mathcal{N}^f(\\mu^f, diag(\\sigma^f)) \\in \\mathbf{R}^{K \\times D}\\] The rest are similar to Slot Attention module. Compositional Neural Rendering Use a Conditional NeRF $g(\\cdot | z)$ that acts like an implicit decoder for each object. $z$ is the generated latent codes. To compose individual objects and background into holistic scene, use a scene mixture model that uses density-weighted mean to combine all components.\\[\\bar{\\sigma} = \\sum_{i=0}^K\\omega_i\\sigma_i, \\, \\bar{\\mathbf{c}} = \\sum_{i=0}^K\\omega_i\\mathbf{c}_i, \\, \\omega_i = \\sigma_i/\\sum_{j=0}^K\\sigma_i\\] Loss functions Use reconstruction loss, perceptual loss, and adversarial loss. Since 3D radiance fields are estimated from a single view, there can be uncertainties about the appearance from other views. Therefore, perceptual loss, which is tolerant to mild appearance changes, is used. There can also exist multi-model distribution. Therefore, adversarial loss which can deal with multi-model distributions is used. Reconstruction loss : $\\mathcal{L}_{recon} = ||\\mathbf{I} - \\hat{\\mathbf{I}}||^2$ where $\\mathbf{I}$ and $\\hat{\\mathbf{I}}$ denote the ground truth image and rendered image, respectively. Perceptual loss : $\\mathcal{L}_{percept} = ||p(\\mathbf{I}) - p(\\hat{\\mathbf{I}})||^2$ where $p$ is a deep feature extractor. Adversarial loss : $\\mathcal{L}_{adv} = \\mathbb{E}[f(D(\\hat{\\mathbf{I}}))] + \\mathbb{E}[f(-D(\\mathbf{I})) + \\lambda_R|| \\nabla D(\\mathbf{I})||^2]$, where $f(t) = -log(1+exp(-t))$Coarse-to-fine Progressive Training Training compositional NeRF requires immense computational cost. To allow training on a higher resolution, coarse-to-fine progressive training is used. In a coarse training stage, uORF is trained on the bilinearly downsampled images to a base resolution. In the following fine training stage, the model is refined by training on patches randomly cropped from images of the higher target resolution.The overall processResults Refer to original paper" }, { "title": "Object-Centric Learning with Slot Attention", "url": "/posts/OCLSA/", "categories": "Paper review, Attention", "tags": "", "date": "2022-01-30 00:00:00 +0900", "snippet": "Introduction Object-centric representations have the potential to improve sample efficiency and generalization of machine learning algorithms across a range of application domains. However, obtaining object-centric representations from raw perceptual input, such as an image or a video, is challenging and often requires either supervision or task-specific architectures. To overcome this challenge, this paper introduce the Slot Attention module, a differentiable interface between perceptual representations (e.g., the output of a CNN) and a set of variables called slots.MethodsSlot Attention Module Maps from a set of $N$ input feature vectors to a set of $K$ output vectors called ‘slots’. Iterative attention mechanism is used to map the inputs to the slots: slots are initialized at random and refined at each iteration. At each iteration, slots compete for explaining pars via softmax-based attention mechanism and update there representation using a recurrent update function. Slot Attention uses a dot-product attention with coefficients normalized over the slots. Overall algorithm: The figure below shows how the Slot Attention is trained. There are 4 slots, for each objects and the background. In each iterations, the most dominant object is mapped to the slot. Object Discovery Unsupervised The order of which object will go into which slot do not matter. Encoder : CNN backbone augmented with positional embeddings + slot attention module Decoder : Using spatial broadcast decoder, slot representations are broadcasted onto a 2D grid and augmented with position embeddings.Set Prediction For a set of $K$ elements, there can be $K!$ possible equivalent representations. Encoder : Similar to object discovery Classifier : For each slots, apply MLP with parameters shared between slots. Next, match them using the Hungarian algorithm.Overall PipelineResults Refer to original paper" }, { "title": "NeRF−−: Neural Radiance Fields Without Known Camera Parameters", "url": "/posts/NeRFmm/", "categories": "Paper review, NVS", "tags": "", "date": "2022-01-23 00:00:00 +0900", "snippet": "AbstractThis paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses or intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF−−, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, includingboth intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFFbenchmark, our model achieves novel view synthesis results on par with the baseline trained with COLMAP pre-computed camera parameters. Wealso conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAPfails, our model still produces robust results.IntroductionNeRF는 NVS에 뛰어난 성능을 보이지만, input image들의 camera pose, intrinsic을 알아야 model을 학습할 수 있다는 단점이 있다. Synthetic data에서는 이것이 큰 문제가 되지 않지만, 실제 카메라로 찍힌 real data에서는 정확한 pose와 intrinsic을 알아내는 것이 어렵다. NeRF에서는 structure-from-motion에 기반한 SFM colmap을 사용해서 이를 구했지만, 이는 100% 정확한 값은 아니었다. 본 논문(NeRF–)에서는 camera pose와 intrinsic을 모르는 상태에서도 학습 가능한 NeRF model을 제시했으며, camera parameter은 학습 과정에서 joint optimization을 통해 얻어진다고 한다.대략적인 학습 과정. Camera extrinsics(pose)와 intrinsics는 NeRF model과 함께 학습된다.BackgroundNeRF–는 NeRF를 기반으로 만들어졌기 때문에, NeRF에 대한 내용을 확인하는 것을 추천한다.NeRF– modelNeRF–는 camera parameters들도 학습을 시켜야 하기 때문에, 다음과 같은 loss function을 사용한다(\\ref{eq1}).\\[\\Theta^\\star, \\Pi^\\star = \\underset{\\Theta, \\Pi}{\\text{argmin}}\\,\\mathcal{L}(\\hat{\\mathcal{I}}, \\hat{\\Pi}|\\mathcal{I}) \\label{eq1} \\tag{1}\\]이때 $\\mathcal{I}$는 RGB image, $\\Theta$는 NeRF model의 weight, $\\Pi$는 camera parameters를 의미한다.Camera parameters먼저 camera intrinsics는 다음과 같이 정의된다(\\ref{eq2}).\\[K = \\begin{pmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\label{eq2} \\tag{2}\\]이때 $f_x$, $f_y$는 focal length, $c_x$, $c_y$는 principle point이다. 학습 과정에서 $c_x$, $c_y$는 각각 $W/2$, $H/2$로 고정되었다.Camera extrinsics(pose)는 다음과 같이 정의된다(\\ref{eq3}).\\[\\mathbf{R} = \\mathbf{I} + \\frac{sin(\\alpha)}{\\alpha} \\mathbf{\\phi}^{\\wedge} + \\frac{1 - cos(\\alpha)}{\\alpha ^2} (\\mathbf{\\phi}^{\\wedge})^2 \\label{eq3}\\tag{3}\\]이는 axis-angle representation으로, $\\mathbf{\\phi} := \\alpha\\mathbf{\\omega}$, $\\mathbf{\\phi} \\in \\mathbb{R}^3$는 rotation axis $\\mathbf{\\omega}$와 rotation angle $\\alpha$의 곱이다. Skew operator $(\\cdot)^\\wedge$ 는 $\\mathbf{\\phi}$를 skew matrix으로 바꿔준다(\\ref{eq4}).\\[\\phi^{\\wedge} = \\begin{pmatrix} \\phi_0 \\\\ \\phi_1 \\\\ \\phi_2 \\end{pmatrix}^{\\wedge} = \\begin{pmatrix} 0 &amp; -\\phi_2 &amp; \\phi_1 \\\\ \\phi_2 &amp; 0 &amp; -\\phi_0 \\\\ -\\phi_1 &amp; \\phi_0 &amp; 0 \\end{pmatrix} \\label{eq4} \\tag{4}\\]Joint optimization of NeRF and Camera ParametersNeRF와 마찬가지로 각 training image $I_i$에 대해, 무작위로 $M$개의 pixel $(p_{i,m})_ {m=1}^M$을 선택해서, 각 pixel에 ray $\\hat{\\mathbf{r}}_{i,m}(h)$을 쏜다. Ray는 다음과 같이 정의된다(\\ref{eq5}).\\[\\begin{align} &amp;\\hat{\\mathbf{r}}_{i,m}(h) = \\hat{\\mathbf{t}}_i + h\\hat{\\mathbf{d}}_{i,m} \\\\ &amp;\\hat{\\mathbf{d}}_{i,m} = \\hat{\\mathbf{R}}_i\\begin{pmatrix} (u-W/2)/\\hat{f}_x \\\\ -(v-H/2)/\\hat{f}_y \\\\ -1 \\end{pmatrix}\\end{align} \\label{eq5} \\tag{5}\\]이후 과정은 NeRF와 동일하며, NeRF model과 함께 $\\hat{pi}_i = (\\hat{f}_x, \\hat{f}_y, \\hat{\\mathbf{\\phi}}_i, \\hat{\\mathbf{t}}_i)$를 학습한다.Refinement of Camera ParametersNeRF–의 저자들에 따르면 camera parameter들은 sub-optimal에 빠질 가능성이 크다고 한다. 그렇기 때문에 initialization이 중요한데, NeRF– model을 조금 학습시킨 뒤 NeRF부분의 parameter은 다시 초기화하고, camera parameter부분은 그대로 유지하는 방법의 refinement를 사용한다고 한다.Overall frameworkNeRF–의 전체 framework.Training detailsNeRF와 비슷한 dataset을 사용했으며, evaluation metric또한 NeRF와 동일한 PSNR, SSIM, LPIPS를 사용했다. 더 자세한 사항은 NeRF– paper을 참조하길 바란다.Result먼저 SFM colmap이 잘 작동하는 scene에 대해서는 거의 유사한 결과를 보였다. 하지만 colmap에서는 pose와 intrinsic을 알아내지 못해서 NeRF에서는 NVS에 실패했지만, NeRF–에서는 효과적으로 이를 알아내어 NVS에 성공한 경우가 있었다. 더 자세한 사항은 NeRF– paper을 참조하길 바란다.SFM colmap이 잘 작동하는 scene에서는 거의 유사한 결과를 보인다.Rotation-dominant sequence에서는 NeRF에서 NVS에 실패한 반면, NeRF–에서는 성공한 것을 확인할 수 있다.ConclusionIn this work, we present an end-to-end NeRF-based pipeline, calledNeRF−−, for novel view synthesis from sparse input views, whichdoes not require any information about the camera parameters fortraining. Specifically, our model jointly optimise the camera parametersfor each input image while simultaneously training the NeRFmodel. This eliminates the need of pre-computing the camera parametersusing potentially erroneous SfM methods (e.g. COLMAP) andstill achieves comparable view synthesis results as the COLMAPbasedNeRF baseline.We present extensive experimental results anddemonstrate the effectiveness of this joint optimisation frameworkunder different camera trajectory patterns, even when the baselineCOLMAP fails to estimate the camera parameters. Despite its currentlimitations discussed above, our proposed joint optimisationpipeline has demonstrated promising results on this highly challengingtask, which presents a step forward towards novel viewsynthesis on more general scenes with an end-to-end approach." }, { "title": "Github Blog에 코드 넣기", "url": "/posts/post2/", "categories": "blog", "tags": "", "date": "2022-01-15 00:00:00 +0900", "snippet": "Code block 사용깃허브 블로그에 코드를 넣을 때 code block이라는 것을 활용할 수 있다. ```py def sum(a, b): return a+b ```이런 식으로 입력을 하면 실제로 아래와 같은 모습으로 변환된다. def sum(a, b): return a+bGithub Gist 사용Github Gist는 Github과 다르게 원하는 code snippet만 업로드 할 수 있다. 업로드를 완료하면 링크를 복사할 수 있는데, 이를 그대로 붙여넣으면 code block을 아래와 같이 삽입할 수 있다. " }, { "title": "NERF++: ANALYZING AND IMPROVING NEURAL RADIANCE FIELDS", "url": "/posts/NeRFpp/", "categories": "Paper review, NVS", "tags": "", "date": "2022-01-13 00:00:00 +0900", "snippet": "AbstractNeural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi- layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method im- proves view synthesis fidelity in this challenging scenario. Code is available at this link.IntroductionNeRF는 NVS 분야에서 뛰어난 성능을 보였다. 본 논문에서는 NeRF model의 성능이 좋은 이유에 대한 분석과, outdoor scene같은 unbounded scene에서 NeRF의 성능을 올리는 NeRF++ model을 제시했다.Shape-Radiance AmbiguityNeRF는 view-dependent하게 scene을 generate하기 때문에, 3D shape과 radiance사이의 ambiguity가 만족되지 않으면 degenerate solution이 도출될 가능성이 크다. 즉, input된 training data의 정확한 shape를 학습하지 못한다면, 학습 과정에서는 좋은 성능을 보일 수 있지만 novel view에서의 image는 이상하게 나올 수 있다.예를 들면 서로 다른 위치에서 같은 point를 바라보고 있는 2개의 ray를 생각해보자. 이 2개의 ray에서 관측되는 color값은 같은 point를 보고 있지만, object의 surface geometry 때문에 다른 값을 가진다. 하지만 이 두 ray는 viewing direction이 다르기 때문에, 색이 다른 것이 geometry가 달라서가 아니라 viewing direction이 다르기 때문에서라고 학습할 가능성이 있다. 이렇게 되면 전체 model이 incorrect shape을 기준으로 학습되며, 당연하게 novel view에서의 image는 이상하게 나오는 것이다. 잘못된 shape $\\hat{S}$에 맞게 학습되면, 실제로 같은 곳을 바라보고 있는 $C_0$와 $C_1$이 다른 곳을 바라보고 있는 것을 알 수 있다. Unit sphere로 shape를 고정하고 학습한 결과, training에서는 좋은 결과를 보이지만 testing에서 novel view에 대해 이상한 결과를 보인다.굳이 shape를 unit sphere로 고정해놓지 않더라도, 학습을 진행하는 과정에서 NeRF가 올바른 shape를 알아내지 못하고, 이상한 shape에 맞게 학습 될 가능성이 충분히 존재한다. 하지만 실제 NeRF를 학습하면 이러한 degenerate solution이 발생하지 않는다. NeRF++의 저자들은 이를 다음 2가지 이유로 설명한다. 잘못된 shape으로 학습이 되면 그만큼 color가 high-dimensional하게 표현될 가능성이 크다. 왜냐하면 서로 같은 지점을 바라보는 2개의 viewpoint를 생각해 봤을 때, 옳은 shape이라면 그냥 1개의 값만 predict하면 되지만, 잘못된 shape이라면 서로 다른 지점을 바라보는 것으로 간주되어 그만큼 더 복잡한 prediction이 이루어져야 한다. NeRF에서 사용하는 MLP는 그러한 high-dimensional한 prediction을 하기에 더 어렵기 때문에, 이러한 경우가 잘 발생하지 않는다. NeRF의 MLP 구조를 보면 position $\\mathbf{x}$보다 direction $\\mathbf{d}$가 MLP layer에 더 늦게 input되는 것을 알 수 있다. $\\mathbf{d}$는 그만큼 학습에 큰 영향력을 행사하기 힘들기 때문에, 이로 인한 잘못된 prediction도 잘 발생하지 않는다. NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공되었다.2번째 이유는 실제로 아주 큰 영향을 미치는데, NeRF의 MLP와 다르게 $\\mathbf{d}$가 $\\mathbf{x}$와 함께 처음부터 input되는 vanilla MLP의 경우 model의 성능이 현저하게 떨어졌다. 일반적인 MLP를 사용했을 때 성능이 현저하게 떨어진 것을 확인할 수 있다.Handling unbounded sceneNeRF에서는 ray를 따라 volume rendering을 해서 최종 color를 구하는데, 이 식은 다음과 같다(\\ref{eq1}).\\[C(\\mathbf{r}) = \\int_{t_n}^{t_f}T(t)\\sigma(\\mathbf{r}(t))\\mathbf{c}(\\mathbf{r}(t), \\mathbf{d})dt, \\text{ where } T(t) = \\exp\\left( -\\int_{t_n}^t\\sigma(\\mathbf{r}(s))ds \\right) \\label{eq1} \\tag{1}\\]이 적분은 ray의 near bound $t_n$과 far bound $t_f$사이에서 sampling을 통해 얻은 sample들을 더하는 방식으로 discrete하게 바껴서 진행된다. 이는 $t_n$과 $t_f$의 차이가 크지 않은 bounded scene에서는 잘 먹히지만, $t_f$가 infinity인 unbounded scene의 경우에는 sampling이 너무 sparse하게 일어나서 좋은 결과가 나오지 않는다. Bounded scene의 경우 dense한 sample들을 얻을 수 있다. Unbounded scene에서는 sample들이 너무 sparse하다.NeRF++에서는 이를 해결하기 위해 Inverted sphere parametrization을 제시한다. NeRF와 비슷하게 이 역시 2개의 MLP model을 사용하는데, 전체 scene을 foreground과 background으로 나눠서 각각 학습시킨다. Foreground과 background는 input image들의 camera를 모두 감싸는 sphere를 경계로 나누어진다. Camera(초록색)를 모두 감싸는 sphere(빨간색)을 기준으로 foreground(sphere 안쪽)과 background(sphere 바깥쪽)이 나눠진다.ForegroundForeground는 bounded scene이기 때문에, NeRF과 같은 방식으로 sampling이 이루어진다.BackgroundBackground는 아직도 unbounded scene이기 때문에, inverted sphere parametrization을 적용해준다. Background에 속한 3D point는 다음과 같이 표현될 수 있다(\\ref{eq2}).\\[\\mathbf{x} = (x,y,z), r(\\mathbf{x}) = \\sqrt{x^2+y^2+z^2} \\quad \\rightarrow \\quad \\mathbf{x}^\\prime = (x/r, y/r, z/r, 1/r) = (x^\\prime, y^\\prime, z^\\prime, 1/r) \\label{eq2} \\tag{2}\\]즉, 멀리 있는 point를 경계가 되는 sphere에 project 시킨 것으로 생각할 수 있다. 이때 sampling은 $1/r$을 기준으로 일어나게 되는데, $r &gt; 1$이기 때문에 $0 &lt; 1/r &lt; 1$을 만족해 sparse sample들을 dense하게 만드는 효과가 있다. 이때 사실 $(x^\\prime, y^\\prime, z^\\prime)$은 실제 projection과는 다른 점이지만, $\\mathbf{x}$가 멀리 떨어진 점이기 때문에 실제 projection과 같다고 근사할 수 있다. $(x^\\prime, y^\\prime, z^\\prime)$는 실제 project된 point $a$와는 다르지만, $a$에 비해 계산하는 것이 훨씬 쉽고, $p$가 멀리 있기 때문에 같다고 근사할 수 있기 때문에 사용한다.Volumetric rendering of NeRF++결과적으로 NeRF++의 volumetric rendering은 다음과 같이 다시 쓸 수 있다(\\ref{eq3}).\\[\\begin{align} \\mathbf{C}(\\mathbf{r}) = &amp;\\underset{\\text{foreground}}{\\underbrace{\\int_{t=0}^{t^\\prime}\\sigma(\\mathbf{o}+t\\mathbf{d})\\cdot \\mathbf{c}(\\mathbf{o} + t\\mathbf{d}, \\mathbf{d})\\cdot \\exp \\left(-\\int_{s=0}^t\\sigma(\\mathbf{o}+s\\mathbf{d})ds \\right)dt}}\\\\ &amp;\\underset{\\text{background}}{\\underbrace{+ \\exp \\left(-\\int_{s=0}^{t^\\prime}\\sigma(\\mathbf{o}+s\\mathbf{d})ds \\right)\\cdot\\int_{t=t^\\prime}^{\\infty}\\sigma(\\mathbf{o}+t\\mathbf{d})\\cdot \\mathbf{c}(\\mathbf{o} + t\\mathbf{d}, \\mathbf{d})\\cdot \\exp \\left(-\\int_{s=t^\\prime}^t\\sigma(\\mathbf{o}+s\\mathbf{d})ds \\right)dt}} \\label{eq3}\\tag{3}\\end{align}\\]Training detailsNeRF와 거의 동일하다. 자세한 사항은 NeRF++ 논문을 참조하길 바란다.ResultsNeRF보다 더 발전된 결과를 보였다. 자세한 사항은 NeRF++ 논문을 참조하길 바란다. NeRF++의 결과. NeRF보다 더 발전된 것을 확인할 수 있다.ConclusionNeRF++ improves the parameterization of unbounded scenes in which both the foreground and the background need to be faithfully represented for photorealism. However, there remain a number of open challenges. First, the training and testing of NeRF and NeRF++ on a single large-scale scene is quite time-consuming and memory-intensive. Training NeRF++ on a node with 4 RTX 2080 Ti GPUs takes ∼24 hours. Rendering a single 1280x720 image on one such GPU takes ∼30 seconds at test time. Liu et al. (2020) have sped up the inference, but rendering is still far from real-time. Second, small camera calibration errors may impede photorealistic synthesis. Robust loss functions, such as the contextual loss (Mechrez et al., 2018), could be applied. Third, photometric effects such as auto-exposure and vignetting can also be taken into account to increase image fidelity. This line of investigation is related to the lighting changes addressed in the orthogonal work of Martin-Brualla et al. (2020)." }, { "title": "ZSH shell 사용하기", "url": "/posts/zshshell/", "categories": "coding", "tags": "", "date": "2022-01-09 00:00:00 +0900", "snippet": "나는 주변환경의 디자인을 꽤나 많이 신경 쓰는 편이다. 개발환경도 이와 마찬가지였는데, linux(그리고 mac)에서 기본적으로 사용하는 bash shell은 디자인적으로도 기능적으로도 마음에 들지 않았었다. 그래서 나는 ZSH라는 shell을 사용한다.설치설치는 bash shell을 이용해서 쉽게 할 수 있다. sudo apt install zsh이러면 자동적으로 기본 shell이 bash에서 zsh로 바뀌게 된다.Oh-My-ZshOh-My-Zsh는 Zsh의 plugin으로, Zsh를 더 쉽고 편리하게(그리고 더 예쁘게)사용할 수 있게 해준다. 아래 명령을 입력하면 자동으로 설치가 이루어진다. sudo apt install curl # curl이 설치되어 있지 않다면 sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"powerlevel10k 테마 사용하기Oh-My-Zsh를 처음 설치하면 아마 터미널이 조금 이상하게 보일 것이다. 맞는 font를 설치하면 글씨가 깨지는 현상이 사라지고, 다양한 테마를 적용할 수 있어서 입맛대로 꾸밀 수 있다. 나 같은 경우에는 ‘powerlevel10k’라는 테마를 사용한다. 공식 github repo에 들어가면 자세한 설치 방법이 나왔지만, 간단하게 설명하면 다음과 같다.위 github repo에서 이 부분을 따라하면 쉽게 적용할 수 있다.위 방식대로 설치를 하면 powerlevel10k 테마를 사용할 수 있다. 설치를 하는 과정에서, 그리고 앞으로도 ‘~/.zshrc’ 파일에 많이 접근을 해서 수정을 해줘야 하는데, 만약 visual studio code를 사용하고 있다면 아래 명령으로 이를 새로운 vscode 창으로 열 수 있다. code ~/.zshrc유용한 plugin 설치On-my-Zsh에는 많은 plugin들이 존재하는데, 그중 필수라고 해야 할 정도로 많이 쓰이고 정말 편한 plugin 2개를 소개하겠다.zsh-autosuggestions이 플러그인은 shell command를 자동완성 시켜주는 기능이다. 내가 최근에 사용한 command를 자동으로 불러와주는데, 사실 shell command는 사용하는 것만 돌려 쓰는 경우가 많기 때문에 정말 편리하다. 다음 명령어로 쉽게 추가할 수 있다. git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestionszsh-syntax-highlighting이 플러그인은 shell command에 highlight 기능을 추가해 주는 기능인데, 외관적으로도 예쁘고, 혹시 잘못된 command나 아직 설치되지 않은 command를 입력했을 때 빨간색을 표시가 떠서 정말 편리하다. 이 역시 다음 명령어로 쉽게 추가할 수 있다. git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlightingPlugin 적용 방법~/.zshrc 파일을 열면 아래와 같은 부분이 나올 것이다.plugins=(git)이 부분에 위에 설치한 plugin들을 다음과 같이 추가해주면 된다.plugins=( git zsh-syntax-highlighting zsh-autosuggestions )추가한 후에는 터미널을 재시작해주거나, 터미널에 ‘source ~/.zshrc’를 입력해주면 된다." }, { "title": "TeX 문법 정리", "url": "/posts/post1/", "categories": "blog", "tags": "", "date": "2022-01-08 00:00:00 +0900", "snippet": "Github blog에 수식 넣기" }, { "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "url": "/posts/NeRF/", "categories": "Paper review, NVS", "tags": "", "date": "2022-01-05 00:00:00 +0900", "snippet": "AbstractWe present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate(spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whoseoutput is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses.We describe how toeffectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.IntroductionNovel view synthesis(NVS)는 특정한 scene에 대한 input이 주어졌을 때, 그 scene을 학습하는 것이다. 즉, input들과는 다른 임의의 viewpoint에서 scene을 봤을 때 어떻게 보일지를 예측하는 것을 목표로 한다. 본 논문(NeRF)에서는 MLP를 사용해서 5D input(color + viewing direction)에서 3D view-dependent color을 만들어내는 것을 목표로 한다.NeRF의 간단한 과정은 다음과 같다. Pixel마다 ray를 쏴서 3D point를 ray를 따라 sampling 한다. Sample된 point와 해당하는 2D viewing direction을 MLP에게 feed해서 color과 density값을 학습한다. 구한 color과 density에 volume rendering을 적용해 2D image를 만들어낸다. NeRF의 대략적인 scheme.Neural Radiance Field Scene RepresentationMLP networkNeRF에서는 특정한 scene을 3D location $\\mathbf{x} = (x,y,z)$와 2D viewing direction $(\\theta, \\phi)$를 이용해서 나타내고, 이를 MLP network에 input으로 줘서 color $\\mathbf{c} = (r,g,b)$와 volume density $\\sigma$를 구한다. 이를 식으로 나타내면 다음과 같다(\\ref{eq1}).\\[F_\\Theta : (\\mathbf{x}, \\mathbf{d}) \\rightarrow (\\mathbf{c}, \\sigma) \\label{eq1} \\tag{1}\\] NeRF에서 사용한 MLP network. Direction에 대한 정보가 거의 마지막 layer에 제공된 것을 알 수 있는데, 이는 다른논문에서 자세히 설명하고 있다.Volume renderingMLP network을 통해 구한 color $\\mathbf{c}(\\mathbf{x})$와 volume density $\\sigma(\\mathbf{x})$는 volume rendering을 통해 실제 color value $C(\\mathbf{r})$로 변환된다. Volume rendering은 $\\mathbf{r}$을 origin $\\mathbf{o}$와 viewing direction $\\mathbf{d}$ 을 이용해 $\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}$로 표현했을 때, 다음과 같이 표현된다(\\ref{eq2}).\\[C(\\mathbf{r}) = \\int_{t_n}^{t_f}T(t)\\sigma(\\mathbf{r}(t))\\mathbf{c}(\\mathbf{r}(t), \\mathbf{d})dt, \\text{ where } T(t) = \\exp\\left( -\\int_{t_n}^t\\sigma(\\mathbf{r}(s))ds \\right) \\label{eq2} \\tag{2}\\]이때 $t_n, t_f$는 scene의 near, far boundary가 된다. 실제로 적분을 하려면 위처럼 continuous 한 식이 아니라, discrete한 형태로 바꿔줘야 한다. 이를 위해서 NeRF에서는 $t_n$과 $t_f$사이에서 stratified sampling을 통해 $N$개의 discrete한 sample을 만든다(\\ref{eq3}).\\[t_i \\sim \\mathcal{U}\\left[ t_n + \\frac{i-1}{N}(t_f - t_n), t_n + \\frac{i}{N}(t_f - t_n) \\right] \\label{eq3} \\tag{3}\\]위 sample들을 이용해 equation \\ref{eq2}를 다시 쓰면 다음과 같다(\\ref{eq4}).\\[\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N}T_i(1-\\exp(-\\sigma_i\\delta_i))\\mathbf{c}_i, \\text{ where } T_i = \\exp\\left( -\\sum_{j=1}^{i-1}\\sigma_j\\delta_j \\right) \\label{eq4} \\tag{4}\\]이때 $\\delta_i = t_{i+1} - t_i$로, 인접한 sample간의 거리를 의미한다. NeRF의 학습 과정.Optimizing NeRF위의 방법대로 NeRF를 학습시켰을 때는 좋은 결과가 나오지 않았다고 한다. NeRF의 저자들은 여러 방법을 통해 NeRF의 성능을 끌여올렸다.Positional encodingMLP network $F_\\Theta$는 $xyz\\theta\\phi$의 5D input만으로는 좋은 성능을 보이지 못한다. 그렇기 때문에 positional encoding $\\gamma$를 이용해 input의 dimension을 증가시켜 줬다(\\ref{eq5}).\\[\\gamma(p) = (\\sin(2^0\\pi p), \\cos(2^0\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p)) \\label{eq5} \\tag{5}\\]본 논문에서는 $\\gamma(\\mathbf{x})$에는 $L=10$을, $\\gamma(\\mathbf{d})$에는 $L=4$를 사용했다.Hierarchical volume samplingVolume rendering을 하는 과정에서 그냥 uniform하게 $N$개의 sample을 만드는 것은 비효율적이다. 왜냐하면 아무 object도 없는 빈 공간일 가능성도 있고, occlude된 region일 수도 있기 때문이다. 그래서 NeRF에서는 더 효율적으로 sampling을 하기 위해 hierarchical volume sampling을 사용한다. 하나의 network로 scene을 학습하는 것이 아니라, NeRF에서는 coarse와 fine이 2개의 network를 사용한다. Coarse network은 uniform한 sample $N_c$들을 이용해 학습되고, 이 결과에 기반해서 다시 한번 sampling이 일어나게 된다. 이때 각 sample이 최종 color에 얼마나 기여하는지에 대한 PDF를 이용해 새로운 sampling을 하는데, 이는 Equation \\ref{eq4}를 다음과 같이 다시 쓰면 쉽게 이해할 수 있다(\\ref{eq6).\\[\\hat{C}_c(\\mathbf{r}) = \\sum_{i=1}^{N_c}w_ic_i, \\quad w_i = T_i(1-\\exp(-\\sigma_i\\delta_i)) \\label{eq6}\\tag{6}\\]위의 weight들을 $\\hat{w}_i = w_i/ \\sum _{j=1}^{N_C}w_j$로 normalize하면, 이를 ray를 따른 각 sample의 기여도의 PDF로 생각할 수 있다. 이에 기반해서 sampling된 $N_f$와 $N_C$를 모두 이용해 Fine network 가 학습이 된다. 이는 더 중요한 region에 더 많은 sample들이 있게 해서, NVS의 성능을 올려준다.Training detailsDatasetDataset은 특정 scene에 대한 200장 정도의 RGB image들과 각 image에 대한 camera pose와 intrinsic으로 구성된다. Real data를 사용하는 경우 pose와 intrinsic은 structure-from-motion package인 SFM colmap을 사용해서 구한다.LossLoss에는 coarse와 fine network의 output이 둘다 사용된다(\\ref{eq7}).\\[\\mathcal{L} = \\sum_{\\mathbf{r} \\in \\mathcal{R}}\\left[ \\left| \\left| \\hat{C}_c(\\mathbf{r}) - C(\\mathbf{r}) \\right| \\right|^2_2 + \\left| \\left| \\hat{C}_f(\\mathbf{r}) - C(\\mathbf{r}) \\right| \\right|^2_2 \\right] \\label{eq7}\\tag{7}\\]Evaluation metricPSNR, SSIM, LPIPS가 사용되었다.더 자세한 사항은 NeRF 논문를 참조하길 바란다.Results이전의 연구들에 비해 크게 향상된 성능을 보였다. 더 자세한 내용은 NeRF 논문를 참조하길 바란다. ] Synthetic data를 사용한 결과. Real data를 사용한 결과.ConclusionOur work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that represent- ing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations.Although we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and ren- der neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes." } ]
